# Focus on MVP Release Infrastructure

**version** v0.1.4  
**date** 10 Feb 2026  

## Context

Right, so everyone should have already read the project brief, the spec index, and my v0.1.0 review on the dev and librarian commits. If you haven't, go read those first — this builds on all of that.

This memo is about what we need to focus on RIGHT NOW, which is getting the first release of the MVP out there. And what I mean by that is: the simplest possible site that has fundamentally all of the moving parts wired up end-to-end. Not the business logic. Not the polish. The infrastructure.

## The Core Idea

Think about it this way — I already have a setup where from commit to dev, we push to a CI pipeline that releases to a lambda function. So before we've even written business logic, we have a full blown CI pipeline including pushing into dev and QA. What I want is to take that to the next level for this project.

The beauty of this approach is that once we have every deployment target working, every test level running, and every mode exercised — we should NOT need to touch this infrastructure again. Everything after that is just improvements happening inside the app, inside the containers. The code is the same. The pipeline is the same.

## What the MVP Actually Needs to Do

The core functionality is dead simple: a form where I can upload a file (drag and drop), the file gets encrypted client-side (AES-256-GCM, Web Crypto API — you know this from the spec), and depending on configuration it either goes via a REST call to the server's configured storage (Memory-FS abstraction — could be memory, disk, zip, S3, whatever) OR via direct S3 pre-signed URLs.

Remember from my v0.1.0 review: the service code has no idea what storage is being used. That's the whole point of the Memory-FS abstraction layer. So we need to exercise ALL of these modes.

## The Deployment Targets

Here's what we need deployed and smoke-tested. Every single one of these needs a GitHub Action:

- **AWS Lambda** (via Lambda URL Functions — no API Gateway needed, as I said in v0.1.0) — this is our primary target, and I already have the reference pipeline for this
- **Docker container** — standalone containerized deployment
- **ECS Fargate** — container orchestration on AWS
- **EC2 instance** — traditional VM
- **AMI** — pre-baked machine image that we can launch as an EC2 instance
- **GCP Cloud Run** (or equivalent) — multi-cloud from day one, I have accounts for all of this
- **Local dev server** — for local development obviously

And remember we have TWO lambda functions now (as per v0.1.0 review): the public endpoints and the admin endpoints. So the admin interface needs the same deployment matrix.

## The Testing

This is where I want us to be really thorough. Tests at ALL levels, for ALL deployment modes, with NO mocks (as I said in v0.1.0 — the fact that we can start the entire stack in memory in ~100ms means we don't need mocks or patches, apart from specific integration tests targeting LocalStack or S3).

What we need:

- **Unit tests** — core library, storage backend adapters, token validation, all the Type_Safe schemas
- **Local integration tests** — spin up the app, test every storage mode (memory, disk, S3 via LocalStack). Test the CLI against each mode too.
- **E2E tests with Playwright** — browser-based automation replicating actual user behaviour. Upload via drag-and-drop, upload via file picker, verify writes to S3, verify writes to memory, verify writes to disk. The full transfer cycle: upload → encrypt → share link+key → download → decrypt → verify the file is intact. Wrong key gives a clear error not a corrupted file. Transparency panel shows the right data.
- **Post-publish tests** — when we publish to PyPI, we need a workflow that in a CLEAN environment, installs from PyPI and tries to actually use it. And again, running against all the different operation modes.
- **Deployment smoke tests** — after each deployment target is stood up, hit `/health`, verify pre-signed URLs work, verify token validation, verify no plaintext in S3 or CloudWatch, verify CORS, verify the frontend loads on mobile, the whole checklist from the spec.

## The CLI

We should have a CLI that can be used to consume this. Same as the website — it wraps the core library, supports all storage modes. This is important for testing and for power users.

## PyPI Publishing

When we publish the package, the CI pipeline needs to: build it, publish it, then in a completely separate clean environment clone/install from PyPI and run the smoke tests. This validates the package actually works when someone installs it fresh.

## What I Will Wire Up

As I mentioned in v0.1.0:
- I will do Feature-8 with the first implementation of the Fast_API class and the GitHub Actions CI pipeline (test → tag → deploy to dev/qa lambda)
- I will wire up the static Fast_API route for the UI
- I will provide the Memory-FS S3 support class
- I will provide code samples for the no-mocks testing pattern from other projects

## Important Reminders from v0.1.0

Since this is all related, let me re-emphasise a few things that directly affect how you build this infrastructure:

- all files must be versioned with the current version prefix (version is at `sgraph_ai_app_send/version`)
- ALL backend data is classified as non-sensitive/non-confidential — we need anonymisation modes (e.g. for IP addresses in stats)
- use Memory-FS in single file mode for the generic data store abstraction
- use LocalStack for integration tests that need S3
- use osbot-aws for ALL AWS operations, never boto3 directly
- use osbot-fast-api for the server (CORS, auth, entry point all handled)
- all code follows Type_Safe patterns and the guides in `./library/guides/development`
- the transfer folder (`transfers/{transfer_id}/`) stores EVERYTHING about that transfer — all actions, all requests, all entries, each event as its own file with a unique event-id
- the action logging mode I described (all REST calls related to a particular id/asset) — this needs to be in from the start, it's essential for the dev/QA workflow and for the transparency UX
- Register Interest moved to external platform (not our problem to implement)
- SPA routing with client-controlled routes, no hash routing
- IFD methodology for all frontend work (see `./library/guides/development/ifd`)
- multiple themes/layouts from day one (A/B testing ready)
- i18n and accessibility support from the start

## What "Done" Looks Like

When this sprint is done, I should be able to:

1. Push a commit and watch it flow through CI → tests → deploy to dev lambda automatically
2. See the minimal upload UI, drag a file, have it encrypt and upload
3. Get the download link + decryption key, open it in another browser, enter the key, get the file back
4. See all of this working on Lambda, Docker, Fargate, EC2, AMI, and GCP
5. Install from PyPI in a clean env and use the CLI to do a transfer
6. See Playwright E2E tests passing against the running app
7. See the admin interface deployed with the same matrix
8. See the action log for a transfer showing every single thing that happened

After this — we don't touch the infrastructure. We just build features inside it.

## For the Agents

- **Architect**: review this against your current architecture docs, flag anything that conflicts or needs updating. Pay particular attention to the Memory-FS abstraction and how it affects the deployment matrix.
- **Dev**: this is your north star for what to build next. Focus on getting one deployment target fully working end-to-end first (Lambda), then replicate across the matrix.
- **QA**: start drafting the test matrix — storage modes × deployment targets × test levels. Every cell in that matrix needs a test.
- **Conductor**: break this down into the Issues FS tree. This is probably an epic with stories per deployment target and per test level.
- **DevOps**: you own the GitHub Actions, the deployment configs, the Lambda/Fargate/EC2/AMI/GCP targets. Work closely with Dev to get each target stood up.
- **AppSec**: review the encryption flow, the token auth model, the pre-signed URL scoping, and the no-plaintext-on-server guarantee. Flag anything that needs hardening before we deploy across all these targets.
- **GRC**: the transparency model and data classification (all backend data non-sensitive/non-confidential) is your territory. Make sure the anonymisation modes are specced out properly.
- **Librarian**: keep the docs organised as this sprint produces artifacts. Version prefixes on everything, cross-references between role folders, make sure nothing gets lost.
- **Cartographer**: map out the dependency graph between deployment targets, test levels, and storage modes. This is getting complex and we need visibility on what blocks what.
- **Historian**: track decisions being made during this sprint. When we change something from the original spec (and we will), capture the what, why, and when.
- **Journalist**: once we have the infrastructure working, we'll need to communicate this to beta users. Start thinking about the "how it works" content and the transparency explanations.

As always, create your review files in your respective `team/roles/` folders, prefixed with the current version, and create a master file I should read first.