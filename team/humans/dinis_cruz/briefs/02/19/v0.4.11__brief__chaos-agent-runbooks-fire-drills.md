# Brief: Operational Maturity — Chaos Agent, Runbooks, and Fire Drills

**version** v0.4.11  
**date** 19 Feb 2026  
**from** Human (project lead) — triggered by conversation at industry dinner  
**to** GRC (lead), AppSec, Conductor, Architect, DevOps  
**type** Operational readiness brief  

---

## Context

At the industry dinner, someone asked whether we have a "Chaos Engine" — an agent whose job is to break things, stress-test the system, and expose weaknesses. The human confirmed this is planned and wants to formalise three related concepts:

1. **Chaos Agent** — a new role that deliberately creates problems to test resilience
2. **Runbooks** — documented procedures for responding to incidents at each severity level
3. **Fire Drills** — scheduled exercises where we test the runbooks and the team's response

These are connected: the Chaos Agent creates scenarios, the runbooks define how we respond, and fire drills verify that the runbooks work.

---

## Part 1: The Chaos Agent (Future Role)

Inspired by Netflix's Chaos Monkey but adapted for an agentic AI team. The Chaos Agent's job: throw spanners into the works and observe what happens.

### What the Chaos Agent Does

| Activity | Description |
|---|---|
| **Load testing** | Generate large volumes of user traffic, file uploads, downloads. Push the system to its limits. How many concurrent users before Lambda throttles? How many files before S3 costs spike? |
| **Token exhaustion** | Simulate scenarios where access tokens are used up, reused, or brute-forced. Does the system fail gracefully? |
| **Resource exhaustion** | Simulate runaway processes — a Lambda that doesn't terminate, an S3 bucket filling up, a CloudFront distribution under DDoS. |
| **Malicious input** | Submit unexpected payloads — oversized files, malformed JSON, XSS payloads in message content, path traversal attempts. Does the system reject or handle them? |
| **Failure injection** | What happens when S3 is unavailable? When Lambda times out? When CloudFront returns errors? Simulate partial infrastructure failures. |
| **Data integrity** | Verify that encrypted data remains intact after all operations. Upload → encrypt → store → retrieve → decrypt → verify. Does the content match? |
| **Benign-gone-wrong** | Simulate legitimate use cases that go out of control — a user uploading 10,000 small files, a script hitting the API in a tight loop, a bot crawling every page. |

### When to Introduce This Role

The human flagged this as a future role — not for immediate implementation. The prerequisites:

1. We need the runbooks first (so we know how to respond to what the Chaos Agent finds)
2. We need monitoring in place (so we can detect what the Chaos Agent does)
3. We need separate environments (the Chaos Agent should never run against production without explicit authorisation)

**Librarian**: index this as a planned future role. Create a placeholder in `team/roles/chaos/` with a brief description. Don't create the full role definition yet.

---

## Part 2: Runbooks — Incident Response Procedures

### What We Need

A runbook for each severity level in our P0–P10 framework. Each runbook answers: when this happens, what do we do?

### Runbook Structure (Per Severity Level)

Each runbook should contain:

```
1. DETECTION   — How do we know this is happening?
2. ASSESSMENT  — How do we confirm the severity?
3. ESCALATION  — Who is notified? In what order?
4. RESPONSE    — Step-by-step actions to take
5. MITIGATION  — How do we stop or contain the problem?
6. RECOVERY    — How do we restore normal operation?
7. POST-MORTEM — How do we learn from this?
```

### Runbooks by Severity

| Level | Trigger | Key Actions |
|---|---|---|
| **P0 — Major Crisis** | Active data breach, complete system compromise, confirmed key leak | **Pull the plug.** Take all services offline immediately. Notify all users. Engage legal. Full incident response team. No shortcuts. |
| **P1 — Major Incident** | Active exploitation with real customer impact, significant data exposure risk | Incident commander assigned. War room (async via documents). All relevant roles engaged. Continuous status updates. External communication if needed. |
| **P2 — Significant Incident** | Confirmed vulnerability being actively exploited, partial service outage, some customer impact | AppSec leads response. Fix and deploy within hours. Affected users notified. Post-mortem within 24 hours. |
| **P3 — Must Fix** | Vulnerability that could be exploited but isn't yet, significant gap in security controls | Fix before next production release. AppSec reviews the fix. QA validates. Deploy through normal pipeline (dev → QA → prod). |
| **P4 — Should Fix** | Defence-in-depth improvements, hardening, supply chain hygiene | Schedule in next sprint. Normal development workflow. AppSec review on completion. |
| **P5–P8** | Minor improvements, cosmetic, theoretical | Backlog. Fix when convenient. Group into hardening sprints. |
| **P9–P10** | Negligible, informational | Document and close. May not require any code change. |

### Special Runbooks

Beyond severity-based runbooks, we need procedure-specific ones:

| Runbook | Description |
|---|---|
| **"Turn it off"** | Exact steps to take all services offline. Which AWS resources to disable, in what order, how to verify nothing is running. This is the P0 "pull the plug" procedure — it must be documented and tested. |
| **Cost spike** | What to do when AWS costs spike unexpectedly (as happened recently with extra logging). Detection, diagnosis, remediation. |
| **Token compromise** | An access token has been leaked or brute-forced. How to revoke it, assess impact, notify affected users. |
| **External security disclosure** | Someone reports a vulnerability (as User A did). How to receive it, acknowledge, classify, respond. This is the runbook we informally followed — now formalise it. |
| **Service degradation** | The service is slow but not down. Lambda throttling, S3 latency, CloudFront errors. Diagnosis and response. |

### GRC Deliverables

| Task | Priority |
|---|---|
| Write P0 runbook ("pull the plug") | P1 |
| Write P1 runbook (major incident) | P1 |
| Write P2 runbook (significant incident) | P2 |
| Write external security disclosure runbook | P2 |
| Write cost spike runbook | P2 |
| Write P3–P10 runbooks (can be lighter-weight) | P3 |
| Write token compromise runbook | P3 |
| Write service degradation runbook | P3 |

---

## Part 3: Fire Drills — Testing the Runbooks

### The Principle

A runbook that has never been exercised is a document, not a procedure. We need to actually run through the steps — under controlled conditions — to verify that they work, that the team knows what to do, and that the procedures produce the expected outcomes.

### What a Fire Drill Looks Like

1. **Announce** the fire drill (so nobody panics — this is a test, not a real incident)
2. **Inject** the scenario (the Conductor or the human declares: "We are running a P2 fire drill. The scenario is: an access token has been compromised.")
3. **Execute** the runbook (the relevant roles follow the documented steps)
4. **Observe** what happens (the Historian records, the GRC evaluates)
5. **Debrief** after the drill (what worked? What didn't? What needs to change in the runbook?)
6. **Update** the runbook based on the debrief findings

### Fire Drill Schedule

| Frequency | What to Drill |
|---|---|
| **Monthly** | A P3–P5 scenario (vulnerability found, cost spike, service degradation) |
| **Quarterly** | A P1–P2 scenario (active exploitation, significant incident) |
| **Annually** | A P0 scenario (full "pull the plug" exercise) |

These are aspirational — we're not at the scale where monthly drills are practical yet. But we should do our first fire drill as soon as the first runbooks are written. Start with the P0 "turn it off" procedure — it's the most critical and the simplest to test.

### Connecting to the Chaos Agent

Once the Chaos Agent role is active, fire drills become more sophisticated:
- The Chaos Agent creates the scenario (not just declares it, but actually injects the failure)
- The team responds as if it's real
- The Chaos Agent observes whether the detection and response systems work
- Unannounced drills become possible (the Chaos Agent triggers a scenario without warning — does the team detect and respond correctly?)

---

## Performance and Load Testing (Separate from Chaos)

The human also mentioned generating traffic and simulations — this overlaps with but is distinct from the Chaos Agent:

| Activity | Owner | Purpose |
|---|---|---|
| **Load testing** | QA, DevOps | How many concurrent users can the system handle? Where does it break? |
| **Stress testing** | DevOps | What happens under sustained heavy load? Does Lambda scale? Does S3 throttle? |
| **Cost modelling** | Accountant, DevOps | What does 1,000 concurrent users cost? 10,000? |
| **Malicious simulation** | AppSec | What does an attacker's traffic look like? Can we detect it? |

These should be run against the dev or QA environment, never against production (unless specifically authorised for a controlled test).

---

## Summary

| Deliverable | Owner | Priority |
|---|---|---|
| P0 "pull the plug" runbook | GRC, DevOps | P1 |
| P1 major incident runbook | GRC, AppSec | P1 |
| P2 significant incident runbook | GRC, AppSec | P2 |
| External security disclosure runbook | GRC, Sherpa | P2 |
| Cost spike runbook | GRC, Accountant, DevOps | P2 |
| Remaining severity runbooks (P3–P10) | GRC | P3 |
| Special runbooks (token compromise, service degradation) | GRC, AppSec | P3 |
| First fire drill: P0 "turn it off" exercise | GRC, Conductor, DevOps | P2 (after P0 runbook is written) |
| Chaos Agent role placeholder (future) | Librarian | P4 |
| Load testing plan for dev/QA environment | QA, DevOps | P3 |

---

This document is released under the Creative Commons Attribution 4.0 International licence (CC BY 4.0). You are free to share and adapt this material for any purpose, including commercially, as long as you give appropriate credit.
