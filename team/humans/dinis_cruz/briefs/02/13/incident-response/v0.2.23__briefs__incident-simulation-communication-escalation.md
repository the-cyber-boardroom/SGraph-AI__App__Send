# First, Do No Harm: Simulation, Communication, and Escalation During Incidents

**version** v0.2.23  
**date** 12 Feb 2026  
**series** Incident Handling (4 of 5)  
**see also** "The P3-as-P1 Philosophy" (1 of 5), "Running P3-as-P1 in Practice" (2 of 5), "Incidents as the Best Time to Fix Things" (3 of 5), "The Football Team" (5 of 5)  

---

## First, Do No Harm

Doctors have a phrase: primum non nocere — first, do no harm. It applies directly to incident response.

When something goes wrong, the instinct is to act. Contain it. Fix it. Shut it down. Roll it back. The problem is that your response actions can cause more damage than the incident itself. A hasty rollback breaks something else. A broad credential rotation locks out legitimate users. An aggressive containment takes down a service that was still functioning. A well-intentioned fix introduces a new vulnerability.

The P3-as-P1 approach gives you something invaluable here: **time**. Because the business impact is low (it's a P3), you have the space to think before you act. You don't have to make decisions in the first five minutes. You can take an hour. You can take a day. You can investigate fully before committing to a response — and in doing so, you practice the discipline of measured response that you'll need when a real P1 hits and the pressure to act immediately is intense.

### Predict the Reactions Before You Act

Before taking any response action during an incident, the team should answer:

- What will this action change in the system?
- What are the second-order effects? (If we rotate this key, what else uses this key? If we block this IP range, who else is in that range?)
- Could this action make the situation worse? (If we restart this service, will it come back in a corrupted state? If we deploy a fix, could the deployment pipeline itself be compromised?)
- Is there a way to test this action before committing to it?
- What's the rollback plan if this action fails?

In a P3-as-P1, these questions are practice. In a real P1, they're the difference between a contained incident and a self-inflicted escalation.

## Simulating Before Acting: The Agentic Advantage

Here's where our setup becomes genuinely powerful. With an agentic team and good system models, we can **run simulations before taking action**.

Think about it: if we have a good understanding of how the system works — the architecture, the dependencies, the data flows, the blast radius maps — we can ask an agent to model the consequences of a proposed response action before we execute it.

"If we rotate the AWS key, what services will be affected? How long will they be down? What's the user-facing impact?"

"If we block this IP range in the WAF, who else is in that range? Are any of our synthetic users running from those IPs? Will Google Analytics be affected?"

"If we deploy a hotfix to Lambda, what's the blast radius if the fix has a bug? Can we deploy to one region first and observe?"

These aren't hypothetical questions. They're queries that can be answered by an agent with access to the architecture documentation, the dependency maps, the deployment configuration, and the cartographer's blast radius visualisations. The agent can model the what-if scenario and present the likely consequences — with confidence levels and caveats — before anyone touches a button.

### Building the World Model

For simulations to work, you need a model of the system that's accurate enough to predict consequences. This is built incrementally:

- **The cartographer** maintains the dependency graph — what connects to what
- **The architect** maintains the design documentation — how things are supposed to work
- **The DevOps** maintains the deployment state — what's actually running where
- **The sherpa** maintains the behavioural model — how users interact with the system
- **The historian** maintains the change log — what changed recently and what the effects were

Each P3-as-P1 tests and improves this model. Every time a simulation predicts something and reality differs, the model gets updated. Over time, the model becomes accurate enough to make pre-action simulations genuinely useful — and the team builds confidence in running simulations before committing to actions.

The goal: before any significant response action, the team can ask "what if we do this?" and get a useful answer within minutes. Not a perfect answer — but a useful one that identifies the obvious risks and second-order effects.

## Personalised Stakeholder Communication

This is one of the most powerful applications of LLMs in incident response, and one that traditional security teams struggle with enormously: **communicating the same incident to different stakeholders in the language, format, and focus that each stakeholder needs**.

### The Problem in Traditional Organisations

During an incident, the CISO needs to brief:

- The board (wants a one-line summary and a risk assessment)
- The CEO (wants to know if customers are affected and what the media exposure is)
- The CFO (wants to know the financial impact)
- The CTO (wants the technical root cause and the fix timeline)
- The legal team (wants to know about regulatory obligations)
- The DPO (wants to know about personal data exposure)
- The comms team (wants to know what to say publicly)
- The affected customers (want to know what happened to their data)
- The development team (wants to know what broke and what they need to fix)
- The compliance team (wants to know which frameworks are implicated)

Each of these audiences needs a different briefing. Different length, different focus, different level of technical detail, different framing. In a traditional organisation, the CISO (or someone on their team) has to manually write each of these briefings — while simultaneously managing the investigation. It's a massive communication burden that directly competes with the technical work of resolving the incident.

### The LLM Solution

With our setup, the incident data lives in an Issues FS database. Every finding, every timeline entry, every decision, every piece of evidence is structured data in a graph. From that single source of truth, we can generate:

- **One paragraph** for the board chair who checks their phone at midnight
- **Five bullet points** for the CEO's morning briefing
- **A full technical timeline** for the CTO and development team
- **A regulatory impact summary** for the DPO and legal team, citing specific UK GDPR articles and ICO notification requirements
- **A customer-facing statement** for the comms team, reviewed by the advocate for user-appropriateness and the DPO for legal accuracy
- **An operational status update** for the DevOps and infrastructure team
- **A risk assessment update** for GRC, mapped to the risk register

All generated from the same underlying data. All consistent (because they're derived from the same source). All customised to what that specific person needs to know. And all produced in minutes, not hours.

### The Journalist's Role

The journalist doesn't just write one incident report — they produce the full range of stakeholder communications, each tailored to its audience. During an active incident, the journalist is continuously producing:

- **Internal status updates** on a cadence (hourly during active investigation)
- **Stakeholder-specific briefings** as the situation evolves
- **Draft external communications** (if customer or public notification is needed)
- **Post-incident report** (the full narrative, for the record)

The journalist draws on the Issues FS incident database as the authoritative source. The historian ensures the timeline is precise. The DPO ensures the regulatory language is accurate. The advocate ensures the user-facing communications serve users.

### Personalisation at Scale

Here's the deeper point: this capability doesn't just apply to incident communication. It's a model for how we communicate everything. Status updates, progress reports, design decisions, risk assessments — all of these can be personalised for different audiences from the same underlying data.

But incidents are where you build the muscle. The urgency of an incident forces you to get good at multi-audience communication fast. The P3-as-P1 approach lets you practice this when the stakes are low.

## Escalation Mapping: The "What Happens When" Graph

Every incident response needs a clear escalation model. Who gets notified when? What triggers each level of response? How does a P3-as-P1 escalate to a genuine P1 if the investigation reveals the situation is worse than initially thought?

### The Escalation Graph

This should be modelled as a graph in Issues FS — not a flat document, but a connected structure that shows:

**Trigger conditions**: what events or findings cause escalation?
- Severity reassessment (P3 turns out to be P1)
- Personal data exposure confirmed (triggers DPO and ICO notification workflow)
- Customer-facing impact detected (triggers advocate and ambassador)
- Media attention or public awareness (triggers journalist external comms workflow)
- Scope expansion (more systems affected than initially thought)

**Notification chains**: who gets told, in what order, through what channel?
- First responders (CISO, DevOps, conductor) — immediate, any hour
- Technical team (AppSec, architect, dev, QA) — within 30 minutes during business hours, within 2 hours outside
- Compliance and legal (DPO, GRC) — within 1 hour if personal data is involved
- Stakeholder communications (journalist, advocate) — as soon as external communication is needed
- Leadership — based on severity and impact thresholds

**The 2am question**: what scenario causes someone to be woken up? For an agentic team, this translates to: what triggers an automated escalation that activates agents, sends alerts, and spins up the incident response infrastructure outside of normal operating hours?

This needs to be defined concretely:
- What monitoring systems detect the triggering conditions?
- What alerting mechanism sends the notification? (PagerDuty, webhook, email, SMS — what do we actually use?)
- What information is included in the initial alert? (Enough context to assess severity without having to log in and investigate)
- What's the acknowledgement protocol? (How do we know someone received the alert and is responding?)
- What's the timeout/escalation? (If no acknowledgement within N minutes, who gets notified next?)

For our current setup, many of these don't exist yet. That's fine — the tabletop exercise and the first few P3-as-P1 incidents will reveal exactly what's missing. But the escalation graph should be designed now, even if the implementation is incomplete.

### Escalation in an Agentic Context

With an agentic team, escalation has an interesting property: agents can be activated in parallel, instantly, without the "calling someone at 2am and waiting for them to wake up" delay. The initial triage — "what happened, what's the scope, what's the likely severity?" — can be performed by agents within minutes, producing a structured assessment that a human can review.

This means the escalation model can include an automated first-response layer:

1. **Automated detection** — monitoring detects an anomaly
2. **Automated triage** — agents assess the scope, severity, and blast radius, producing a structured briefing
3. **Human notification** — based on the triage assessment, the right human is notified with enough context to decide the response level
4. **Full team activation** — if the situation warrants it, the full P3-as-P1 (or actual P1) process kicks off

The human doesn't get woken up for the triage. The human gets woken up with the triage already done.

## Framework Integration: Organic, Not Comprehensive

There are excellent incident response frameworks that we should be using:

- **MITRE ATT&CK** — the taxonomy of adversary tactics, techniques, and procedures
- **NIST Cybersecurity Framework** — the high-level structure for identifying, protecting, detecting, responding, and recovering
- **NIST SP 800-61** — the specific incident handling guide
- **Various incident response playbooks** — pre-built response procedures for common incident types

The instinct is to map everything comprehensively — take MITRE ATT&CK, map every technique to our infrastructure, produce a complete coverage matrix. Take NIST, map every control to our implementation. Take ISO 27001, take the GDPR framework, take everything, map everything.

Don't. This is the diminishing returns trap.

### The Organic Approach

Instead, expand the framework coverage one incident at a time:

1. An incident occurs
2. During the investigation, identify which MITRE ATT&CK techniques are relevant
3. Map those specific techniques to our infrastructure (what's our detection capability for this technique? What's our response? Where are the gaps?)
4. Link the incident in Issues FS to the relevant MITRE ATT&CK nodes
5. Move on

Next incident, repeat. Over time, the graph expands organically. Every framework connection is driven by a real event, not a theoretical exercise. Every mapping has an incident attached to it that proves why it matters.

This means:
- **No wasted effort** — you never map a technique that's not relevant to your actual threat landscape
- **Every mapping has context** — it's linked to the incident that triggered it, so anyone reviewing the mapping can see why it exists
- **Progressive coverage** — after 10 incidents, you have 10 clusters of framework coverage, each one battle-tested
- **Multiple frameworks simultaneously** — a single incident might touch MITRE ATT&CK (the technique used), NIST CSF (the detection gap), UK GDPR (the data protection implication), and ISO 27001 (the control deficiency). Map all of them, but only the relevant nodes.

The GRC role, the DPO, and the CISO each own different framework mappings. They expand their frameworks in parallel, each from their own perspective, each driven by the same incidents.

### The Graph Advantage

This is where Issues FS shines. Each framework is a graph. Each incident is a graph. The connections between them are edges in the graph. Over time, you can query:

- "Which MITRE ATT&CK techniques have we actually encountered?" (the ones linked to incidents)
- "Which NIST CSF functions have the most gaps?" (the ones where incidents keep revealing missing controls)
- "Which ISO 27001 controls are least mature?" (the ones that keep appearing in incident improvement backlogs)
- "Which incidents had the broadest framework impact?" (the ones with the most cross-framework connections)

This is a living, growing, evidence-based compliance and security posture — not a static spreadsheet updated once a year for audit season.

## Systemic Weaknesses: The Third Stories

The first story of an incident is what happened. The second story is why it happened. The third story is the systemic infrastructure weaknesses that made the second story possible.

These third stories keep appearing across incidents. They're the fundamental problems that the team keeps bumping into — the things that make everything harder, that constrain what's possible, that create risk in every operation. In our current setup, these include:

### Identity and Access Management
- Cannot create proper individual accounts for agents (they share credentials or use over-privileged service accounts)
- GitHub account limitations — can't create the accounts we need, can't properly separate agent identities
- Agent over-permissions — agents have far more capability than they need for any specific task
- Human over-permissions — same problem, different actors
- No proper identity federation across services

### Code Integrity
- Commits not being properly signed — can't verify who (or what) made a change
- No enforcement of signing policies in the CI pipeline
- Gap between "who made this commit" and "who authorised this change"

### Secure Communication
- No secure channel for inter-agent communication during incidents
- No secure channel for agent-to-human escalation with confidentiality guarantees
- SGraph Send could solve this — but it's the product we're building, not infrastructure we can use yet
- The irony: we're building a secure communication tool but can't securely communicate during our own incidents

### Observability Gaps
- (Detailed in the DPO/observability brief — AWS logging not fully enabled, trail data not yet flowing, dashboards not yet built)

Each P3-as-P1 incident will bump into these systemic weaknesses. Each encounter is an opportunity to document the impact, quantify the risk, and build the case for fixing them. Over time, the accumulation of evidence from real incidents makes the case for infrastructure investment undeniable.

These are the "things that we started to see and that we're seeing in action" — the fundamental constraints that limit how effectively the agentic team can operate. Incidents reveal them. The improvement backlog captures them. The conductor prioritises them. And gradually, they get resolved.

---

## For the Agents

- **CISO**: develop the "do no harm" checklist — before any response action, what questions must be answered? Start with the five questions from the "predict the reactions" section. Refine based on experience.
- **Conductor**: design the escalation graph. Define trigger conditions, notification chains, timeouts, and acknowledgement protocols. This should live in Issues FS as a queryable structure, not a flat document.
- **Architect**: work with the cartographer to build the world model that enables pre-action simulation. The model needs: dependency graph, deployment state, data flow map, and blast radius calculations. Each P3-as-P1 tests and improves this model.
- **Journalist**: develop the multi-audience briefing capability. Create templates for each stakeholder type (board, technical, regulatory, customer-facing, operational). Practice generating all of them from a single incident database during the next P3-as-P1.
- **DPO**: own the regulatory communication templates. During any incident involving personal data, you produce the ICO notification draft (within the 72-hour window), the data subject notification draft (if required), and the GDPR-specific assessment. These should be templated and ready to customise, not written from scratch each time.
- **GRC**: own the framework mapping expansion. After each incident, map the relevant nodes in MITRE ATT&CK, NIST CSF, and any other applicable framework. Track framework coverage growth over time.
- **AppSec**: maintain the systemic weakness register — the third-story findings that keep appearing across incidents. Identity management, code integrity, secure communication, observability gaps. Track which incidents each weakness contributed to.
- **DevOps**: for the automated first-response layer — what monitoring can you wire up now that would produce an automated triage report? What data can be gathered automatically the moment an anomaly is detected? Build this incrementally.
- **Advocate**: you're the voice of the user in every stakeholder communication. When the journalist produces the customer-facing statement, you validate it. When the DPO drafts the data subject notification, you review it for human-appropriateness. The user should never receive a notification that's legally correct but incomprehensible.
- **Cartographer**: the escalation graph and the what-if simulation both depend on your dependency maps being current. Every incident that reveals an unmapped dependency is a signal to update. The blast radius visualisations are a critical input to the "do no harm" assessment.
