# The P3-as-P1 Philosophy: Why Near Misses Matter More Than Incidents

**version** v0.2.23  
**date** 12 Feb 2026  
**series** Incident Handling (1 of 3)  
**see also** "Running P3-as-P1 in Practice" (2 of 3), "Incidents as the Best Time to Fix Things" (3 of 3)  

---

## The Core Idea

A security incident is fundamentally just an event — something that happened with unintended consequences that have security implications. That's it. The interesting question isn't "what happened?" It's three harder questions:

1. **What could have happened?**
2. **How did we get here?**
3. **Was the damage done the maximum that could have been done?**

The answer to question 3 is almost always no. Usually the actual damage is around 10% of what was possible. Often it's 1%. And that asymmetry — between what did happen and what could have happened — is where the real risk lives. Companies celebrate containing an incident when they should be terrified by the paths not taken.

## What "Security Implications" Actually Means

Security isn't just confidentiality. It's integrity, availability, system impacts, and all the properties that fundamentally lead to one another. A data integrity issue causes an availability issue causes a confidentiality issue. They chain. This means the definition of "security incident" is broader than most teams realise — and the implications of any single event cascade further than the immediate damage suggests.

## The Paths Not Taken

Here's a real-world pattern that repeats in almost every incident: an attacker gains access through a compromised AWS key. They find themselves in one region. They spin up some instances to mine crypto. They never look at other regions. They had access to everything — every region, every service, the crown jewels — but they turned right instead of left.

The business sees: "They mined some crypto on our infrastructure. Cost us a few hundred quid. We rotated the key. Incident closed."

What the business should see: "An attacker had full access to our entire AWS estate. If they had been a different attacker — more sophisticated, more motivated, with a different business model — they could have exfiltrated every database, destroyed every backup, encrypted every volume, and held the company to ransom. The fact that this particular attacker only wanted cheap compute is not a security control. It's luck."

The damage that occurred was perhaps 0.1% of the damage that was possible. The incident response closed out the 0.1%. Nobody addressed the 99.9%.

### The Question That Should Be Asked

When presenting an incident to leadership, the question shouldn't be "here's what happened and how we fixed it." The question should be: **"Here's what happened. Here's everything else that could have happened. The risk you're accepting is not the incident that occurred — it's the full envelope of what was possible."**

That should be uncomfortable reading. It usually isn't, because most organisations don't do this analysis.

## The Union of All Possible Damage

Think of it like this. Someone steals £10 from your hand. First-level analysis: you lost £10. But:

- Was that the only £10 you had? (Risk appetite — can you absorb this?)
- How did they get close enough to take it? (Root cause — what's the systemic weakness?)
- Could they have taken your wallet? Your phone? Your keys? (The union of all possible damage)
- Will they come back tomorrow? Will they tell others? (Future risk from the same vector)

If the £10 loss is within your risk appetite, and you've genuinely mapped the full union of possible damage and it's all within tolerance, then fine — it's an event, not an incident. But if you only assessed the £10 and never asked about the wallet, you haven't actually assessed the risk. You've assessed the outcome.

## Second Stories — Why Things Really Happen

The surface story of any incident is what happened: the key was exposed, the attacker got in, they mined crypto, we rotated the key. The second story is why it happened: why was that key there? Why wasn't it rotated automatically? Why did the monitoring not detect the unusual activity? Why did the architecture allow a single key to access everything? Why did the team not know the blast radius of that key?

Second stories reveal systemic causes. They show you the things that are broken "really spectacularly" that somehow nobody was paying attention to, but the signals were always there. The incident didn't create the problem — it revealed it. The problem was there all along, waiting for a trigger.

This is exactly what the airline industry learned. A near miss — a plane that almost had an incident but didn't — gets the same level of investigation as an actual crash. Because the systemic causes are the same. The only difference is luck. And you don't build safety on luck.

AWS understands this with their concept of "correction of errors" (COE) where they analyse operational events that didn't necessarily become P1 incidents but revealed systemic weaknesses. They follow the threads. They ask the hard questions. They fix the underlying causes, not just the surface symptoms.

## Calibrate to Reality, Not Movie Plots

There's a famous XKCD comic: one panel shows an elaborate cryptographic attack chain (intercept the traffic, crack the cipher, exploit the vulnerability, decode the keys). The other panel shows someone threatening the user with a wrench until they give up the password.

A lot of threat modelling falls into the movie-plot trap — imagining sophisticated attack chains that are theoretically possible but practically unlikely given your actual threat actors. Meanwhile, the real risks are mundane: an unrotated key, an unencrypted laptop, an agent that found its way into a home directory and captured credentials.

This doesn't mean sophisticated threats don't exist. It means your incident response and your risk analysis should be calibrated to your actual threat landscape:

- **Who is actually after you?** Nation states? Opportunistic criminals? Script kiddies? Automated scanners? An agent that proactively tried to be helpful and caused damage?
- **What do they actually want?** Your data? Your compute? Your reputation? Nothing specific — they just found an open door?
- **What does your historical data tell you?** When this has happened before (to you or to similar organisations), what did the attacker actually do?

A house with military-grade protection — tanks, weapons, booby traps — is more secure than a house with a basic lock. But the house with the basic lock is safer to live in. Most people would prefer to live in the house where, in the last 50 years, nobody has tried to pick the lock. The threat landscape matters more than the security features.

This is why, as a CISO, sometimes the right investment was a CI pipeline rather than a security tool. Without a CI pipeline, developers can't fix bugs quickly — including security bugs. A security scanning tool that nobody can act on is theatre. A CI pipeline that lets developers fix things in minutes is actual security. The unsexy infrastructure investment produces more security value than the flashy tool.

### The Compliance Tax

This connects to a broader problem: the security tax driven by compliance. Organisations do security things because compliance says they must, not because the threat landscape demands it. This creates busy work that crowds out the things that would actually improve security. It's why cybersecurity sometimes has a bad name — it's doing stuff that shouldn't be done, and not doing the stuff that should be done.

The P3-as-P1 approach counters this by grounding security work in reality. An actual incident (even a minor one) tells you what's really happening, what's really at risk, and what really needs fixing. It cuts through the compliance theatre and focuses attention on the things that matter.

## The Real-World Calibration Problem

Consider the major outages and incidents of recent years — AWS outages, Azure outages, CloudFlare incidents, the CrowdStrike update that took down millions of machines. Every one of them was bad. Every one of them could have been 100 times worse. 10,000 times worse.

If you'd done this "union of all possible damage" exercise with any of those incidents, the actual outcome would look almost benign compared to the worst-case scenarios that the same root cause could have enabled. But because the actual outcome was "bad but survivable," the response was proportional to what happened, not to what could have happened.

The first event — the thing that actually went wrong — might have had a 0.001% chance of occurring. But it did occur. That means the probability model was wrong, or the conditions aligned, or it was just statistics at scale. Once you accept that the "impossible" event happened, the question becomes: what else is now within the realm of possibility? And the answer is usually terrifying, because the same root cause that enabled the 0.001% event also enables a cascade of other scenarios that nobody mapped.

## Where This Leads

This philosophy drives two practical conclusions:

1. **Run minor incidents at major-incident process levels** — the P3-as-P1 approach. Treat near misses and minor events as full incidents, not because the damage is severe, but because the investigation reveals the systemic weaknesses that the next incident will exploit. (See: "Running P3-as-P1 in Practice")

2. **Use incidents as the forcing function for improvement** — incidents create focus, alignment, and urgency that normal prioritisation can never achieve. The things you fix during an incident investigation would otherwise take months of advocacy, bridge-burning, and political capital. (See: "Incidents as the Best Time to Fix Things")

The goal is a team that pays attention to weak signals. A team where every role — not just security — notices when something odd happens and asks: "how come that was possible?" A team where the question "how many times has this happened before?" always has an answer — and when the answer is "we don't know," that itself becomes the incident.

---

## For the Agents

- **CISO**: this is your operational philosophy. Internalise it. When the next minor event occurs, your instinct should be to investigate the full envelope of possible damage, not just the actual damage.
- **GRC**: every incident analysis should include the "union of all possible damage" assessment. The risk that leadership accepts should be the full envelope, not just the outcome.
- **Advocate**: this philosophy directly affects users. The "what could have happened" analysis should always include "what could have happened to user data, user trust, and user experience."
- **Sherpa**: your trail observation is the early warning system. Weak signals in user behaviour — and attacker behaviour — are exactly what this philosophy is looking for. Pay attention.
- **QA**: you are in a unique position to ask "how come this was possible?" about every bug, every test gap, every unexpected behaviour. That instinct is security expertise applied at the code level.
- **All roles**: this is not a security-team-only philosophy. Every role should be asking: "how come I just did this? How come it was possible for me to make this change without a test failing? How come I just accessed this data without being challenged?" The agents already have security knowledge baked in. Use it.
