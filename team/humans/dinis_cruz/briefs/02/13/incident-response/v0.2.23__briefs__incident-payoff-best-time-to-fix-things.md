# Incidents as the Best Time to Fix Things: The 5x Multiplier

**version** v0.2.23  
**date** 12 Feb 2026  
**series** Incident Handling (3 of 3)  
**see also** "The P3-as-P1 Philosophy" (1 of 3), "Running P3-as-P1 in Practice" (2 of 3)  

---

## The Punch Line

Here's the data from years of running P3-as-P1 incidents as a CISO: **whatever time you spend on an incident — two days, five days, a week, two weeks — you will get more done than in 5x that time under normal operations.**

Two weeks of incident work produces what would normally take two months. And not two months of smooth sailing — two months of advocacy, bridge-burning, forcing people to do things, navigating politics, and fighting for prioritisation.

This isn't an abstract claim. It's a consistent, observable pattern. Incidents are the single most productive periods in an organisation's life. Understanding why — and deliberately harnessing that productivity — is one of the most powerful things a security leader can do.

## Why Incidents Create Hyperproductivity

### 1. The Argument Disappears

Under normal operations, every improvement requires justification. "We should add monitoring for X" gets met with "what's the business case? What's the priority versus feature Y? Can it wait until next quarter?" The improvement might be obviously needed, but it competes with everything else for attention and resources.

During an incident, the improvement isn't theoretical — it's the thing that would have prevented or detected what just happened. There's no argument. The gap is visible, concrete, and urgent. The conversation shifts from "should we do this?" to "how fast can we do this?"

### 2. Alignment Materialises

In normal operations, getting alignment across roles is hard. Everyone has their own priorities, their own backlog, their own view of what matters. Cross-cutting improvements that touch multiple roles (like observability infrastructure) struggle to get coordinated attention.

During an incident, everyone is aligned on the same problem. The architect, the dev, the DevOps, the QA, the CISO — they're all looking at the same gap, at the same time, with the same context. Decisions that normally take weeks of meetings happen in minutes because everyone can see why.

### 3. Help Turns Up

There's a phenomenon in incident response: people who are normally too busy to help suddenly become available. Not because they're free — because they can see the real problem. The abstractness is gone. The thing is right there, and they want to help fix it.

In an agentic setup, this translates to: every role has the context to contribute meaningfully, because the incident provides the narrative that connects their domain to the problem. The dev can see why the monitoring gap matters. The designer can see why the error message confused the user. The cartographer can see why the dependency wasn't mapped.

### 4. Focus Replaces Prioritisation

Normal operations require constant prioritisation: what matters most? What can wait? What's the right sequence? This is necessary work, but it's also slow work — and it's subject to politics, changing priorities, and the gravitational pull of feature development.

An incident creates focus. The question isn't "what should we work on?" — it's "what do we need to fix to handle this?" The incident itself has done the prioritisation for you, by revealing the specific gaps that matter right now. You're not choosing from a menu of possible improvements. You're fixing the things that just failed.

## What Gets Fixed During Incidents

Every incident investigation surfaces a predictable set of gaps. These are the things that get fixed at 5x normal speed:

### Observability Gaps
"Why didn't we see this?" is the most common question in an incident. The answer is almost always: because we weren't logging it, or we were logging it but nobody was watching, or we were watching but the alert wasn't configured, or the alert fired but nobody knew what to do with it.

Each of these is a fixable gap. During the incident, the DevOps team adds the logging. The CISO configures the alert. The conductor adds the response to the runbook. Under normal operations, each of these would be a ticket that sits in the backlog for weeks. During the incident, they get done that day.

### Runbook Gaps
"What's the procedure for this?" If the answer is "there isn't one" or "there is, but it's out of date," that's a gap that gets filled during the incident — because the team just wrote the procedure by doing it. The historian and journalist captured the steps. The librarian catalogues it. Next time this happens, the runbook exists.

### Dashboard Gaps
"Where can I see this data?" If the answer requires SSHing into a server, grepping through raw logs, or asking someone who knows where to look — that's a dashboard that needs to exist. During the incident, the DevOps team builds it, because the investigation literally can't proceed without it.

### Knowledge Gaps
"Who knows how this works?" If the answer is one person (or nobody), that's a bus-factor problem that's been invisible until now. The incident forces knowledge transfer — the person who knows explains it to the team, the explanation gets documented, the knowledge becomes shared.

### Architecture Gaps
"How does this connect to that?" If the answer requires investigation rather than looking at a diagram, the cartographer has work to do. The incident forces the mapping that should have existed already, and it gets done because the investigation needs it.

### Test Gaps
"Why didn't a test catch this?" If the answer is "there's no test for this path" — QA now has a very concrete, very motivated reason to write that test. The test doesn't just cover the incident scenario; it covers the class of scenarios the investigation revealed.

## The "How Many Times Before?" Question

This is the most revealing question you can ask during an incident: **how many times has this happened before?**

There are three possible answers:

**"Never — this is the first time."** Good. This is genuinely new. Investigate it fully, understand it, build the detection and response for next time.

**"It's happened N times before."** Interesting. Why did it take until now to trigger an investigation? What was different this time? Why weren't the previous occurrences flagged? This usually reveals that the monitoring existed but the threshold was wrong, or the alert was firing but being ignored, or someone knew about it but didn't escalate.

**"We don't know."** This is the worst answer and the most valuable finding. If you cannot determine whether something has happened before, you have a fundamental gap in your observability. You don't have the data, or you don't have the tools to query it, or you don't have the retention period to look back far enough. This answer alone justifies the entire P3-as-P1 exercise, because it reveals a blind spot that affects everything, not just this incident.

When the answer is "we don't know," the incident investigation immediately produces a concrete requirement: build the capability to answer this question. That capability — the logging, the dashboard, the query, the retention policy — benefits every future incident and every future investigation. It's infrastructure that you'd never have prioritised through normal backlog grooming, but that the incident reveals as essential.

## The Known Unknowns vs Unknown Unknowns

There's a hierarchy of concern:

**Known knowns** — we know about this gap, we've assessed the risk, we have a plan. The incident may accelerate the plan, but it's not a surprise.

**Known unknowns** — we know we don't know something (e.g., "we haven't mapped all the places IP addresses appear in logs"). The incident may force us to finally do the mapping. Uncomfortable but manageable.

**Unknown unknowns** — we didn't even know this was a question. The incident reveals an entire category of risk we hadn't considered. This is where the real value of P3-as-P1 lives — it converts unknown unknowns into known unknowns (or known knowns) before the high-severity incident exploits them.

The things that keep a CISO awake at night aren't the known gaps (those are on the roadmap). They're the things they don't know they don't know. Every P3-as-P1 shrinks that space.

## Incidents and Self-Healing Systems

The end state — the thing all of this is building toward — is a system with such comprehensive visibility on the happy path that deviations are detected automatically.

The logic:

1. We know what normal looks like (because the sherpa's trail observation has mapped user behaviour, the DevOps monitoring has baselined system behaviour, the synthetic users have established performance benchmarks)
2. When something deviates from normal, we detect it immediately (because the alerting is calibrated from real incidents, not theoretical thresholds)
3. When we detect a deviation, we have a response (because previous incidents built the runbooks, the automation, and the knowledge)
4. In many cases, the response is automatic (because we've handled this pattern enough times to codify the response as automation)

This is self-healing. Not magic — just the accumulated result of many incidents, each one producing better visibility, better runbooks, better automation, and a smaller surface area of surprises.

The path to self-healing goes through P3-as-P1. Each incident teaches the system something. Each teaching becomes a detection. Each detection becomes a response. Each response becomes automation. Over time, what used to be an incident becomes an event, and what used to require human intervention happens automatically.

## The Trail Connection: Users and Attackers

The sherpa's trail observation capability connects directly to incident detection and investigation. Here's the insight: **attacker trails and user trails often overlap**.

When an attacker probes a system, they follow paths. Some of those paths are also followed by legitimate users who are confused, lost, or exploring. The behaviours look similar — unusual navigation patterns, repeated errors, accessing unexpected endpoints, abandoning flows midway.

The difference is intent, but the trail data looks the same. And here's the thing: before an attack exploits a weakness, a user has almost always already hit that weakness. They just experienced it as confusion or friction instead of a security breach. The signals were there. Someone clicked the wrong thing, hit an error, found an unintended path — and the system didn't notice, because nobody was watching the trails.

This means:

- **Trail anomalies are incident triggers**. When the sherpa detects unusual behaviour patterns, that's a potential P3-as-P1 trigger — investigate why that behaviour is possible, what it reveals about the system, and what an attacker could do with the same path.
- **User confusion signals security weakness**. If users consistently get lost at a particular point in the flow, an attacker will find that same confusion exploitable. Fixing the UX friction fixes the security weakness.
- **Every user journey that "shouldn't be possible" is an incident**. If a user reaches a state that the design doesn't account for — they shouldn't have been able to get there, but they did — that's an unknown unknown being revealed by real-world usage.

The sherpa and the CISO are natural allies in this model. The sherpa reads the trails. The CISO interprets the security implications. Together, they convert weak behavioural signals into investigated, understood, and resolved findings — before an attacker exploits them.

## Every Role as a Security Sensor

This is the cultural shift that P3-as-P1 enables: **every role becomes a security sensor, not just the security roles**.

The QA agent could be one of the fiercest security testers. Not because it's a security specialist, but because it's already asking: "does this work correctly? Is this the expected behaviour? What happens when I do something unexpected?" Those are security questions with the label filed off.

The dev agent should be asking: "how come I just committed this change and no test failed? That means either my change is trivial (unlikely) or the test coverage has a gap (likely). That gap is a finding."

The architect should be asking: "how come this component can access that data? The design didn't intend that. If I can reach it, an attacker can reach it."

The designer should be asking: "how come the user can reach this state? The flow wasn't designed for this. If a user can get confused here, an attacker can exploit the confusion."

The conductor should be asking: "how come this task had no dependency checks? We just deployed something that could have broken something else, and the sequencing didn't prevent it."

None of these are traditional security questions. All of them are security-relevant. In an agentic team, every role has security knowledge baked in — the models understand security concepts, vulnerabilities, and implications. The P3-as-P1 culture gives every role permission and encouragement to use that knowledge.

The question "how come this was possible?" should be the most natural question in the team's vocabulary. And every time it's asked, the answer should be investigated, documented, and — if warranted — run as a P3-as-P1.

## Capturing It All: Issues FS as the Incident Graph

The final piece: all of this works because we have the infrastructure to capture it. Issues FS gives us:

- **Incident databases** that capture every action, question, finding, and decision as linked issues
- **Replay capability** — walk through the exact sequence of events after the fact
- **Cross-incident analysis** — overlay multiple incident databases to see patterns (the same gap appearing in multiple incidents, the same role being the bottleneck, the same runbook being missing)
- **Improvement tracking** — every gap found becomes a task, linked to the incident that revealed it, tracked through to resolution
- **Timeline precision** — if every handover between roles is a ticket, the timeline is automatically captured. No retrospective reconstruction needed.
- **Zip-based archival** — incidents become portable databases that can be stored, shared, compared, and used as training material for new team members (or new agents)

The incident database isn't just a record of what happened. It's a replayable, queryable, linkable graph of the entire investigation — from trigger to finding to fix. It's the institutional memory that makes each subsequent incident faster, more focused, and more productive.

---

## The Summary

Incidents are not interruptions to productive work. Incidents ARE the most productive work. They create focus, alignment, and urgency that normal operations cannot match. They reveal the real gaps — not the theoretical ones, but the ones that just caused (or almost caused) a real problem. And they produce fixes at 5x the speed of normal backlog grooming.

The P3-as-P1 approach deliberately harnesses this productivity by treating minor incidents as major investigations. The cost in an agentic setup is low. The return is enormous. And the cumulative effect — incident after incident, each one building better visibility, better runbooks, better automation — is a system that progressively heals itself and a team that progressively surprises itself less.

Run the incidents. Follow the threads. Fix the things. The system gets better every time.

---

## For the Agents

- **Conductor**: after every P3-as-P1, the improvement backlog needs to be integrated into the normal work plan. These aren't optional nice-to-haves — they're gaps revealed by reality. Prioritise them accordingly. Track their resolution. Report on the backlog burn-down.
- **CISO**: maintain a cross-incident pattern analysis. After 3–5 incidents, look for the meta-patterns: which gaps keep appearing? Which roles are consistently under-resourced during incidents? Which runbooks are always missing? Those patterns reveal systemic issues that individual incidents can't see.
- **DevOps**: you are the primary producer of fixes during incidents. Every "why can't I see this?" becomes a logging/dashboard/alerting task that you deliver. Build the muscle to respond to these requests quickly — have templates for common monitoring additions, pre-built dashboard components, and standardised alerting patterns.
- **Sherpa**: your trail data is both an incident trigger and an investigation resource. Build the capability to answer: "show me all user behaviour for the last N hours at this flow stage." That query will be asked in every incident. Have it ready.
- **QA**: every incident should produce at least one new test. The test should reproduce the condition that triggered the incident. Over time, the test suite becomes a regression safety net built from real incidents, not theoretical scenarios.
- **Journalist**: incidents are stories. Each one has a narrative arc: trigger, investigation, discovery, resolution, lessons learned. Write them as stories, not just reports. These become the institutional memory that new team members (and new agents) learn from. Regular updates during active incidents. A wrap-up article when the incident closes. Follow-up articles when the improvement tasks are completed.
- **Historian**: maintain the incident register. Every P3-as-P1 gets an entry: date, trigger, severity (actual vs possible), key findings, improvement tasks generated, resolution status. This register becomes the data source for the cross-incident pattern analysis.
- **Librarian**: archive every incident database. Index them by trigger type, by roles involved, by gaps found. Make them searchable. When a new incident starts, the first question the CISO should ask is: "have we seen anything like this before?" — and the librarian should be able to answer immediately.
- **Advocate**: every incident has a user impact dimension, even when users weren't directly affected. "What would this have meant for users if it had been worse?" is a question the advocate should be asking in every incident. The answers feed into the full-envelope analysis and into the prioritisation of improvements.
- **Cartographer**: after every incident, update the blast radius maps. If the investigation revealed connections or dependencies that weren't previously mapped, add them. The maps should get richer after every incident.
- **DPO**: in every incident, ask: "was personal data involved? Could personal data have been involved if the incident had been worse?" This feeds into the full-envelope analysis and determines whether ICO notification obligations exist (or would exist in the worse scenario).
- **GRC**: after every incident, update the risk register. The full-envelope analysis may reveal risks that weren't previously registered, or may change the severity of existing risks. The register should be a living document that gets sharper with each incident.
- **Ambassador**: some incidents, once resolved and published, become trust-building content. "Here's how we found and fixed a weakness before it became a problem" is powerful market positioning for a security-focused product. Work with the journalist on external-facing versions of resolved incidents (appropriately sanitised).
