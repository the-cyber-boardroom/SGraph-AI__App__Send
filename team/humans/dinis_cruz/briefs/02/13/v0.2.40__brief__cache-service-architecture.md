# Brief: Cache Service Architecture — 13 Feb 2026

**Version:** v0.2.40
**Date:** 13 February 2026
**Focus:** How the MGraph-AI Cache Service integrates into SGraph Send
**Follows:** [Brief #2 Debrief](v0.2.33__debrief__daily-brief-2-responses-13-feb.md) | [v0.1.3 Final Debrief](v0.2.33__debrief__v0.1.3-role-reviews-final.md)

---

## What changed

The earlier Architect and Dev responses (v0.2.33) designed a cache service to be **built from scratch** — 25-35 hours of implementation for `Cache__Service__Send`, custom hash resolution, temporal storage, namespace management, and aggregation pipelines.

That was wrong. The **MGraph-AI Cache Service already exists** — it's a production-ready, serverless caching system (v0.6.0+) with a Python client library (v0.10.1), deployed at `cache.dev.mgraph.ai`, with exactly the capabilities the Architect specified: content-addressable storage, multiple strategies (direct, temporal, temporal_latest, temporal_versioned, key_based), hierarchical child data, namespace isolation, and the same `Storage_FS` / `Memory_FS` / `Type_Safe` / `Cache__Hash__Generator` foundation that SGraph Send already uses.

**The implementation effort drops from 25-35 hours to a thin wrapper + LETS aggregation logic.**

---

## The Architecture: In-Memory Client, Same S3 Bucket

### Integration Model

The cache service runs **in-process** inside the SGraph Send Lambda, using `IN_MEMORY` execution mode (FastAPI `TestClient`, zero network latency). It shares the same S3 bucket as the existing transfer storage, differentiated by prefix:

```
SGraph Send Lambda (single process)
│
├── Transfer__Service                                    ← mirrors lambda__user
│   └── Storage_FS__S3(s3_bucket="...", s3_prefix="user/")
│       └── user/transfers/{transfer_id}/meta.json
│       └── user/transfers/{transfer_id}/payload
│
└── Send__Cache__Client  (thin wrapper, like Html_Cache__Client)
    └── Cache__Service__Client(mode=IN_MEMORY)           ← mirrors lambda__admin
        └── FastAPI TestClient (same process, no network)
            └── Cache Service App
                └── Storage_FS__S3(s3_bucket="...", s3_prefix="admin/")
                    └── admin/analytics/...
                    └── admin/tokens/...
                    └── admin/costs/...
                    └── admin/transfers/...
```

The S3 prefix structure mirrors the Lambda architecture:

| Lambda | S3 Prefix | Data Domain |
|--------|-----------|-------------|
| `lambda__user` | `user/` | What users create (transfers, encrypted payloads) |
| `lambda__admin` | `admin/` | What the platform manages (analytics, tokens, costs, event caches) |

Both the cache service and client are available on PyPI:
- **Service:** [mgraph-ai-service-cache](https://pypi.org/project/mgraph-ai-service-cache/)
- **Client:** [mgraph-ai-service-cache-client](https://pypi.org/project/mgraph-ai-service-cache-client/)

### Why IN_MEMORY mode (not REMOTE)

| Consideration | IN_MEMORY | REMOTE |
|---------------|-----------|--------|
| **Latency** | Zero — same process | HTTP round-trip per call |
| **Analytics recording** | No latency added to user requests | Adds ~50-100ms per request |
| **Infrastructure** | No separate Lambda needed | Requires deployed cache Lambda + auth |
| **Testing** | `Storage_FS__Memory` — same pattern as existing tests | Needs mock or running service |
| **Future extraction** | Flip to REMOTE mode, point at cache Lambda | Already there |

The critical benefit: **when it's time to extract the cache into its own Lambda, the only change is switching `IN_MEMORY` → `REMOTE` and providing the service URL.** The `Send__Cache__Client` wrapper, all the application logic, and all the tests stay identical.

### S3 Layout: Same Bucket, Prefix Mirrors Lambda Architecture

The existing `Storage_FS__S3` class already supports `s3_prefix`. The transfer service gets `s3_prefix="user/"`, the cache service gets `s3_prefix="admin/"`. The prefixes mirror the Lambda split:

```
S3 Bucket: {account_id}--sgraph-send-transfers--{region}

user/                                   ← lambda__user domain
    transfers/
        {transfer_id}/
            meta.json
            payload

admin/                                  ← lambda__admin domain (cache service)
    analytics/
        data/
            temporal/                   ← Raw events (one file per request)
                2026/02/13/14/30/
                    {cache_id}.json
            key-based/                  ← Aggregations (predictable paths)
                aggregations/daily/
                    2026-02-13.json
                aggregations/hourly/
                    2026-02-13-14.json
                pulse/
                    latest.json
    tokens/
        data/
            key-based/                  ← Token metadata
                community-x/
                    community-x.json
            (child data)/               ← Token usage events
                {cache_id}/data/
                    events/2026/02/13/
                        {event_id}.json
    costs/
        data/
            key-based/                  ← Cost aggregations
                daily/2026-02-13.json
                monthly/2026-02.json
    transfers/
        data/
            (child data)/               ← Per-file event cache
                {cache_id}/data/
                    events/...
```

Zero new S3 buckets. Zero new IAM policies. The old `transfers/` root folder contains only test data — delete it and start clean with `user/transfers/`.

---

## Strategy Mapping: Cache Service Features → SGraph Send Needs

| SGraph Send Need | Cache Service Strategy | Why This Strategy |
|---|---|---|
| **Analytics raw events** (one per request) | `TEMPORAL` | Time-organized, one file per event, never overwritten. Perfect for append-only event streams |
| **Analytics aggregations** (daily, hourly, etc.) | `KEY_BASED` | Predictable paths like `aggregations/daily/2026-02-13`. Readable, queryable by time window |
| **Analytics pulse** (real-time) | Computed on-demand, stored via `TEMPORAL_LATEST` | Always recomputed from last 5 min of temporal raw events. Latest pointer for fast reads |
| **Token metadata** (`community-x` → config) | `KEY_BASED` | Lookup by human-friendly name as `cache_key`. Hash computed from `cache_key` field via `json_field_path` |
| **Token usage events** | Child data under token's `cache_id` | `data__store_json` with `data_key="events/2026/02/13"` — hierarchical under parent |
| **Cost data** (daily, weekly, monthly) | `KEY_BASED` | Predictable paths, same pattern as analytics aggregations |
| **Transfer event cache** | Child data under transfer `cache_id` | Per-file events stored hierarchically, cross-referenced with analytics stream |

---

## What to Build: Send__Cache__Client

### The Pattern: Follow Html_Cache__Client

The `Html_Cache__Client` (from the MGraph-AI HTML service) is the exact template. It's a `Type_Safe` wrapper that:

1. Holds a `Cache__Service__Client` and a `Cache__Hash__Generator`
2. Provides domain-specific methods (`entry__store`, `entry__retrieve`, `data__store_json`, etc.)
3. Uses `@type_safe` decorators on every method
4. Maps domain operations to the right cache service strategy and namespace

`Send__Cache__Client` follows the same pattern:

```python
class Send__Cache__Client(Type_Safe):
    cache_client   : Cache__Service__Client          # IN_MEMORY mode
    hash_generator : Cache__Hash__Generator

    NS_ANALYTICS   : str = 'analytics'
    NS_TOKENS      : str = 'tokens'
    NS_COSTS       : str = 'costs'
    NS_TRANSFERS   : str = 'transfers'

    # ── Token Operations ──────────────────────────────────
    def token__create(self, name, limit): ...         # KEY_BASED store
    def token__lookup(self, name): ...                # Retrieve by cache_key hash
    def token__use(self, name, ip_hash, action): ...  # Child data store + increment
    def token__revoke(self, name): ...                # Update entry
    def token__list(self): ...                        # Namespace listing

    # ── Analytics Operations ──────────────────────────────
    def analytics__record_event(self, event): ...     # TEMPORAL store (every request)
    def analytics__pulse(self, window_minutes=5): ... # Compute from recent temporal data
    def analytics__aggregation(self, window_type, window_key): ...  # LETS pipeline

    # ── Cost Operations ───────────────────────────────────
    def cost__record(self, date, cost_data): ...      # KEY_BASED store
    def cost__aggregation(self, window_type, key): ...# Read or compute

    # ── Transfer Event Operations ─────────────────────────
    def transfer__record_event(self, transfer_id, event): ...  # Child data + analytics dual-write
    def transfer__summary(self, transfer_id): ...              # Aggregate per-transfer events
```

### Integration Point in Existing Code

```python
# Fast_API__SGraph__App__Send__User.setup()

# Updated: transfer service gets user/ prefix
self.send_config = Send__Config()
storage_fs = self.send_config.create_storage_backend()    # s3_prefix="user/"
self.transfer_service = Transfer__Service(storage_fs=storage_fs)

# New: cache client with admin/ prefix (same bucket)
self.send_cache = Send__Cache__Client(
    cache_client = Cache__Service__Client(mode=IN_MEMORY, ...),   # s3_prefix="admin/"
    ...
)

# New: pass cache client to routes for analytics recording
self.add_routes(Routes__Transfers,
                transfer_service = self.transfer_service,
                send_cache       = self.send_cache      )
```

### What's Custom (the LETS Pipeline)

The cache service handles all storage. The **LETS aggregation logic** is the only substantial custom code — ~200-300 lines:

1. **Record**: on every request, write a raw analytics event via `TEMPORAL` strategy
2. **Pulse**: read last 5 minutes of temporal files, count, return (always recomputed)
3. **Aggregate**: read raw events for a time window → count/sum/average → store result via `KEY_BASED`
4. **Cascade**: hourly = combine 2 × 30-min; daily = combine 24 × hourly (read existing aggregations, combine, save)
5. **Cache check**: if aggregation file exists and window is closed (past), return it; don't recompute

This is the same logic the Architect specified — but it's operating through cache service API calls, not custom Storage_FS path generation.

---

## What the Earlier Documents Got Right vs. Wrong

### Got Right (keep these)

| Element | Source | Status |
|---|---|---|
| 4 namespaces: analytics, tokens, costs, transfers | Architect D026 | ✅ Maps directly to cache service namespaces |
| LETS pipeline: on-demand with cascading save | Architect D027 | ✅ Custom logic, runs on top of cache service storage |
| Human-friendly token names via hash | Architect D028 | ✅ `KEY_BASED` strategy with `json_field_path='cache_key'` — exact match |
| Token lifecycle (create → use → exhaust → revoke) | Architect Brief #2 | ✅ Entry operations + child data events |
| Pulse as always-recomputed rolling window | Architect Brief #2 | ✅ Read temporal files, compute, optionally save via `TEMPORAL_LATEST` |
| Dual-write for transfer events | Architect Brief #2 | ✅ Write to both analytics raw stream and transfer child data |
| Same S3 bucket with prefix separation | Architect D026 | ✅ `s3_prefix="admin/"` on cache service, `s3_prefix="user/"` on transfer service — mirrors Lambda architecture |
| All Type_Safe schemas (raw events, aggregations, tokens, costs) | Architect Brief #2 | ✅ Still valid as data models — they're what gets stored/retrieved |

### Got Wrong (superseded)

| Element | Earlier Plan | Actual Approach |
|---|---|---|
| Build `Cache__Service__Send` from scratch | Dev plan: 25-35 hours, custom hash resolution, custom temporal write, custom namespace setup | **Thin wrapper** over `Cache__Service__Client` (like `Html_Cache__Client`). ~4-6 hours |
| Custom `Storage_FS` path generation | Architect: detailed file tree layouts with manual path construction | **Cache service handles all path generation** — strategies do this automatically |
| Custom hash-to-ID resolution with sharded directories | Architect: `refs/by-hash/a3f7/b2c1/9d4e8f01.json` | **Built into cache service** — content-addressable storage with automatic deduplication and hash sharding |
| Dev Q1/Q2 (need reference code, need osbot-utils walkthrough) | Dev: blocking questions | **Answered** — the reference is `Html_Cache__Client.py` and the client library docs |
| 25-35 hour implementation estimate | Dev plan across 3 weeks | **~8-12 hours total**: wrapper (~4h), LETS pipeline (~4-6h), admin endpoints (~2-4h) |

---

## Revised Implementation Sequence

### Phase 1: Wrapper + Raw Events (one session, ~4-6 hours)

| # | Task | Deliverable |
|---|------|-------------|
| 1 | `Send__Cache__Client` scaffolding: configure `Cache__Service__Client` in `IN_MEMORY` mode, wire S3 backend with `admin/` prefix, set up 4 namespaces | Thin wrapper class with health check |
| 2 | Analytics raw event recording: FastAPI middleware writes one event per request via `TEMPORAL` strategy | Every request produces a JSON file in `admin/analytics/` |
| 3 | Pulse computation: read recent temporal files, count, return | `/health/pulse` endpoint answers "is anyone using the site?" |
| 4 | Token CRUD: create/lookup/use/revoke via `KEY_BASED` + child data | Token system operational |

**Phase 1 deliverable:** Server-side analytics recording + pulse (minimum viable GA replacement) + token system. Answers the two critical questions: "is anyone using the site?" and "can we onboard friendlies with tokens?"

### Phase 2: LETS Aggregations + Admin (follow-on, ~4-6 hours)

| # | Task | Deliverable |
|---|------|-------------|
| 5 | 30-min and hourly aggregation: read temporal raw events, compute, save via `KEY_BASED` | "How many people today?" |
| 6 | Daily/monthly cascading: combine lower-granularity aggregations | Historical analytics |
| 7 | Admin REST endpoints: token management + analytics queries | Admin Lambda serves dashboard data |
| 8 | Transfer event dual-write: record to both analytics stream and transfer child data | Per-file analytics |

### Phase 3: Admin UI + Cost Tracking (future sprint)

| # | Task | Deliverable |
|---|------|-------------|
| 9 | Admin UI v0.1.1: token management panel (IFD) | Create, list, revoke tokens |
| 10 | Admin UI v0.1.2: analytics dashboard (IFD) | Pulse view, daily/hourly charts |
| 11 | AWS cost collector + cost aggregations | Unit economics |

---

## Decisions Updated

| # | Decision | Original | Updated |
|---|----------|----------|---------|
| D024 | Cache service as layer on Memory-FS | Build custom `Cache__Service__Send` | **Use existing MGraph-AI Cache Service via `Cache__Service__Client` in `IN_MEMORY` mode** |
| D025 | Reuse `Cache__Hash__Generator` | Import and wire manually | **Already integrated in cache service** — just configure `json_field_path` for field-level hashing |
| D026 | Namespace separation | Custom directory creation | **First-class cache service feature** — namespace parameter on every API call |
| D027 | On-demand LETS pipeline | Custom implementation on Storage_FS | **Custom implementation on top of cache service API** — storage handled, compute logic is ours |

Decisions D028-D030 (human-friendly tokens, cost namespace, GA removal gate) remain unchanged.

**New decision:**

| # | Decision | Recommendation |
|---|----------|----------------|
| D034 | Cache service runs in-process (`IN_MEMORY` mode) with `s3_prefix="admin/"`. Transfer service gets `s3_prefix="user/"`. Prefixes mirror Lambda architecture (`lambda__admin`, `lambda__user`). Extraction to separate Lambda is a configuration change (flip to `REMOTE`). | **APPROVE** |

---

## Risk Assessment Update

| Earlier Risk | Status | Notes |
|---|---|---|
| RF-15: Cache service schema delay | **SUPERSEDED** | No custom schema needed — cache service already exists |
| RF-18: LETS aggregation correctness | **Still open** | This is the remaining custom logic — needs tests |
| RF-21: Scope expansion (5 cache consumers) | **Reduced** | Cache service handles complexity; wrapper is thin |
| RF-23: Zero analytics (P0) | **Faster to resolve** | Phase 1 delivers raw events + pulse in one session |

| New Risk | Severity | Mitigation |
|---|---|---|
| Cache service client as new dependency in Lambda | Medium | `IN_MEMORY` mode means no network dependency. Both packages on PyPI: [mgraph-ai-service-cache](https://pypi.org/project/mgraph-ai-service-cache/) and [mgraph-ai-service-cache-client](https://pypi.org/project/mgraph-ai-service-cache-client/). Add to `APP__SEND__USER__LAMBDA_DEPENDENCIES` |
| Lambda package size increase | Low | Cache service client is lightweight. Monitor cold start times |
| Shared S3 bucket: cache operations could interfere with transfer operations | Low | Prefix isolation (`admin/` vs `user/`). Cache service never writes outside its prefix. Mirrors Lambda boundary |

---

## Summary

The MGraph-AI Cache Service is already built and available on PyPI ([service](https://pypi.org/project/mgraph-ai-service-cache/), [client](https://pypi.org/project/mgraph-ai-service-cache-client/)). SGraph Send needs a thin wrapper (`Send__Cache__Client`, modelled on `Html_Cache__Client`) and custom LETS aggregation logic. The integration uses `IN_MEMORY` mode (zero latency, same process) with the same S3 bucket — `user/` prefix for transfers, `admin/` prefix for cache data, mirroring the Lambda architecture. The estimated effort drops from 25-35 hours to ~8-12 hours. The migration path to a separate cache Lambda is a configuration change.

**Next step:** Build `Send__Cache__Client` wrapper and wire raw analytics event recording into the FastAPI middleware. That's the gating item — once events are flowing, everything else (pulse, aggregations, tokens, admin UI) cascades from it.

---

---

## Reference Documentation for Agents

Before implementing the cache service integration, read these two documents in the SGraph Send repo:

| Document | Location | What it covers |
|----------|----------|----------------|
| **Cache Service LLM Brief** | `library/dependencies/cache-service/v0.5.68__cache-service__llm-brief.md` | Service architecture, storage strategies (direct, temporal, temporal_latest, temporal_versioned, key_based), API endpoints, namespace isolation, all enums |
| **Cache Client How-To-Use** | `library/dependencies/cache-service/v0.6.0__v0.10.1__cache_service__client__how_to_use.md` | Python client library, three execution modes (REMOTE, IN_MEMORY, LOCAL_SERVER), store/retrieve/data operations, real-world examples, testing patterns |
| **Html_Cache__Client (reference implementation)** | `team/humans/dinis_cruz/code-example/cache-service/Html_Cache__Client.py` | The pattern to follow for `Send__Cache__Client` — thin `Type_Safe` wrapper with `@type_safe` decorators, domain-specific methods over `Cache__Service__Client`, `Cache__Hash__Generator` integration, entry/data/cache/namespace operations |

The `Html_Cache__Client.py` demonstrates the exact wrapper pattern: entry CRUD, child data operations, cache hash lookups, namespace listing — all delegating to `Cache__Service__Client` methods with type-safe signatures.

---

*This brief supersedes the "build from scratch" framing in the Architect's cache service schema (v0.2.33) and the Dev's implementation plan (v0.2.33). The schemas and data models from those documents remain valid — they describe what gets stored. The change is how it gets stored: through an existing service ([mgraph-ai-service-cache](https://pypi.org/project/mgraph-ai-service-cache/) + [mgraph-ai-service-cache-client](https://pypi.org/project/mgraph-ai-service-cache-client/)), not custom infrastructure. The `admin/` / `user/` S3 prefix design mirrors the Lambda architecture and provides a clean extraction path.*
