# Daily Brief #2: 13 February 2026

**version** v0.2.32  
**date** 13 Feb 2026  
**from** Human (project lead)  
**to** All roles via Conductor  
**follows** Daily Brief #1 (first user feedback, UX changes, observability pipeline, admin tokens)  

---

## Theme: No Cookies. Zero. None.

Adding Google Analytics to the site was a useful experiment — it reminded me how much Google Analytics sucks. For what it does, for the implications, and for the fact that it forces us to have cookie tracking, consent banners, and all the GDPR complexity that comes with it — it's not worth it. It breaks our promise to users. It allows third-party tracking. It adds complexity. And fundamentally, it's front-end data — it's what the client decides to tell us, not a true representation of what's happening. It can be gamed. It's not even accurate.

We're ditching it. We're replacing it with something better, built by us, from the server side.

### The No-Cookie Policy

The target state is simple: **we have no cookies. None.** And because we have no cookies, we don't need cookie banners. We don't need consent flows. We don't need the PECR compliance overhead. We can have a clean, honest policy that says "this site uses no cookies" — and we can prove it.

**DPO**: do a formal analysis of Google Analytics and its data protection implications. The conclusion should be that we should remove it — it breaks our zero-tracking promise, it sends data to Google (third-party processing, international data transfer implications), and it requires cookie consent infrastructure that adds complexity and erodes user trust.

**GRC**: support the DPO analysis with a risk assessment. The risk of keeping GA (compliance burden, user trust erosion, third-party data exposure) versus the risk of removing it (we need to build our own analytics — which we're doing anyway).

**Advocate**: review from the user perspective. A site with zero cookies is a trust signal. "We don't track you. We don't use cookies. We can prove it." That's powerful positioning for a privacy-focused product.

**Ambassador**: this becomes a market differentiator. Most sites drown users in cookie banners. We don't have one. That's a story worth telling.

---

## The Cache Service: Our Data Backbone

Daily brief #1 described the observability pipeline as "collect → store in S3 → visualise." This brief introduces the specific technology we're going to use for the storage and query layer: the **cache service**.

The cache service is built on top of memory FS (which we're already using for file storage). It introduces properties that solve a bunch of hard problems we're already hitting around data organisation, retrieval, and aggregation.

### How It Works

**Cache ID**: every entry in the cache service has a cache ID — think of it as the central gravity for that entry. It's a unique identifier that lets you find and access everything stored against that entry (files, data, metadata, events). The cache ID resolves to a file tree — a folder structure where you can store anything.

**Cache Hash**: a parallel lookup mechanism. The cache hash is derived from a value you control, and it resolves to a cache ID. This is the key insight — it lets you find a cache entry using a human-meaningful value instead of a GUID.

**Example — token resolution**: when we create an invitation token like `community-x` or `football-123` or `abc-2025`, we hash that token string to produce a cache hash. The cache hash resolves to a cache ID. The cache ID contains all the data about that token — its usage count, its limit, its creation date, who used it, every event associated with it.

This means tokens can be **human-friendly and brand-friendly**. No GUIDs. No random strings. If I'm giving a token to a community, I want it to be something memorable and meaningful, not `a3f7b2c1-9d4e-...`.

### What We Store in Cache Entries

Each cache entry can hold:

- **JSON or text data** — configuration, metadata, event records
- **Binary data** — the actual encrypted file contents (bytes). Perfect for our encrypted file storage
- **Data files in a folder structure** — we can recreate a full directory tree inside a cache entry. This means events, activities, logs, and any supplementary information about a specific file or token can live inside its cache entry

### Temporal Storage

The cache service has a built-in **temporal mode** that automatically saves files on a date path: `year/month/day/hour`. It also maintains a **latest** folder that always contains the most recent version.

When you write a file, it can be saved in both places simultaneously:
- The temporal path (historical record — never overwritten)
- The latest path (current state — always overwritten)

This gives us the best of both worlds: fast access to "what's happening right now" via latest, and complete historical record via the temporal path.

### The LETS Principle: Load, Extract, Transform, Save

This is the data processing philosophy for the entire analytics pipeline:

**Every request, every event, every action writes a file to the cache service.** That's the raw data layer. Save everything.

**When you query, save the result.** This is the critical optimisation. If I ask "what happened in the last 30 minutes?", the system reads all the raw files from the last 30 minutes and produces an aggregation. That aggregation gets saved. Next time I ask a similar question, the system only needs to process the files since the last saved aggregation — not reprocess everything.

**Aggregations cascade.** If I have a 30-minute aggregation, building an hourly aggregation only requires combining two 30-minute aggregations. A daily aggregation combines 24 hourly ones. A monthly combines daily ones. You only ever pay the cost of creating a specific granularity once.

```
RAW DATA (every request saves a file)
    │
    ▼
30-MINUTE AGGREGATION (calculated once, then saved)
    │
    ▼
HOURLY AGGREGATION (combines 2 × 30min, saved)
    │
    ▼
DAILY AGGREGATION (combines 24 × hourly, saved)
    │
    ▼
MONTHLY / YEARLY (combines daily, saved)
```

The fundamental principle: **optimise for read, save when needed, cache the queries.** Reads should be near-instant because you're always reading pre-computed aggregations, never reprocessing raw files. This scales spectacularly — whether you have 100 requests a day or 100,000.

---

## Replacing Google Analytics: Server-Side Site Analytics

With the cache service as our backbone, we build our own analytics. Call it **Site Analytics** (or similar — we'll name it properly).

### What It Does

Everything Google Analytics does that we actually care about, but done properly from the server side:

- **Real-time pulse**: what's happening on the site right now? (GA isn't even real-time — it's near-real-time. We can do better.)
- **Historical traffic view**: what happened over the last hour, day, week, month?
- **Per-file analytics**: how many times was each file viewed, downloaded, etc.? (We already log these events — now we aggregate them)
- **No cookies, no client-side tracking, no third-party data sharing**

### How It Works

Every server-side event (CloudFront request, Lambda invocation, S3 access, API call) writes a file to the cache service. The LETS pipeline aggregates them. The UI reads the aggregations.

Source data:
- **CloudFront access logs** (with IP addresses masked — DPO requirement from daily brief #1)
- **Lambda execution logs** (request metadata, timing, errors)
- **Application events** (file created, file viewed, file downloaded, token used, etc.)

No client-side JavaScript analytics. No cookies. No consent required. Server-side truth.

**DevOps**: the CloudFront logging brief requested in daily brief #1 feeds directly into this. We need those logs enabled and flowing into the cache service. Make sure IP masking is configured — the DPO needs to confirm before we enable.

**Architect**: design the cache service schema for site analytics. What cache entries do we create? What temporal granularity? What aggregation levels? How does the LETS pipeline run (on-demand, scheduled, triggered)?

**Dev**: implement the analytics collection and aggregation pipeline using the cache service.

---

## Token System: Human-Friendly with Usage Limits

This expands on the admin token system from daily brief #1. With the cache service, we can now define the full token architecture:

### Token Properties

Each token should have:
- **A human-friendly name**: `community-x`, `football-123`, `beta-2025`, `do-not-share` — not GUIDs
- **A usage limit**: 5, 10, 20, 100, 1,000, 10,000 uses. This controls abuse, tests the commercial workflow, and lets us give different allocations to different communities
- **A cache entry**: resolved via cache hash from the token string. Contains: usage count, usage limit, creation date, every usage event, any metadata

### The Flow

1. Admin creates a token with a name and a usage limit
2. The token name is hashed → cache hash → resolves to cache ID
3. Cache entry stores the token metadata (limit, creation date, etc.)
4. Token is shared as a URL (from daily brief #1)
5. When a user presents the token, the system hashes it, finds the cache entry, checks the usage count against the limit, increments if allowed, records the event
6. All token usage data is available for analytics and dashboard display

### Why This Matters Beyond Access Control

This is the foundation for the commercial model. Credits, budgets, per-community allocations, per-partner quotas — they all follow this pattern. We're not building a billing system yet, but we're building the infrastructure that billing will sit on. Every token with a usage limit is a prototype of a paid plan.

**Architect**: design the token-to-cache-hash resolution. **Dev**: implement. **Ambassador**: think about how token allocations map to community engagement and growth strategy — giving a community 1,000 tokens is a partnership, not just access control.

---

## Local Storage Transparency Panel

We should add a UI panel that shows the user exactly what we're storing locally in their browser.

### What We Might Store Locally

- Encryption keys the user has used (so they don't have to re-enter them)
- The invitation token that was used to connect
- User preferences (theme, language)

### The Principle: Optional and Visible

- **Everything stored locally should be visible** in a transparency panel. The user can open it and see exactly what's in their browser storage — no hidden state.
- **Key storage should be optional.** The user can choose whether to cache encryption keys locally. If they don't, nothing is stored — true zero-state. If they do, they can see exactly what's cached and clear it at any time.
- **We should explore a mode with zero local storage.** Nothing saved. Every session is stateless. The user enters everything fresh each time. This is the maximum-privacy option.

**Designer**: design the transparency panel UI. It should feel like opening the hood of a car — "here's exactly what's inside, nothing hidden." **Dev**: implement. **DPO**: review the data protection implications of local key storage — what are the risks? What disclosures are needed? **Advocate**: this is a trust feature. Users who care about privacy (our core audience) will love being able to see exactly what's stored.

---

## AWS Cost Capture

Same LETS principle, different data source: we should be downloading and tracking our AWS costs.

- Collect the cost data (same collector pattern — don't download what we already have)
- Store in the cache service
- Aggregations: daily, weekly, monthly cost
- Correlate with traffic (cost per request, cost per file transfer, cost per active user)

This gives us unit economics visibility from day one. As we scale, we'll know exactly what each user costs us.

**DevOps**: add AWS cost data to the collection pipeline. **Architect**: design the cost cache entry schema. **GRC**: cost data feeds into the financial risk model.

---

## Admin UI: Modular Event-Driven Architecture

There's already shared code for a more advanced modular UI — event-driven, multi-module, dynamically loaded. Let's implement this on the **admin side first** (not the user-facing site), because:

- The admin UI is where we need the most flexibility (dashboards, token management, analytics, event logs)
- We can iterate faster without worrying about user-facing polish
- Once it's working on admin, we can selectively bring modules to the user-facing UI

**Dev**: implement the modular admin UI using the shared code. Event-driven module loading. **Designer**: design the admin layout — this is an internal tool, but it should still be usable and clear.

---

## IFD: Iterative Flow Development — The Methodology

**This is critical and non-negotiable.** All client-side development must follow the IFD (Iterative Flow Development) methodology.

The core rules:

- **Never override existing HTML/UI.** Each change creates a new version. The previous version must remain accessible. We should always be able to go back to any previous UI version instantly.
- **Minor versions reuse as much as possible.** Don't rebuild — extend. The minor version inherits from the previous version and adds or modifies only what's needed.
- **Major versions consolidate.** When we've accumulated enough minor versions, we consolidate into a clean major version that becomes the new baseline.
- **Every change has a version number.** No unnamed changes. No "quick fixes" that bypass versioning.

There's existing guidance on IFD — follow it. The point is iterative development with full rollback capability. We never lose a working UI. We never have to guess what changed. And we can always show a user any previous version of the experience.

**Dev**: this applies to all UI work from this brief forward — the admin UI, the transparency panel, the landing page improvements, the theme system, the analytics dashboard. All of it follows IFD. **Conductor**: enforce IFD compliance in the work sequencing. If a PR doesn't follow IFD, it doesn't merge. **QA**: verify that previous versions remain accessible after each new version is deployed.

---

## How This Connects to Daily Brief #1

| Brief #1 Topic | Brief #2 Evolution |
|---|---|
| Google Analytics integration | **Replaced.** We're removing GA and building server-side analytics instead |
| Observability pipeline (collect → S3 → visualise) | **Refined.** The cache service is the storage layer, LETS is the processing pattern, aggregations are the query model |
| Admin token creation UI | **Expanded.** Tokens are now human-friendly, cache-hash-resolved, with usage limits — the foundation for the commercial model |
| DevOps brief on AWS settings | **Still needed.** CloudFront logs feed the server-side analytics. IP masking is required before enabling |
| Expose file event data in UI | **Confirmed.** Cache service stores per-file events, UI reads aggregations |
| DPO review of event data | **Expanded.** DPO now also formally reviews and recommends removal of Google Analytics |

Note: the hash-in-URL proposal, text input, multilingual support, themes, and email sharing from brief #1 are all unchanged and still active priorities.

---

## Task Summary

| Priority | Task | Owner(s) |
|----------|------|----------|
| **P1** | DPO + GRC formal analysis: recommend removing Google Analytics | DPO, GRC |
| **P1** | Design cache service schema for site analytics, tokens, and file storage | Architect |
| **P1** | Implement cache service integration (analytics collection, LETS pipeline) | Dev |
| **P1** | Remove Google Analytics from the site once replacement is live | Dev |
| **P1** | Follow IFD methodology for all UI changes (non-negotiable) | Dev, Conductor, QA |
| **P2** | Implement human-friendly token system with usage limits via cache hash | Dev, Architect |
| **P2** | Build local storage transparency panel | Dev, Designer |
| **P2** | Implement modular event-driven admin UI (from shared code) | Dev, Designer |
| **P2** | Add AWS cost data collection to the pipeline | DevOps |
| **P2** | Enable CloudFront logs with IP masking (coordinate with DPO) | DevOps, DPO |
| **P3** | Explore zero-local-storage mode (full stateless sessions) | Dev, DPO |
| **P3** | DPO review of local key caching — privacy implications and disclosures | DPO |

---

## For the Conductor

Brief #1 and brief #2 are now a paired set for today. The big shift from brief #1 to brief #2: **Google Analytics is out, server-side analytics via the cache service is in.** This simplifies the DPO's job (no third-party data processor to assess) and strengthens our privacy posture (no cookies, no banners, no consent flows).

The cache service is the central new technical component. It underpins: file storage, token management, site analytics, event logging, and AWS cost tracking. The architect needs to design the schema before the dev can implement. That's the gating item for this brief.

IFD compliance is non-negotiable. Every UI change versions properly. No exceptions. QA verifies rollback capability.

The removal of Google Analytics should happen as soon as the server-side replacement can provide the basic pulse (real-time-ish traffic view + historical counts). It doesn't need to be feature-complete with GA — it just needs to answer "is anyone using the site right now?" and "how many people used the site today?" Once it can do that, GA comes out.

Let's build this properly.
