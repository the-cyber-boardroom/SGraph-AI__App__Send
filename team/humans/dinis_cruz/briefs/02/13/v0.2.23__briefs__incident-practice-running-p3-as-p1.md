# Running P3-as-P1 in Practice: The Operational Playbook

**version** v0.2.23  
**date** 12 Feb 2026  
**series** Incident Handling (2 of 3)  
**see also** "The P3-as-P1 Philosophy" (1 of 3), "Incidents as the Best Time to Fix Things" (3 of 3)  

---

## What P3-as-P1 Means Operationally

**P3**: from a business impact perspective, the incident is low severity. The actual damage is contained. Nothing is on fire. Nobody is paging you at 2am.

**As P1**: we run the full incident response process anyway. Full team mobilisation. Full investigation. Full analysis of what happened, what could have happened, and what needs to change.

The point is not that the incident is severe. The point is that the process of running a full response is where the value lives — in revealing the gaps, exercising the team, and building the muscle memory that you'll need when a real P1 hits.

## Why the First One Is Revelatory

The first time you run a P3-as-P1, the team discovers how dramatically unprepared they are. Not for the incident — for the logistics of responding to one.

In real-world experience, the first exercise reveals that people don't know:

- Where to convene (what channel, what room, what system)
- How to coordinate across roles (who talks to whom, in what order)
- How to think strategically about an unfolding situation (versus just reacting)
- Who to assemble (which roles are needed, who has the relevant knowledge)
- Where the data is (logs, dashboards, access records, deployment history)
- How to get data from systems they don't normally touch
- What runbooks exist (usually very few, and the ones that exist are outdated)
- How to maintain situation awareness (who knows what, when)

Every one of these gaps is a finding. Every finding is an improvement opportunity. After the first P3-as-P1, you have a concrete, prioritised list of infrastructure, process, and knowledge gaps — derived from reality, not from a theoretical risk assessment.

## The Agentic Advantage

In a traditional organisation, running a P3-as-P1 is expensive. You're pulling people off their normal work. Half the team is in an incident room instead of shipping features. There's a real opportunity cost.

In our agentic setup, the cost model is fundamentally different:

- **Parallel team structures**: you can spin up a complete separate team of agents focused on the incident without disrupting the normal work. The "incident team" operates alongside the "business as usual" team.
- **No context-switching penalty**: agents don't lose half a day getting back into flow after an interruption. You invoke them with the incident context, they work, they produce outputs.
- **Full expertise available simultaneously**: you can have the CISO, DevOps, AppSec, GRC, DPO, sherpa, journalist, and historian all working the incident at the same time, each from their own perspective.
- **Perfect documentation**: if the incident runs inside Issues FS, every action, every question, every finding, every handover is captured automatically. No meeting notes to write up. No "what did we agree?" conversations the next day.

This means we can run P3-as-P1 exercises frequently — far more frequently than a traditional organisation could afford — and the learning compounds.

## What Triggers a P3-as-P1

Not everything should be an incident. The triggers should be:

### Things That Surprised Us
Something happened that we didn't expect. Not necessarily bad — just unexpected. "How come that was possible?" is the trigger question. Examples:

- An agent made a change and no test failed (which means the test coverage has a gap)
- A user hit an error path we didn't know existed
- A log entry appeared that we can't explain
- A piece of infrastructure behaved differently than documented
- A deployment had a side effect we didn't predict (the sherpa's trail data might catch this first)

### Things Where the Actual Damage ≠ Possible Damage
The incident itself was minor, but the root cause could have enabled much worse outcomes. The AWS key example from the philosophy doc: the attacker only mined crypto, but they could have done anything.

### Things We Can't Answer "How Many Times Before?"
If something happens and we ask "has this happened before?" and the answer is "we don't know" — that's a trigger. The inability to answer that question reveals a gap in logging, monitoring, or institutional knowledge.

### Weak Signals from Trail Observation
The sherpa's trail data shows something unusual — a behavioural pattern that doesn't match known user personas, a spike in errors at an unexpected flow stage, an access pattern that looks like probing rather than normal usage.

### Near Misses
Something almost went wrong but didn't. A deployment almost broke production but was caught in staging. A configuration change almost exposed data but was caught in review. The "almost" is the trigger — because the systemic conditions that allowed "almost" are the same ones that will cause "actually" next time.

## How to Run It: The Process

### 1. Declaration
Someone declares a P3-as-P1. This can be any role — not just CISO. The conductor acknowledges and activates the incident process.

**Create an Issues FS database for the incident.** Everything that follows lives in this database. This is non-negotiable. The incident database is the single source of truth, the replay mechanism, and the audit trail.

### 2. Assembly
The conductor determines which roles are needed based on the nature of the incident. A typical P3-as-P1 might involve:

**Always involved:**
- **CISO**: leads the technical investigation, defines the threat model for this specific incident
- **Conductor**: coordinates the response, manages sequencing, maintains situation awareness
- **Librarian**: information management — where's the data, where's the documentation, what do we already know about this
- **Journalist**: real-time documentation, status packaging, communication drafts
- **Historian**: records the timeline, captures decisions and their rationale

**Usually involved:**
- **DevOps**: infrastructure investigation — logs, access records, deployment history, system state
- **AppSec**: vulnerability analysis — what's the attack surface, what could be exploited
- **GRC**: risk assessment — what's the exposure, what are the regulatory implications
- **DPO**: data protection analysis — was personal data involved, are there notification obligations

**Involved when relevant:**
- **Architect**: system design questions — how does this component connect, what's the blast radius
- **Dev**: code-level investigation — what changed, when, what are the side effects
- **Sherpa**: trail analysis — what did user/attacker behaviour look like before, during, and after
- **Advocate**: user impact assessment — are users affected, do they need to be informed
- **Cartographer**: dependency mapping — what else is connected, what's the blast radius visually
- **QA**: test gap analysis — what tests should have caught this, why didn't they
- **Ambassador**: external impact — does the market need to know, does positioning need to change
- **Designer**: if the incident involves UX (e.g., a confusing flow that led to user data exposure)

### 3. Situation Awareness
First priority: establish what we know and what we don't know. The CISO and conductor lead this.

Key questions to answer immediately:
- What happened? (The surface event)
- When did it happen? When did we detect it?
- What's the current state? (Is it ongoing? Contained? Resolved?)
- What data and systems are involved?
- What's the actual impact so far?

The librarian's job at this stage is critical: pull together all relevant documentation, prior incidents, existing runbooks, architecture diagrams, and anything else the team needs. The incident team shouldn't be hunting for information — the librarian should be delivering it.

### 4. Investigation
Now go deeper. This is where the P3-as-P1 approach diverges from a normal P3 response:

**Root cause analysis**: don't just find what happened — find why it was possible. Follow the second stories. Why was that key there? Why wasn't that log being monitored? Why was that configuration possible?

**Full envelope analysis**: map the union of all possible damage. Given the same root cause, what else could an attacker (or a misconfigured agent, or a bug, or a failure cascade) have done? This should be uncomfortable. If it isn't, you haven't gone deep enough.

**"How many times before?" audit**: for every finding, ask whether this has happened before. Search the logs. Search the trail data. Search the incident history. If you can't answer the question, that itself is a finding — a gap in observability.

**Blast radius mapping**: the cartographer maps the technical blast radius (code and infrastructure dependencies). The sherpa maps the behavioural blast radius (what did users experience, what trails changed). Both views feed into the investigation.

### 5. Runbook Assessment
For every action taken during the response, ask: was there a runbook for this? If yes, was it accurate and useful? If no, that's a gap to be filled.

Document every missing runbook, every outdated procedure, every step where someone had to improvise. These become the improvement backlog.

### 6. Documentation and Timeline
The journalist captures the narrative in real time. The historian maintains the precise timeline. At the end of the investigation, there should be:

- A complete timeline of the incident (when things happened)
- A complete timeline of the response (when we did things)
- The gap between the two (time to detect, time to respond, time to contain)
- The full investigation findings (root cause, second stories, full envelope analysis)
- The list of gaps discovered (missing runbooks, missing monitoring, missing knowledge)
- The improvement backlog (what needs to be fixed, prioritised by risk)

### 7. Close-Out and Learning
The incident closes when:
- The immediate issue is resolved (or accepted as a known risk)
- The full envelope analysis is complete
- The improvement backlog is captured in Issues FS as actionable tasks
- The journalist has written the incident report
- The historian has archived the incident database

The improvement tasks don't have to be done during the incident — but they must be captured, prioritised, and tracked. This is where the conductor takes over, integrating the improvement backlog into the normal work sequencing.

## Issues FS as the Incident Backbone

This is where our infrastructure gives us a massive advantage. The entire incident runs inside an Issues FS database:

- Every question is an issue
- Every finding is an issue
- Every action is an issue
- Every handover between roles is a linked issue
- Every gap discovered is an issue that becomes a task
- Screenshots, log extracts, trail data — all attached to issues

The incident database can be:

- **Replayed**: walk through every action in sequence, see exactly what happened and when
- **Analysed**: graph queries show which roles were most active, where handovers got stuck, where gaps clustered
- **Archived**: zip the database, store it, refer back to it for future incidents
- **Compared**: overlay multiple incident databases to see patterns across incidents
- **Used as a template**: the structure of one incident becomes the starting template for the next

The slow game of tennis metaphor from the tabletop exercise brief applies here too: one role serves a question, another returns an answer, every volley is an issue in the graph.

### Communication Format

During the incident, the journalist packages regular status updates. These should be:

- **Concise**: what do we know now that we didn't know an hour ago?
- **Structured**: current status, findings so far, open questions, next actions
- **Audience-appropriate**: one version for the technical team, one version for leadership (if applicable)

The journalist doesn't wait to be asked. They actively check in with each role and produce updates on a cadence — hourly for active investigation, daily for slow-burn analysis.

## When an Event Stops Being an Incident

Here's the operational maturity model: as you operationalise responses, incidents become events.

A lost laptop is a security incident when:
- You don't know if the disk was encrypted
- You don't have a remote wipe capability
- You don't know what data was on it
- You don't have a protocol to disable the accounts that were accessible from it
- You don't know how to determine if anything was exfiltrated
- You can't replace the laptop quickly

A lost laptop is just an event when:
- You know the disk is encrypted (and you verified this, not just assumed it)
- You can remote wipe it
- You know exactly what data and credentials were accessible
- You have a runbook that disables all relevant accounts within minutes
- You have monitoring that would detect if the laptop was used to access systems before the wipe
- You can replace it the same day

The difference is preparation. Every P3-as-P1 you run moves more incidents into the "event" category — because you build the runbooks, the monitoring, the automation, and the knowledge that turn surprises into expected scenarios.

The definition of an incident: **something that happened that we were not prepared for, that we don't have the right workflow, automation, data, or capability to handle, and that could escalate based on events we don't control.**

The goal is to shrink the surface area of "things we're not prepared for" — and P3-as-P1 is how you do it.

---

## For the Agents

- **CISO**: you lead the investigation in every P3-as-P1. Start developing a trigger framework — what constitutes a declarable event for our specific setup? What are our current weak signals?
- **Conductor**: you coordinate the response. Define the assembly process — how are roles notified, how is the incident database created, how is the cadence set?
- **DevOps**: you will be in every incident. Start inventorying what data you can produce on demand — logs, access records, deployment history, system state snapshots. What can you deliver in 5 minutes? What takes an hour? What takes a day?
- **Librarian**: you are the information backbone during an incident. Build an index of where critical information lives — architecture docs, deployment configs, access control policies, prior incident records. When the CISO asks "where's the CloudFront config?", you should have the answer immediately.
- **Journalist**: you document in real time and produce status updates on cadence. Develop a status update template — current state, new findings, open questions, next actions. Practice this format.
- **Historian**: you maintain the timeline. Every incident needs an exact, timestamped sequence of events (what happened) and response actions (what we did). Define the timeline format now, before the first incident.
- **All roles**: when something surprises you, say so. "How come this was possible?" is always a valid question. Don't wait for the CISO to declare an incident — flag the weak signal and let the team decide if it warrants investigation.
