# Explorer Team: Daily Brief — 15 February 2026

**version** v0.3.10  
**date** 15 Feb 2026  
**from** Human (project lead)  
**to** Explorer team via Conductor  

---

## Progress Update: What Shipped

Good news. We just pushed a really solid update:

**Cache service logging pipeline is live.** We built the first collector for AWS data, deployed using the cache service itself. The LETS (Load, Extract, Transform, Save) workflow is working end-to-end: collect from AWS → store in cache → display in admin UI. This is the foundation for everything in the Server Analytics workstream.

**AWS logging is fully configured.** Working with DevOps, we've enabled log capture across CloudFront, Lambda, S3 access, and CloudWatch. All the data we need is now flowing.

**Privacy by design worked.** This is a great validation of the role model. When configuring the AWS logging, the DPO role was keeping an eye on the brief, and the DevOps configuration that went into AWS made sure we **don't capture IP addresses or other sensitive data** in the logs. This happened naturally through the briefing process — the DPO's presence in the team ensured privacy was built in from the start, not bolted on after.

We also confirmed the end-to-end admin flow: push changes to the admin UI → grab data from the cache service → display it. This loop is now working and ready to be expanded.

---

## UX Fixes and Improvements

### 1. Three-Word Keys (Exploration)

An idea worth exploring: instead of complex encryption keys and long hash strings, offer a mode where the user can choose **three memorable words** as their passphrase.

The concept (similar to What3Words but for encryption):
- User uploads a file
- Instead of getting a complex key, they choose or are given three words: `sunset-bicycle-ocean`
- They tell the recipient: "go to send.sgraph.com and enter `sunset-bicycle-ocean`"
- The recipient enters the three words and downloads the file

This could work on multiple levels:
- **Derive the cache ID** from the three-word combination (find the file)
- **Derive the decryption key** from the three-word combination (decrypt the file)
- Or split them: three words for lookup, separate mechanism for decryption

The trade-off is obvious — three dictionary words have less entropy than a 256-bit key. But for many use cases, the user-friendliness outweigh the cryptographic strength. Not every file transfer needs military-grade encryption. Sometimes you just want to say "the password is sunset-bicycle-ocean" over the phone.

**Note**: check whether "What Three Words" or similar concepts are trademarked/patented. We need our own branding regardless.

**Architect + AppSec**: explore how this could work technically. What's the entropy of three random words from a dictionary? Can we derive both lookup and decryption from the same three words, or should they be separated? What are the security implications? **Dev**: don't implement yet — this needs the security review first. **Advocate**: from a UX perspective, how much easier does this make the sharing experience?

### 2. Token Expiry Message (Bug Fix)

**Bug**: when a user has an invalid, expired, or used-up token, we silently redirect to the first page with no message. The user has no idea what happened.

**Fix**: show a clear message explaining why they can't proceed. "This token has expired", "This token has reached its usage limit", "This token is not valid" — with guidance on what to do next (request a new token, contact the sender, etc.).

**Dev**: fix this. It's a straightforward UX bug.

### 3. Show Token Usage (Feature)

On the UI, show the user how many tokens have been used and how many remain. We already have this data — it's in the cache service. Surface it in the UI as:
- A usage counter (e.g., "3 of 10 uses remaining")
- A confirmation of the token being used ("Token accepted: community-beta-2025")

This is also the beginning of the payment model visibility — showing users that usage has a budget.

**Dev + Designer**: add token usage display to the UI.

### 4. Download Page Loading Experience (Bug Fix)

**Bug**: when a user opens the download page, because we load a lot of JavaScript, there's a delay before anything appears. The first thing the user sees during this delay is the "join the Early Access programme" / mailing list prompt — which is confusing and wrong. The user came here to download a file, not sign up.

**Fix**: show a loading indicator immediately. "Preparing your download..." or similar. The mailing list prompt should not be the first thing visible. The download experience should be front and centre from the first millisecond.

**Dev**: fix the loading sequence. Show a loading state immediately, defer the mailing list prompt until after the download UI is ready.

### 5. Token-Aware Page State (Bug Fix)

**Bug**: the download page shows "Want to send files too? Join the Early Access programme" even when the user already has a token in local storage. This is confusing — they're already in the programme.

**Fix**: detect whether the user has a valid token in local storage. If they do:
- Show "Send another file" (or "Send a file") — link back to the upload page
- Do NOT show the "join" prompt — they've already joined

If they don't have a token:
- Show the "Want to send files too?" prompt with the join/waitlist flow

**Dev**: implement token-aware conditional display. The page should know whether the user is already an active user or a new visitor.

### 6. Admin UI Links

Add navigation links to the admin UI:
- Link to the Swagger API documentation page
- Link to the main site
- Link to the docs site

**Dev**: straightforward — add the links.

### 7. Admin Lambda: Analytics Pulse Not Working (Bug)

**Bug**: the "Analytics Pulse" feature on the admin Lambda is not functioning. This is supposed to be the real-time-ish heartbeat showing current activity. It needs to be fixed — this is one of the first things an admin looks at.

**Dev**: investigate and fix.

### 8. Admin Lambda: IFD Version Switching (Feature)

The admin panel should allow the user to choose between IFD versions — switching between available minor versions to compare, test, or roll back.

To make this work, we need a **shared manifest file at the root of the major version** that lists all available versions. Without this, you can only go backward (by knowing the previous version number) but not forward (you don't know what versions exist ahead of you).

The manifest should contain:
- All available minor versions
- Which version is currently active
- Timestamp of each version

**Dev + Architect**: implement the version manifest and the version switcher in the admin UI.

### 9. Admin Lambda: Token Fetch Performance (Bug)

**Bug**: fetching existing tokens is taking too long. This should be near-instant — we're reading from the cache service, not making expensive queries.

I don't think the cache service is being used as effectively as possible here. Before fixing the symptom, we need to understand the cause.

**Architect**: produce an **architecture mapping of how the cache service is currently being used** for token storage and retrieval. Where are the cache entries? What's the lookup path? Are we doing unnecessary reads? Are we loading more data than we need? Are we hitting S3 directly when we should be hitting a cached aggregation?

Once we understand the current architecture, the fix should be obvious. This is a good example of "map it first, then optimise" rather than guessing.

**Dev**: implement the fix once the architect has identified the bottleneck.

### 10. User Lambda: Upload Progress Granularity (Bug)

**Bug**: the upload process appears to hang — the progress bar doesn't move with enough granularity. The user sees the progress stall and thinks something is broken.

We need much more granular events driving the progress bar. The upload workflow has multiple stages, and each should produce progress events:
- File selected
- Encryption starting
- Encryption progress (percentage of file encrypted)
- Encryption complete
- Upload starting
- Upload progress (percentage of data sent to S3)
- Upload complete
- Server-side processing
- Link generation
- Done

Each of these stages should update the progress bar. Right now, it seems like some of these stages are silent — the user sees nothing happening for long stretches.

**Dev**: add granular progress events for every stage of the upload workflow.

### 11. User Lambda: End-to-End Timing Capture (Feature)

We need to capture the **actual timings of the entire "Send" button workflow** — both client-side and server-side. We need to understand where the bottlenecks are:

- How long does encryption take? (client-side)
- How long does the upload take? (client → S3)
- How long does server-side processing take? (Lambda execution)
- How long does link generation take?
- What's the total end-to-end time?

Capture these timings on every send operation and store them in the cache service. This data feeds directly into the SA dashboard — we should be able to see timing breakdowns over time, identify regressions, and optimise the slowest stages.

**Dev**: instrument the send workflow with timing capture at each stage, both client-side (JavaScript performance marks) and server-side (Lambda execution timing). Store the results in the cache service.

### 12. User Lambda: Prevent Navigation During Upload/Download (Bug)

**Bug**: there's nothing stopping the user from navigating away from the page while an upload or download is in progress. If the user clicks a link on the page — or hits the back button, or closes the tab — the transfer silently dies. The file is lost, the progress is lost, and there's no warning.

This could be quite problematic because there ARE clickable links on the page (admin links, "send another file", mailing list, etc.) that a user might click without realising they're aborting their transfer.

**Fix**: implement a navigation guard during active transfers:
- Use the `beforeunload` event to show a browser-native "are you sure you want to leave?" warning if a transfer is in progress
- Disable or visually de-emphasise on-page links during active transfers (or at minimum, intercept clicks and show a confirmation: "Upload in progress — are you sure you want to leave?")
- If the user does navigate away, consider whether we can preserve transfer state for resume (ties into the upload/download resilience research)

**Dev**: implement the `beforeunload` guard immediately — it's a one-liner that prevents accidental data loss. The on-page link handling is a second step.

This deserves its own section because it's going to become increasingly critical as we move toward large file transfers.

### The Problem

Right now, the upload and download workflows are basic — they work for small files, but they don't have the resilience, granularity, or control needed for large files or unreliable networks. As file sizes grow (and user feedback has already told us that large file transfer is our sweet spot), this becomes the most important technical challenge in the product.

### What "Super Solid" Looks Like

A production-grade file transfer workflow needs:

**Upload resilience:**
- Chunked/multipart uploads (break large files into pieces)
- Retry logic per chunk (if one chunk fails, retry just that chunk, not the whole file)
- Resume capability (if the connection drops, pick up where you left off)
- Checksum verification per chunk (integrity guarantee at every stage)
- Progress reporting per chunk (granular progress bar)
- Client-side encryption per chunk (stream encryption, not load-entire-file-then-encrypt)
- Bandwidth estimation and adaptive chunk sizing
- Timeout handling and graceful degradation

**Download resilience:**
- Chunked/range-based downloads
- Resume capability (partial download recovery)
- Integrity verification (checksum of reconstructed file matches original)
- Decryption streaming (decrypt as chunks arrive, not after full download)
- Progress reporting with accurate time estimation

**Control and observability:**
- Cancel/pause/resume controls in the UI
- Detailed status reporting at every stage
- Server-side tracking of transfer state (so the sender can see transfer progress)
- Error reporting that tells the user what went wrong and what to do about it

### The Research Task

This is an important enough topic that I'll create a separate briefing focused entirely on it. But that shouldn't block the initial research.

**Architect + Dev**: produce a research document on upload/download resilience. Cover:

1. **Current state**: how does the upload/download workflow work today? What are the specific limitations and failure modes?
2. **Browser capabilities**: what do modern browsers support for chunked uploads, streaming encryption, resumable transfers? What APIs are available (ReadableStream, Fetch with streaming, File API slicing, etc.)?
3. **S3 capabilities**: what does S3 offer for multipart uploads, presigned URLs for chunked upload, transfer acceleration? How does this interact with our client-side encryption model?
4. **Existing solutions**: what open-source libraries or patterns exist for resilient browser-based file upload? (tus protocol, Uppy, Resumable.js, etc.) What can we learn from or build on?
5. **Encryption interaction**: how does chunked upload interact with client-side encryption? Can we encrypt per-chunk, or do we need to encrypt the whole file first? What are the trade-offs?
6. **Proposed architecture**: a recommended approach for resilient upload/download that works with our zero-knowledge encryption model

This research feeds directly into the large file transfer roadmap item from the previous daily brief. It's one of the most important differentiators for the product.

**Deadline**: produce the initial research document within the current workstream. It doesn't need to be exhaustive — it needs to map the landscape, identify the best approach, and propose a path forward.

---

## Server Analytics (SA): The Deep Dive

This is the major exploration workstream for this phase. What we're building is the foundation of **Server Analytics** — eventually a standalone project (I already have the `server-analytics` PyPI package reserved), but for now being developed as part of the SGraph Send admin tooling.

### Two Types of AWS Data

There are two fundamentally different types of data we collect from AWS, and we need to support both:

**1. Event-driven data (logs)**
This is data that AWS captures automatically as things happen:
- CloudFront access logs (via Firehose → S3)
- CloudWatch logs (Lambda execution logs)
- CloudTrail audit logs
- S3 access logs

Characteristics: raw, unprocessed, high-volume, often noisy. Contains a lot of repetition (the same log format repeated thousands of times with different values). Needs heavy transformation to be useful.

**2. Collected data (API calls)**
This is data we actively fetch from AWS by calling its APIs:
- Lambda function configurations and metrics
- Lambda performance data (execution times, error rates, invocation counts)
- CloudFront distribution settings
- S3 bucket configurations
- Route 53 settings
- Security group configurations
- IAM settings

Characteristics: already well-structured by AWS, represents the current state of the infrastructure, point-in-time snapshots. Less volume, more immediately useful.

### The LETS Pipeline for Both Types

Both types flow through the same LETS (Load, Extract, Transform, Save) pipeline, but the processing is different:

**Event-driven data pipeline:**
```
CloudFront/Lambda/S3 → logs appear in S3
    │
    ▼
S3 event triggers Lambda function (automatic)
    │
    ▼
Lambda extracts what we need, normalises, compresses
(e.g., 50 identical log entries → "50x GET /api/file, unique IPs: 12")
    │
    ▼
Saves curated version to cache service (temporal + latest)
```

This is a key architectural pattern: **S3-triggered Lambda transformation**. Every time a log file lands in S3, a Lambda function fires automatically to process it. The main application never triggers raw data creation — it only ever reads the already-curated version from the cache service.

The first transformation typically achieves **~90% compression** of raw data while preserving everything meaningful. Raw CloudFront logs are verbose — most of the content is repeated structure. After extraction, you have just the data points that matter.

**Collected data pipeline:**
```
Scheduled collector (or on-demand trigger)
    │
    ▼
Calls AWS APIs (Lambda metrics, configurations, etc.)
    │
    ▼
Saves to cache service: latest (current state) + temporal (historical)
    │
    ▼
Compares with previous version → saves only if changed (delta tracking)
```

The collected data pipeline introduces **delta tracking**: when you fetch the current state of a Lambda function's configuration, compare it to the last saved version. If nothing changed, don't save a new temporal entry. If something changed, save the new version (and optionally a diff). Over time, this builds a **change history** of the entire AWS environment.

### The Digital Twin

This is where it gets powerful. By collecting both event data (what happened) and state data (what things look like) over time, we're building a **digital twin of the AWS environment**:

- What is the current configuration of every Lambda function, S3 bucket, CloudFront distribution? → **latest** folder
- What was the configuration 5 days ago? 10 days ago? → **temporal** folder
- What changed between then and now? → **diff** between temporal snapshots
- What happened during that time? → **event logs** correlated with configuration changes

This is enormously valuable for:
- Debugging ("what changed in the last 24 hours that could have caused this?")
- Security ("was anything modified that shouldn't have been?")
- Compliance ("prove that the configuration was X at time Y")
- Cost analysis ("how did the configuration changes affect cost?")

### Smart Collection: Never Fetch Twice

The absolute principle: **minimise AWS API calls and never fetch the same data twice.**

This means:
- Track high-water marks for every data source (last timestamp fetched, last collection time)
- On each collection run, only fetch the delta since the last run
- Use checksums to detect actual changes vs. timestamp-only changes (some AWS responses include timestamps that change even when the data hasn't)
- Cache the collection metadata so we know exactly what we have and what we need

The cache service's temporal structure makes this natural: check the latest folder to see what we last collected, then only fetch what's newer.

---

## SA Dashboard: Dedicated Page with Path-Based Navigation

### Separate Page, Not a Tab

Server Analytics needs its own dedicated page in the admin UI — not a section on the existing admin index. This is its own application within the admin.

### Path-Based Navigation (Not Hash-Based)

This is important. The SA dashboard should use **path-based URLs**, not hash-fragment navigation:

```
/admin/dashboard/                          → SA overview
/admin/dashboard/lambda/                   → all Lambda functions
/admin/dashboard/lambda/{lambda-id}        → specific Lambda function detail
/admin/dashboard/cloudfront/               → CloudFront overview
/admin/dashboard/cloudfront/logs/          → CloudFront log viewer
/admin/dashboard/s3/                       → S3 overview
/admin/dashboard/cache/                    → Cache service browser
/admin/dashboard/costs/                    → AWS cost dashboard
```

Think of it as a **REST-style UI** — the URL tells you exactly what you're looking at, and you can navigate directly to any view by typing the URL. Each component should be loadable individually (single-page app style), but also reachable directly via URL.

Why this matters:
- I can bookmark specific views
- I can share a link to a specific dashboard directly
- Each view is independently deployable and testable
- As the SA grows, the URL structure stays clean and predictable

**Dev + Architect**: implement path-based routing for the SA dashboard. Each route maps to a self-contained view component.

### Priority Views to Build

**1. Cache Service Browser (build first)**
This is the most important view because everything else depends on it. A UI that knows how to navigate the cache service:
- Browse cache IDs
- View temporal data (date-based navigation)
- View latest data
- Inspect individual cache entries (JSON, text, binary)
- Navigate the folder structure within cache entries

This is a development tool as much as an operational dashboard — it lets us see what data we're collecting, how it's structured, and whether the LETS pipeline is working correctly.

**2. Lambda Functions View**
- List all Lambda functions in the environment
- For each: configuration, metrics (invocations, errors, duration), recent execution data
- Only show the ~20 settings we care about, not the full AWS console view
- Historical comparison (what changed since last collection?)

**3. CloudFront Log Viewer**
- Visualise the CloudFront access logs that are being collected via Firehose
- Show traffic patterns, request rates, response codes, popular endpoints
- Real-time-ish view (how fast can we process and display new logs?)
- This is the front-end view of all user traffic — the closest thing to "Google Analytics replacement" we have

**4. AWS Infrastructure Overview**
A slim, read-only version of the AWS console showing our specific deployment:
- CloudFront distributions and their configurations
- Route 53 DNS settings
- S3 buckets and their configurations
- Lambda functions and their configurations
- All on one page, all for one deployment

This is the beginning of a **deployment-aware admin view** — since we're going to have many deployments (Explorer, Villager, per-customer eventually), we need a consolidated view of everything in one deployment.

### Three Missing UI Elements

The admin UI is currently missing three critical UI components that I have working in other projects:

1. **API call log**: visualise every API call the admin UI makes — what endpoint, what response, timing. Essential for debugging and understanding data flow.
2. **Event log**: visualise UI events — what the user clicked, what triggered, what loaded. Essential for understanding UI behaviour.
3. **Message panel**: system messages, status updates, errors — a centralised place for the UI to communicate with the developer/admin.

These exist in code I've already developed. If they're not in the current guidance, ask for them — I'll share the implementation. Adding these three elements to the admin UI dramatically improves the development experience.

**Dev**: add these three UI elements to the admin page. Check existing guidance first; if not there, request the code from me.

---

## Task Summary

| Priority | Task | Owner(s) |
|----------|------|----------|
| **P1** | Fix download page loading experience (show loading state, not mailing list) | Dev |
| **P1** | Fix token expiry message (show error instead of silent redirect) | Dev |
| **P1** | Implement token-aware page state (detect existing token, adjust UI) | Dev |
| **P1** | Fix Analytics Pulse on admin Lambda | Dev |
| **P1** | Fix upload progress bar granularity (granular events per stage) | Dev |
| **P1** | Prevent navigation during active upload/download (beforeunload guard) | Dev |
| **P1** | Build cache service browser UI (path-based, under /admin/dashboard/cache/) | Dev, Architect |
| **P2** | Capture end-to-end send workflow timings (client + server) | Dev |
| **P2** | Architecture mapping of cache service usage for token storage | Architect |
| **P2** | Fix token fetch performance (based on architecture mapping) | Dev, Architect |
| **P2** | Implement IFD version manifest and version switcher in admin UI | Dev, Architect |
| **P2** | Build Lambda functions dashboard view | Dev |
| **P2** | Build CloudFront log viewer | Dev |
| **P2** | Show token usage counter in the UI | Dev, Designer |
| **P2** | Implement S3-triggered Lambda for automatic log transformation (LETS) | Dev, DevOps |
| **P2** | Add API call log, event log, and message panel to admin UI | Dev |
| **P2** | Set up path-based routing for SA dashboard | Dev, Architect |
| **P2** | Add navigation links to admin UI (Swagger, main site, docs) | Dev |
| **P2** | **Research: upload/download resilience** (chunked, retry, resume, streaming encryption) | Architect, Dev |
| **P3** | Explore three-word key concept (security review first) | Architect, AppSec |
| **P3** | Build AWS infrastructure overview (slim read-only console) | Dev |
| **P3** | Implement delta tracking for collected data (temporal diffs) | Dev |

---

## For the Conductor

Three tracks today: **UX bug fixes** (items 1–5 and 7–11 — these directly affect users and admin usability, ship fast), **Server Analytics foundation** (the SA dashboard with cache service browser as the gating item), and **upload/download resilience research** (doesn't block anything, but start early — it's going to be one of the most important technical investments we make).

The UX bugs split into user-facing (download page loading, token expiry, token-aware state, upload progress granularity) and admin-facing (Analytics Pulse, token fetch performance, IFD version switching). The user-facing bugs are higher priority — every new user hits them.

The token fetch performance issue needs the architect's cache service usage mapping before the dev can fix it effectively. Don't let the dev guess — map it first.

The cache service browser remains the highest-value SA item because it unblocks everything else. Build that first, then fan out to Lambda views, CloudFront views, and cost dashboards.

The upload/download resilience research is flagged as P2 but is strategically critical. A separate detailed briefing is coming, but the architect and dev should start the research immediately — understanding the landscape of chunked uploads, streaming encryption, and resumable transfers. This is what makes the product work for large files, which is our identified sweet spot.

Let's keep building.
