# The File Transfer Engine: Architecture, Principles, and Research Brief

**version** v0.3.12  
**date** 15 Feb 2026  
**from** Human (project lead)  
**to** Explorer team — Architect, Dev, AppSec, DevOps  
**status** Research brief → triggers multiple exploration workstreams  

---

## What This Is

This document defines the architecture, principles, and research agenda for the core file upload/download engine that sits at the heart of SGraph Send. This component is so fundamental — and so important to get right — that it deserves its own brief, its own exploration workstreams, and eventually its own standalone project.

The key insight: **encryption is just a callback.** It's a business decision layered on top of the real engineering challenge, which is getting files from A to B reliably, fast, and at scale. The encryption/decryption step is a module that plugs into the transfer pipeline — it's not where the complexity lives. The complexity lives in the transfer itself: chunking, progress, resilience, retry, resume, streaming, compression, and all the edge cases that emerge with large files, slow networks, and impatient users.

Build the transfer engine right, and encryption is trivial to add. Build it wrong, and no amount of clever cryptography will save the user experience.

---

## The Primitives

The system has three components:

```
CLIENT (browser or Node.js)          SERVER (cloud storage)
──────────────────────────          ─────────────────────

Has a file                          Will store it
Wants to upload or download         Serves or receives chunks
Controls the UI and progress        Controls the storage and metadata
```

Two transport paths:

1. **Direct to cloud storage** — client uploads directly to S3 (or Azure Blob, GCP Cloud Storage) using presigned URLs or platform SDKs. No Lambda in the data path. No 6 MB payload limit. This is the path for large files.

2. **Via API (Lambda)** — client uploads through our API Gateway → Lambda pipeline. Simpler, but limited by Lambda's payload constraints (~6 MB). Fine for small files, simple deployments. Note: Lambda streaming responses (via the Web Adapter / function URLs) may allow larger downloads even through Lambda — research needed.

The transfer engine should be **transport-agnostic**. The same chunking, progress, and resilience logic works regardless of whether chunks go directly to S3 or through Lambda. The transport is pluggable.

---

## Core Principles

### 1. Maximum Visibility to the User

Three levels of progress detail, chosen by the user:

**Level 1 — Simple progress bar**
"Sending your file... 64%"
One bar. One percentage. That's it. For most users, most of the time.

**Level 2 — Stage detail**
Shows each stage of the transfer with its own progress:
```
✓ Compressing file          (2.1 MB → 1.4 MB, 33% saved)
✓ Encrypting                (completed in 0.8s)
■ Uploading                 (chunk 7 of 12, 58%)
○ Verifying integrity
○ Generating link
```
Each stage has its own mini progress indicator. Users who want to understand what's happening can see exactly where they are.

**Level 3 — Raw technical detail**
The developer view. Every API call, every chunk hash, every retry, every timing measurement:
```
[14:32:01.234] Chunk 7/12: 1.2MB, SHA-256: a3f7b2...
[14:32:01.235] PUT s3://bucket/chunks/007 (presigned, expires 3600s)
[14:32:02.891] Response: 200 OK, ETag: "d41d8cd9..."
[14:32:02.892] Chunk 7 verified. Elapsed: 1.657s, throughput: 723 KB/s
[14:32:02.893] Updating manifest: 7/12 chunks complete
```

This is simultaneously:
- A debugging tool for us during development
- A trust-building feature for technical users and developers integrating with us
- A transparency mechanism that proves we're doing what we say we're doing

**All three levels read from the same event stream.** The transfer engine emits detailed events for everything. The UI layers choose which events to display at each level.

### 2. No Data Loss — Ever

The user should never lose a file due to a transfer failure. This is the hardest principle to implement fully, but it's the most important. Build toward it in maturity stages:

**Maturity 0 (current):** user writes text or drops a file, refresh happens, everything is gone. No resilience.

**Maturity 1 (near-term):** `beforeunload` warning prevents accidental navigation. Retry on transient failures. Resume from the last successful chunk if the connection drops.

**Maturity 2 (medium-term):** file is copied to browser persistent storage (IndexedDB — see storage note below) before upload begins. Even if the tab crashes, the file can be recovered on return. Transfer state is persisted in the transfer manifest, so resume is automatic.

**Maturity 3 (advanced):** full resume across sessions. User closes the laptop, opens it tomorrow, the transfer picks up where it left off. Chunks already uploaded don't need to be re-uploaded. The manifest on the server tracks exactly which chunks are present and verified.

**Important: browser storage options.** `localStorage` is limited to ~5–10 MB — far too small for file resilience. The correct APIs are:
- **IndexedDB**: hundreds of MB to GB depending on browser and available disk. Binary-friendly. This is the primary option for client-side file caching.
- **Origin Private File System (OPFS)**: newer API, designed for high-performance file operations. Better for large files than IndexedDB.
- **Cache API (Service Workers)**: generous limits, good for caching responses. Less suitable for raw file storage but worth knowing about.

The brief should be clear: `localStorage` is NOT viable for file resilience. IndexedDB or OPFS is the path.

### 3. Reduce User Anxiety

This is a first-class design principle, not a nice-to-have. Users are conditioned by years of technology that "kind of works and fails." File transfers are particularly anxiety-inducing — especially large ones.

Specific anxiety-reduction features:

**Upload-side anxiety:** the progress bar moves smoothly and frequently (not stuck for long periods), every stage is visible, the user can see exactly what's happening.

**Download-side anxiety — the killer feature:** when the recipient opens the download page, they can see the **upload status** even if the upload is still in progress. Instead of "file not found" or a blank page, they see:

```
Your file is being prepared...
Upload progress: 72% (estimated 2 minutes remaining)
Download will begin automatically when ready.
```

This is possible because the transfer manifest exists on the server from the moment the upload starts. The download page reads the manifest and shows real-time status. The moment the upload completes (or even as chunks become available), the download can begin.

This turns the most stressful moment ("is it there yet? refresh refresh refresh") into a calm, informed wait.

**Retry anxiety:** when a chunk fails and is retried, show it at level 2. "Chunk 7 failed, retrying... success." The user knows something went wrong AND that it was handled. Silence during failures is what creates anxiety.

### 4. Minimise Resources

Every byte counts. Not for philosophical reasons — for cost, speed, and ecological impact:

- **AWS charges per GB** stored and transferred
- **Users on slow/metered connections** don't want unnecessary bandwidth usage
- **Mobile users** have data caps
- **Large files** multiply every inefficiency

Specific strategies:

**Compression before upload.** The client compresses the file before sending it. This reduces upload time, storage cost, download time, and bandwidth. The bottleneck for most users is network speed, not CPU — spending a few seconds of local CPU to compress the data and save minutes of upload time is an excellent trade-off.

**CRITICAL: compress BEFORE encrypting.** This order is non-negotiable. Encrypted data is essentially random and does not compress. If you encrypt first then compress, you get zero compression benefit. The pipeline must be: **compress → encrypt → upload** and **download → decrypt → decompress**. State this as an invariant in the code.

**Multiple compression levels.** Offer options:
- Fast compression (e.g., LZ4, Snappy) — minimal CPU, moderate compression ratio. Good default.
- Strong compression (e.g., Brotli, Zstd) — more CPU, better compression ratio. Worth it for large files on slow connections.
- No compression — for files that are already compressed (ZIP, JPEG, MP4, etc.). The engine should detect this and skip compression automatically.

**Bandwidth-aware transfer.** The user should be able to choose:
- Full speed (default) — use all available bandwidth
- Background mode — throttle the transfer to avoid saturating the connection. The user can keep browsing, on a call, etc. The transfer runs slowly but doesn't interfere with other activity.
- Custom throttle — set a bandwidth limit

**Content-aware optimisation (future).** For files that have been previously uploaded (or partial matches), only send the diff. This requires content-addressable chunk storage — if a chunk's hash matches one already stored, skip it. This is the same principle Git uses, and it's incredibly powerful for:
- Re-uploading a slightly modified file (only changed chunks are sent)
- Uploading files that share content (deduplicated storage)

### 5. Respect User Wishes

The transfer engine is a tool for the user, not a tool that uses the user:

- User can pause, resume, cancel at any time
- User can choose bandwidth limits
- User can choose compression level
- User controls what's stored in their browser (and can clear it)
- User can see exactly what's happening (level 1, 2, or 3 detail)

---

## The Transfer Manifest

Every transfer is governed by a **transfer manifest** — a JSON document stored in the cache service that is the single source of truth for the transfer's state.

```json
{
  "transfer_id": "abc-123",
  "cache_id": "...",
  "status": "uploading",
  "created": "2026-02-15T14:32:00Z",
  "file": {
    "name": "quarterly-report.pdf",
    "size_original": 15728640,
    "size_compressed": 10485760,
    "size_encrypted": 10485776,
    "content_hash": "sha256:a3f7b2c1...",
    "compression": "zstd",
    "encryption": "aes-256-gcm"
  },
  "chunks": {
    "total": 12,
    "size": 1048576,
    "completed": [0, 1, 2, 3, 4, 5, 6],
    "pending": [7, 8, 9, 10, 11],
    "failed": [],
    "checksums": {
      "0": "sha256:...",
      "1": "sha256:...",
      ...
    }
  },
  "timing": {
    "compression_ms": 2100,
    "encryption_ms": 800,
    "upload_started": "2026-02-15T14:32:03Z",
    "chunks_per_second": 1.2,
    "estimated_completion": "2026-02-15T14:32:12Z"
  },
  "encryption_metadata": {
    "wrapped_key": "base64:...",
    "iv": "base64:...",
    "algorithm": "aes-256-gcm",
    "key_wrapping": "user-passphrase-derived"
  }
}
```

The manifest enables everything:
- **Progress visibility from both sides** — uploader and downloader both read the manifest
- **Resume capability** — the manifest knows which chunks are complete, only missing chunks need re-uploading
- **Integrity verification** — every chunk has a checksum, verified after upload and before download
- **Real-time streaming** — the downloader can start downloading completed chunks before the upload finishes
- **Retry logic** — failed chunks are tracked in the manifest and retried
- **Transfer history** — the manifest is a complete record of the transfer

The manifest lives in the cache service as a cache entry. The chunks are data files within that entry (or separate S3 objects referenced by the manifest). This ties directly into the cache service architecture.

### Symmetric Key Wrapping

The encryption metadata in the manifest uses **hybrid encryption** (the standard approach used by TLS, PGP, and every serious encryption system):

1. A random symmetric key is generated per file (AES-256)
2. The file data is encrypted with this symmetric key (fast, efficient)
3. The symmetric key is wrapped (encrypted) with the user's passphrase-derived key
4. The wrapped key is stored in the manifest

This means:
- Changing the passphrase doesn't require re-encrypting the entire file — just re-wrap the symmetric key
- Multiple recipients can be given access by wrapping the symmetric key with each recipient's key
- The per-file symmetric key is never stored in plaintext — only the wrapped version exists

---

## Advanced Transfer Modes

### Real-Time Streaming Between Browsers

When both the uploader and downloader are online simultaneously, we can stream chunks in near-real-time:

```
BROWSER A (uploader)                SERVER (relay)              BROWSER B (downloader)
────────────────────                ──────────────              ──────────────────────

Upload chunk 1 ──────────►  Store chunk 1  ◄────────── Download chunk 1
Upload chunk 2 ──────────►  Store chunk 2  ◄────────── Download chunk 2
Upload chunk 3 ──────────►  Store chunk 3  ◄────────── Download chunk 3
   ...                        ...                        ...
```

The server (S3 / cache service) acts as a relay buffer. Chunks are available for download as soon as they're uploaded. The recipient doesn't wait for the complete upload — they're downloading in parallel.

The transfer manifest is the coordination point: the downloader polls or watches the manifest for new completed chunks, then fetches them.

**What if the downloader isn't online yet?** The server buffers all chunks. When the downloader eventually connects, they download everything that's available — same as a normal transfer, just delayed.

**Cost optimisation for large files:** in streaming mode, chunks could be stored temporarily and purged after the downloader confirms receipt. For very large files, this means the server doesn't need to hold the entire file permanently — just the chunks in transit. This could significantly reduce storage costs for high-volume transfers.

### WebRTC Peer-to-Peer (Research Item)

When both browsers are online simultaneously, there's another option: **skip the server entirely for the data transfer** using WebRTC.

```
BROWSER A ◄══════ WebRTC data channel ══════► BROWSER B
                (direct peer-to-peer)

SERVER: only handles signalling and metadata
        (transfer manifest, chunk verification)
```

This is worth researching because:
- Zero server-side bandwidth cost for the actual file data
- Potentially faster (no round-trip through cloud storage)
- End-to-end encryption is inherent (the server never sees the data)
- Good precedent: WebTorrent, ShareDrop, and similar projects prove this works in browsers

Trade-offs:
- Requires both browsers to be online simultaneously
- NAT traversal can be unreliable (need TURN server fallback)
- No server-side caching (if the connection drops, chunks need to be re-sent from the source)
- More complex connection setup

The principle: **don't reinvent the wheel.** WebRTC is a mature, battle-tested technology for browser-to-browser data transfer. If it fits our use case (both parties online, want maximum speed and privacy), we should use it rather than building our own relay protocol.

**Hybrid approach:** use WebRTC when both browsers are online, fall back to server relay when only one is online. The transfer manifest and chunk protocol stay the same — only the transport layer changes.

**Architect**: research WebRTC data channels for file transfer. Evaluate: reliability, NAT traversal success rates, throughput vs. S3 direct upload, complexity of implementation. Look at existing libraries (simple-peer, PeerJS, etc.).

---

## Platform-Native Large File Upload Support (Research Item)

Before building everything from scratch, we need to understand what the major cloud platforms already provide. Each hyperscaler has invested heavily in optimised file upload capabilities, and we should leverage them rather than reinvent the wheel.

### What to Research

**AWS S3:**
- **S3 Multipart Upload** — native support for uploading large files in parts. Handles retry per part, parallel upload, and automatic assembly. What does the official AWS SDK for JavaScript (`@aws-sdk/client-s3`) provide out of the box?
- **S3 Transfer Acceleration** — uses CloudFront edge locations to speed up uploads. How much faster is it? What does it cost?
- **S3 presigned URLs** — can we generate per-chunk presigned URLs so the browser uploads directly to S3 without our Lambda in the data path?
- **S3 Object Lambda** — can we intercept uploads/downloads to do transformation (compression, encryption) at the S3 layer?

**Azure Blob Storage:**
- **Block Blob upload** — Azure's equivalent of multipart upload. Native SDK support?
- **Azure CDN integration** for accelerated uploads?

**GCP Cloud Storage:**
- **Resumable uploads** — GCP's native resumable upload protocol. How does it compare to S3 multipart?
- **Signed URLs** for direct browser-to-GCS upload?

**Other platforms:**
- **Cloudflare R2** — S3-compatible but with zero egress fees. Could be significant for download costs.
- **Backblaze B2** — another S3-compatible option with lower costs.

**Existing libraries and protocols:**
- **tus protocol** (https://tus.io) — open protocol for resumable file uploads. Widely adopted. Has JS client and server implementations. Handles chunking, retry, resume automatically. Worth evaluating as a foundation rather than building our own.
- **Uppy** (https://uppy.io) — file upload library with tus support, S3 multipart support, progress UI, drag-and-drop, etc. Covers a lot of what we need.
- **Resumable.js** — another option for chunked, resumable uploads.

The principle: **use the best of what already exists.** If S3 multipart upload with the official SDK handles chunking, retry, and resume natively, we should build on top of it — not rebuild those capabilities from scratch. Our value-add is the encryption layer, the transfer manifest, the three-level visibility, and the user experience — not the raw upload mechanics.

**Architect**: produce a comparison matrix of platform-native capabilities vs. what we'd need to build ourselves. Identify the gaps. Recommend whether to build on tus, on the S3 SDK directly, or on something else.

---

## Pure JavaScript: Multi-Runtime from the Start

This is a critical architectural decision: **the file transfer engine is pure JavaScript, designed to run in browsers, Node.js, Deno, and Bun from day one.**

### Why This Matters

The entire file transfer logic — chunking, checksums, compression, encryption, manifest management, retry, resume — is JavaScript. It runs in the browser for the web product. But there is nothing browser-specific about the core logic. It should also run in:

- **Node.js** — for CLI tools, server-side processing, automated testing
- **Deno** — same benefits, different runtime
- **Bun** — same benefits, fastest runtime

### The Massive Testing Advantage

This is the killer reason to do this from the start. Right now, testing the upload/download flow requires:
- Two browsers open
- Our full AWS infrastructure running (S3, Lambda, API Gateway, CloudFront)
- Manual or Playwright-driven interaction

With a Node.js-compatible engine, we can:
- Run upload/download tests from the command line
- Test against local storage (filesystem) instead of S3
- Test without browsers entirely
- Run thousands of tests in CI/CD
- Stress test with arbitrary file sizes and concurrency
- Test the chunking, retry, and resume logic in isolation
- Benchmark compression and encryption performance across runtimes

**By the time we run this code in a browser, we already know it works.** The browser just provides the UI layer on top of a tested, proven engine.

### Architecture for Multi-Runtime

```
┌─────────────────────────────────────────────────────┐
│                                                     │
│  CORE ENGINE (pure JS, no browser APIs)             │
│                                                     │
│  Chunking, checksums, manifest management,          │
│  retry logic, progress events, transfer protocol    │
│                                                     │
├─────────────────────────────────────────────────────┤
│                                                     │
│  PLUGGABLE LAYERS                                   │
│                                                     │
│  Storage adapter:  S3 | Azure | GCP | Local FS      │
│  Crypto adapter:   Web Crypto | Node crypto | none  │
│  Compression:      Browser zstd | Node zstd | none  │
│  Transport:        Fetch | Node HTTP | WebRTC       │
│  UI adapter:       DOM events | CLI output | silent │
│  Cache adapter:    IndexedDB | FS | Memory          │
│                                                     │
└─────────────────────────────────────────────────────┘
```

The core engine is pure JavaScript with zero platform-specific imports. Every platform-specific capability (file system access, crypto, network, UI) is injected via adapters. The same engine code runs everywhere — only the adapters change.

### CLI Version

The Node.js adapter immediately gives us a **command-line tool**:

```bash
# Upload a file
sgraph-send upload ./quarterly-report.pdf --encrypt --compress

# Download a file
sgraph-send download abc-123 --output ./downloads/

# Upload with three-word key
sgraph-send upload ./report.pdf --key "sunset-bicycle-ocean"

# Check transfer status
sgraph-send status abc-123

# Stress test: upload 100 random files of 50MB each
sgraph-send test --files 100 --size 50MB --parallel 4
```

This CLI is:
- A testing tool (stress testing, benchmarking, CI integration)
- A developer tool (script file transfers, automate workflows)
- A product feature (users who prefer CLI over browser)
- A building block for server-side integrations

### NPM Package

The engine should be publishable as an NPM package that others can use:

```javascript
import { TransferEngine } from '@sgraph/transfer-engine';

const engine = new TransferEngine({
  storage: new S3Adapter({ bucket: 'my-bucket' }),
  crypto: new AES256Adapter(),
  compression: new ZstdAdapter(),
  onProgress: (event) => console.log(event),
});

const transfer = await engine.upload('./large-file.zip');
console.log(`Transfer ID: ${transfer.id}`);
```

**Dev + Architect**: design the adapter interfaces from the start. Every platform-specific operation must go through an adapter. Test by implementing both the browser adapter and the Node.js adapter in parallel — if the core engine works with both, the abstraction is correct.

---

## Multi-File and Folder Support (Roadmap)

### File Bundling

When uploading multiple files (especially many small ones), the overhead of creating individual transfers for each file is wasteful. Instead: **bundle multiple small files into a single package** before upload.

This reduces:
- Number of S3 objects (fewer PUT requests, lower cost)
- Number of transfer manifests (simpler management)
- Upload overhead (one chunked transfer instead of many small ones)
- Server-side storage fragmentation

The bundle is a container (tar, zip, or a custom format) that holds multiple files. The manifest tracks the individual files within the bundle. The recipient downloads one package and the client unpacks it.

### Folder Sync (Future)

Taking file bundling further: **syncing folders between users.** If we have delta/diff capability at the chunk level, we can sync folders:

1. User A has a folder with 100 files
2. User A uploads the folder (bundled, chunked, content-addressed)
3. User A modifies 3 files
4. User A syncs — only the changed chunks of the 3 modified files are uploaded (delta)
5. User B syncs — only the new/changed chunks are downloaded

This is essentially Git for file sharing — content-addressable storage with delta transfers. It's not for the initial release, but the architecture should support it from the start. Specifically:
- Content-addressable chunks (chunks identified by their hash, not their position)
- Manifest structure that can describe a tree of files (not just a single file)
- Delta computation (compare local state to manifest, identify what's changed)

**Note**: because the transfer engine is pure JavaScript running in Node.js, folder sync works on both the client (browser with File System Access API) and the server/CLI (Node.js with `fs` module). The same sync logic runs everywhere.

---

## WASM: Available When Needed

WebAssembly is a tool we should keep in our back pocket. The current use case for pure JavaScript is strong — modern JS engines are fast, the Web Crypto API is hardware-accelerated, and JavaScript compression libraries (fflate, zstd-wasm) are performant.

But WASM becomes relevant when:
- We need compression algorithms that don't have good JS implementations (or where the WASM version is significantly faster)
- We want to use battle-tested C/Rust crypto libraries (e.g., libsodium via sodium-native / libsodium.js)
- We need maximum performance for very large files where even small per-chunk overhead adds up
- We find a specific bottleneck where JS is measurably slower than native code

The architecture supports this naturally — the crypto and compression adapters can wrap WASM modules without changing the core engine. We don't need to design for WASM specifically; we just need to keep the adapter boundaries clean so WASM can be dropped in when a concrete use case emerges.

**Architect**: note WASM as available but don't prioritise it. If performance profiling reveals a bottleneck where WASM would help, we'll add it then.

---

## S3 Rate Limits and Constraints (Research Item)

Before stress testing, we need to understand the platform constraints:

- **S3 request rate limits**: S3 supports 3,500 PUT/COPY/POST/DELETE and 5,500 GET/HEAD requests per second per prefix. Is this sufficient for our parallel chunk upload model? Do we need to distribute chunks across multiple prefixes?
- **S3 multipart upload limits**: up to 10,000 parts per upload, each part 5 MB to 5 GB. Maximum object size 5 TB. How does this map to our chunk sizes?
- **Lambda concurrency limits**: if we use S3-triggered Lambda for processing, what's the concurrency ceiling?
- **CloudFront upload limits**: if uploading through CloudFront, are there size or timeout constraints?
- **Cost per operation**: PUT vs. GET vs. storage vs. transfer costs. How does chunk size affect total cost?

**DevOps + Architect**: document the specific AWS constraints that affect our transfer engine design. Recommend optimal chunk sizes, parallelism levels, and prefix strategies.

---

## Performance Testing Plan

The transfer engine needs comprehensive performance testing:

### What to Test

- **Throughput**: upload and download speed at various file sizes (1 MB, 10 MB, 100 MB, 1 GB, 10 GB)
- **Chunk size optimisation**: what chunk size gives the best throughput for different file sizes and network conditions?
- **Parallel transfer sweet spot**: how many concurrent chunk uploads/downloads before diminishing returns? (Hypothesis: 4–6, but benchmark it)
- **Compression ratios**: for different file types (text, images, video, archives), what's the compression ratio and time trade-off?
- **Encryption overhead**: how much time does client-side encryption add per chunk? Is it the bottleneck?
- **Resume efficiency**: after a simulated failure, how quickly does the transfer resume? What's the overhead of the manifest check?
- **Multi-runtime comparison**: same tests across browser, Node.js, Deno, Bun. Where are the performance differences?

### How to Test

With the pure JS / multi-runtime architecture, most of these tests can run from the command line:

```bash
# Benchmark upload at different chunk sizes
sgraph-send bench --file 1GB --chunk-sizes "512KB,1MB,2MB,4MB,8MB" --parallel 4

# Benchmark parallel upload connections
sgraph-send bench --file 100MB --chunk-size 2MB --parallel-range "1,2,4,6,8,12"

# Benchmark compression algorithms
sgraph-send bench --file 100MB --compression "none,lz4,zstd,brotli"

# Stress test: sustained upload/download over time
sgraph-send stress --duration 1h --file-size-range "1MB-100MB" --concurrency 10
```

This is where the CLI version pays for itself immediately.

---

## Implementation Phases

### Phase 1: Core Engine and CLI

- Define the adapter interfaces (storage, crypto, compression, transport, UI, cache)
- Implement the core engine (chunking, manifest, retry, resume, progress events)
- Implement Node.js adapters (local filesystem storage, Node crypto, CLI output)
- Implement basic S3 adapter (presigned URL upload, multipart where beneficial)
- Build the CLI tool
- Write the test suite — run entirely from command line against local storage

### Phase 2: Browser Integration

- Implement browser adapters (Fetch transport, Web Crypto, IndexedDB cache, DOM events)
- Integrate into SGraph Send UI
- Add the three-level progress display
- Add `beforeunload` navigation guard
- Add pause/resume/cancel controls

### Phase 3: Advanced Features

- Compression pipeline (compress → encrypt → upload)
- Streaming mode (download starts before upload finishes)
- Transfer manifest visible to downloader (live upload progress on download page)
- Bandwidth throttling / background mode
- WebRTC peer-to-peer option

### Phase 4: Optimisation and Scale

- Content-addressable chunks (deduplication, delta uploads)
- Multi-file bundling
- Platform-native optimisations (S3 Transfer Acceleration, multipart fine-tuning)
- WASM for specific bottlenecks (if identified by profiling)
- Folder sync capability

---

## Research Task Summary

| Research Item | Owner(s) | Priority |
|---|---|---|
| Platform-native large file upload: S3 multipart, Transfer Acceleration, presigned URLs, official JS SDK capabilities | Architect, DevOps | **P1** |
| Azure Blob, GCP Cloud Storage, Cloudflare R2, Backblaze B2 — equivalent capabilities | Architect | **P2** |
| Existing libraries: tus protocol, Uppy, Resumable.js — evaluate as foundations | Architect, Dev | **P1** |
| WebRTC data channels for peer-to-peer file transfer (simple-peer, PeerJS) | Architect | **P2** |
| Browser storage APIs: IndexedDB limits, OPFS capabilities, per-browser behaviour | Dev | **P1** |
| S3 rate limits, multipart constraints, cost per operation, optimal chunk sizing | DevOps, Architect | **P1** |
| Compression algorithms in JS/WASM: fflate, zstd-wasm, brotli — performance comparison | Dev | **P2** |
| Lambda streaming responses via Web Adapter — can we bypass the 6 MB limit for downloads? | DevOps, Architect | **P3** |
| Adapter interface design for multi-runtime (browser, Node, Deno, Bun) | Architect, Dev | **P1** |
| Content-addressable storage and delta/diff upload feasibility | Architect | **P3** |

---

## For the Conductor

This is a foundational workstream that will run in parallel with the UX and Server Analytics work. The transfer engine is the product's core — everything else (encryption, sharing, themes, analytics) sits on top of it.

Phase 1 (core engine + CLI + tests) is the priority. The pure JS / multi-runtime decision means we can develop and test faster than if we built browser-first. By the time this code runs in a browser, we'll know it works.

The platform-native research (S3 multipart, tus protocol, existing libraries) should happen before major implementation — we want to build on the best available foundation, not rediscover what's already solved.

This component will eventually be extracted as a standalone project. Design it that way from the start — clean interfaces, pluggable adapters, no SGraph Send-specific logic in the core engine.

Let's build the engine.
