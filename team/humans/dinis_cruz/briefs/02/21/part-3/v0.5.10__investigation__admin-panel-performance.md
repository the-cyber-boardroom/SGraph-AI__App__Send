# Investigation Brief: Admin Panel Performance

**version** v0.5.10  
**date** 21 Feb 2026  
**from** Human (project lead)  
**to** Developer, QA  
**type** Investigation brief — performance root cause analysis  
**status** ⛔ BLOCKED — waiting for human to provide guidance on `osbot-utils` performance testing utilities. Do not start implementation until that guidance is received.  

---

## Why This Matters

The admin panel has two distinct performance problems running on localhost. Both are orders of magnitude worse than they should be. The root causes are likely to affect other parts of the platform (data rooms, fleet management, the public team site), so understanding them now prevents compounding problems later.

There are also S3 billing spikes that are suspected to correlate with the expensive backend queries. Finding the root cause here may also explain unexpected infrastructure costs.

---

## Problem 1: Static Request Latency

**Expected**: 1–50ms for static files served from localhost  
**Actual**: ~800–900ms per request

These are static assets — HTML, JS, CSS, images. There is no reason for them to take nearly a second on localhost. Something is fundamentally wrong in the request path.

### Possible Root Causes to Investigate

| Hypothesis | What to Check |
|---|---|
| **No caching / cache disabled** | Are `Cache-Control` headers being set? Is the browser re-fetching on every request? Check response headers. |
| **Unnecessary logging on every request** | Is the FastAPI middleware logging every static file request to S3 or a database? Walk through the middleware chain. |
| **Authentication check on static files** | Is the auth middleware running (and making backend calls) for static file requests? Static files should bypass auth entirely. |
| **File system access pattern** | Is FastAPI reading from disk on every request instead of serving from memory? Check `StaticFiles` mount configuration. |
| **Middleware overhead** | How many middleware layers does every request pass through? Profile the middleware chain. |
| **DNS resolution on localhost** | Is something resolving `localhost` through DNS instead of using `127.0.0.1`? Unlikely but check. |

### How to Investigate

Start with a single static file request. Trace it through every layer:

```
Browser request → FastAPI receives → middleware chain → static file handler → response
```

Measure time at each boundary. The spike will be obvious. Then zoom in on the slow layer and repeat.

---

## Problem 2: Backend API Call Latency

**Expected**: <100ms for simple API calls on localhost  
**Actual**: 
- Health endpoint: **5–10 seconds**
- Auth token stats: **~5 seconds**  
- File listing: **~5 seconds**

These are API calls that should be fast: health is a ping, stats is a count, listing is an index read. Multi-second responses indicate expensive operations happening synchronously.

### Possible Root Causes to Investigate

| Hypothesis | What to Check |
|---|---|
| **Expensive S3 queries** | Is the health endpoint listing S3 objects? Is the stats endpoint scanning buckets? Check what S3 API calls each endpoint makes. This likely correlates with the S3 billing spikes. |
| **No pagination / full scans** | Are listing endpoints loading ALL objects instead of paginating? An S3 `list_objects_v2` on a large bucket is slow and expensive. |
| **Synchronous S3 calls in series** | Are multiple S3 calls being made sequentially when they could be parallelised? |
| **Missing indexes / inefficient data structure** | Is the auth token stats endpoint scanning all tokens instead of maintaining a count? |
| **Network round-trips to AWS** | Even on localhost, the FastAPI server calls S3 in AWS. Each call has network latency. Count the number of S3 API calls per endpoint. |
| **Cold start / connection pooling** | Is a new S3 connection being created per request instead of reusing a connection pool? |

### How to Investigate

For each slow endpoint, trace the backend execution:

```
API request → route handler → what S3 calls are made? → how long does each take? → response
```

Instrument the S3 calls specifically. Count them. Time them. The pattern will reveal whether it's "one slow call" or "fifty fast calls that add up."

---

## Test Approach: Zoomed-In Test Infrastructure

The human has existing tests that confirm both problems. These tests need to be adapted to the `osbot-utils` performance testing framework.

### Important: Light Mode Only

The `osbot-utils` performance framework is designed for internal testing and by default makes **50–100 requests per check** to compute averages. For this investigation, use **light mode only — 1–2 requests per check**. We're looking for a baseline benchmark, not statistical significance. The numbers are so far off (800ms vs 50ms, 5s vs 100ms) that one request is sufficient to see the problem.

### The Zoomed-In Pattern

Start broad, then progressively narrow:

```
Level 1: Full endpoint test
  "GET /admin/index.html takes 850ms"
  → confirms the problem exists, gives baseline

Level 2: Decompose the request path
  "Middleware chain takes 800ms, file serving takes 50ms"
  → identifies which layer is slow

Level 3: Zoom into the slow layer
  "Auth middleware makes 3 S3 calls, each takes 250ms"
  → identifies the specific operation

Level 4: Root cause
  "Auth middleware calls list_objects_v2 on the tokens bucket 
   for every request, including static files"
  → the fix is obvious
```

Each level is a test. The test suite becomes the documentation of the investigation. When we fix the root cause, the tests become regression tests.

---

## Suspected Correlation: S3 Billing Spikes

The S3 billing has spikes that may be caused by the excessive API calls. If every static file request triggers S3 calls (auth checks, logging), and every API endpoint scans S3 buckets, then normal admin panel usage generates far more S3 API calls than expected.

Once the root causes are identified, cross-reference:
- How many S3 API calls per page load (static + API)?
- Multiply by number of page loads per day
- Compare with S3 billing data (LIST, GET, PUT call counts)

This may explain the billing spikes entirely.

---

## Deliverables

| # | Deliverable |
|---|---|
| 1 | Test suite adapted to `osbot-utils` performance framework (light mode) |
| 2 | Baseline measurements for all slow endpoints (static + API) |
| 3 | Root cause analysis for static request latency (with evidence) |
| 4 | Root cause analysis for API call latency (with evidence) |
| 5 | Count of S3 API calls per admin page load |
| 6 | Correlation analysis: S3 call count vs billing spikes |
| 7 | Recommended fixes (with expected improvement per fix) |

---

## Reminder

**⛔ Do not start until the human provides guidance on the `osbot-utils` performance testing utilities.** The framework has specific patterns for benchmark tests, and using it incorrectly (e.g., the default 50-100 request mode) will generate excessive load and potentially inflate S3 costs further — exactly the problem we're trying to diagnose.

---

This document is released under the Creative Commons Attribution 4.0 International licence (CC BY 4.0). You are free to share and adapt this material for any purpose, including commercially, as long as you give appropriate credit.
