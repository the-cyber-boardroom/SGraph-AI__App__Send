# Developer Brief: Admin Agent Chat

**version** v0.5.10  
**date** 21 Feb 2026  
**from** Human (project lead)  
**to** Developer (lead), Designer  
**type** Developer brief — new admin page  

---

## What to Build

An "Ask Agent" page in the admin panel where the human can talk directly to any of the project's 19 roles. The agent's context (role definition, recent work, current status) is loaded from publicly available markdown files, combined with a project introduction, and sent to an LLM.

**Location**: `/admin/agent-chat.html`

---

## How It Works

```
1. User selects an agent (e.g., "Architect", "AppSec", "Ambassador")
2. Page fetches the agent's context files via GET requests:
   - ROLE.md (role definition)
   - Latest briefs/tasks created by that role
   - Status/updates
3. Page constructs an LLM prompt:
   - Project introduction (fixed preamble)
   - Role definition (from ROLE.md)
   - Recent work context (latest briefs/tasks)
   - User's question
4. Page sends the prompt to the selected LLM provider
5. API key for the LLM provider is held by the Browser Pod (see architecture brief)
   — the page never sees the raw API key
6. Response displayed in a chat interface
```

---

## LLM Providers

Support five providers from day one. The user selects which to use per conversation:

| Provider | Endpoint | Where It Runs | Notes |
|---|---|---|---|
| **Anthropic (Claude)** | `api.anthropic.com/v1/messages` | Cloud | Primary. Human's preferred for complex reasoning. |
| **OpenAI** | `api.openai.com/v1/chat/completions` | Cloud | Wide model selection. |
| **OpenRouter** | `openrouter.ai/api/v1/chat/completions` | Cloud | Access to many models via single API key. |
| **Ollama** | `localhost:11434/api/chat` | Local | No API key needed. Models run on user's machine. |
| **Chrome AI (Gemini Nano)** | `window.ai` API | Local (in-browser) | No API key. Currently Chrome Canary only. Human has working implementation. |

### Provider Selection UI

```
┌─────────────────────────────────────────────────────────────────┐
│  LLM Provider:  [Anthropic (Claude)  ▾]                        │
│  Model:         [claude-sonnet-4-5-20250514   ▾]                        │
│  Status:        ✓ API key configured (via Browser Pod)          │
└─────────────────────────────────────────────────────────────────┘
```

For Ollama and Chrome AI, the status shows whether the local service is reachable:
```
│  Status:        ✓ Ollama running (localhost:11434)              │
│  Status:        ✗ Chrome AI not available (requires Chrome Canary) │
```

### Unified Call Interface

All providers are called through a common interface:

```javascript
async function chatCompletion({ provider, model, messages, apiKey }) {
  switch (provider) {
    case "anthropic":
      return await callAnthropic(model, messages, apiKey);
    case "openai":
    case "openrouter":
      return await callOpenAICompatible(provider, model, messages, apiKey);
    case "ollama":
      return await callOllama(model, messages); // no API key
    case "chrome-ai":
      return await callChromeAI(messages); // no API key, in-browser
  }
}
```

The Browser Pod (see `v0.5.10__architecture__browser-pod.md`) handles API key injection for cloud providers. For Ollama and Chrome AI, no pod involvement is needed — there's no secret to protect.

---

## Agent Context Loading

### Context Sources

Agent context files are publicly available on GitHub and can be fetched via GET requests. Two source options:

| Source | URL Pattern | Pros | Cons |
|---|---|---|---|
| **GitHub raw** | `raw.githubusercontent.com/{org}/{repo}/main/roles/{role}/ROLE.md` | Always up to date | Rate limited (60 req/hr unauthenticated) |
| **GitHub Pages** | `{org}.github.io/{repo}/roles/{role}/ROLE.md` | Fast CDN, no rate limits | Needs GH Pages deployment (see Librarian brief) |

**Recommendation**: use GitHub Pages as the primary source (fast, no rate limits, cacheable). Fall back to raw GitHub if Pages is unavailable.

### Prompt Construction

The LLM prompt is assembled from three layers:

```
┌─────────────────────────────────────────────────┐
│  Layer 1: Project Introduction (fixed preamble) │
│  - What is SG/Send                              │
│  - Current version (v0.5.10)                    │
│  - Architecture overview (brief)                │
│  - The 19-role team structure                   │
│  - Current priorities                           │
│  - Key principles (zero-knowledge, PKI, etc.)   │
├─────────────────────────────────────────────────┤
│  Layer 2: Role Definition (from ROLE.md)        │
│  - Role name and purpose                        │
│  - Responsibilities                             │
│  - Current focus areas                          │
│  - Key decisions made                           │
│  - Relationships with other roles               │
├─────────────────────────────────────────────────┤
│  Layer 3: Recent Context (latest work)          │
│  - Last N briefs/tasks created by this role     │
│  - Current status / open items                  │
│  - Recent decisions and their rationale         │
├─────────────────────────────────────────────────┤
│  Layer 4: User's Question                       │
│  - The actual question from the human           │
└─────────────────────────────────────────────────┘
```

The system prompt sets the agent's persona:

```
You are the {role_name} for the SG/Send project. You have been given 
your role definition and recent context below. Answer questions from 
the project lead's perspective, staying in character as this role. 
Reference specific briefs, decisions, and technical details from your 
context when relevant.
```

---

## The Chat UI

```
┌─────────────────────────────────────────────────────────────────┐
│  Admin  ›  Agent Chat                                          │
├─────────────────────────────────────────────────────────────────┤
│                                                                 │
│  Agent: [Architect         ▾]    Provider: [Anthropic    ▾]    │
│  Context loaded: ✓ ROLE.md, ✓ 3 recent briefs                 │
│                                                                 │
│  ┌──────────────────────────────────────────────────────────┐   │
│  │                                                          │   │
│  │  You: What's the current status of the trust graph       │   │
│  │  implementation and what should we prioritise next?      │   │
│  │                                                          │   │
│  │  Architect: Based on the chain-of-trust architecture     │   │
│  │  brief (v0.4.12), the trust graph data model needs to    │   │
│  │  be designed in Issues-FS. The PKI key management is     │   │
│  │  built and deployed. The key discovery UI is the         │   │
│  │  immediate next priority. For the trust graph            │   │
│  │  specifically, I'd recommend...                          │   │
│  │                                                          │   │
│  │  You: How does this connect to the data room work?       │   │
│  │                                                          │   │
│  │  Architect: The data room directory IS a trust graph,    │   │
│  │  stored as JSON in a GitHub repo. The v0.5.10            │   │
│  │  architecture brief established that...                  │   │
│  │                                                          │   │
│  └──────────────────────────────────────────────────────────┘   │
│                                                                 │
│  ┌──────────────────────────────────────────────────────────┐   │
│  │  Ask the Architect...                                    │   │
│  └──────────────────────────────────────────────────────────┘   │
│  [Send]                                          [Clear Chat]   │
│                                                                 │
│  Context:  [View loaded context]  [Reload context]             │
│  Tokens:   1,247 in / 892 out (this session)                   │
│                                                                 │
└─────────────────────────────────────────────────────────────────┘
```

### Features

- **Agent selector**: dropdown of all 19 roles. Changing agent reloads context.
- **Provider selector**: dropdown of configured providers. Shows status (API key configured, service reachable).
- **Context viewer**: expandable panel showing exactly what markdown was loaded. Useful for debugging and verifying the agent has the right context.
- **Token counter**: running count of tokens in/out for the session. Feeds into budget controls.
- **Multi-turn conversation**: maintains chat history within the session. History is sent with each request (standard chat completion pattern).
- **Clear chat**: resets the conversation but keeps the agent context.
- **Reload context**: re-fetches the agent's markdown files (useful after pushing updates to GitHub).

---

## Browser Pod Integration

Cloud LLM providers (Anthropic, OpenAI, OpenRouter) require API keys. These keys are stored in the Browser Pod (see `v0.5.10__architecture__browser-pod.md`), not in the page's JavaScript.

The flow:

```
1. User sends a message
2. Page constructs the full prompt (context + history + question)
3. Page sends the prompt to the Browser Pod (via postMessage)
4. Pod:
   a. Retrieves the API key from its encrypted store
   b. Checks budget (has this page exceeded its call limit?)
   c. Makes the API call to the LLM provider
   d. Returns the response to the page (via postMessage)
5. Page displays the response
```

The page never sees the API key. The Pod enforces rate limits and budgets. If the page's JavaScript is compromised (XSS, malicious extension), the attacker can trigger LLM calls but:
- Each call is rate-limited
- The budget caps total usage
- The API key itself is never exposed
- The Pod can be nuked and recreated with a new key

For Ollama and Chrome AI, the page calls them directly — no secrets involved.

---

## File Structure

```
/admin/
├── index.html
├── pki.html
├── key-lookup.html
├── key-registry.html
├── fleet.html
├── secrets.html
├── agent-chat.html       ← NEW
├── agent-chat.js         ← NEW (chat UI, context loading, provider abstraction)
├── browser-pod.js        ← NEW (Web Worker / Service Worker — see architecture brief)
└── shared/
    ├── pki-common.js
    └── llm-providers.js  ← NEW (unified interface for all 5 providers)
```

---

## Acceptance Criteria

| # | Criterion |
|---|---|
| 1 | Can select any of the 19 agents from a dropdown |
| 2 | Agent's ROLE.md and recent context loads via GET request |
| 3 | Can send a question and receive an LLM response |
| 4 | Supports Anthropic, OpenAI, OpenRouter, Ollama, and Chrome AI |
| 5 | API keys for cloud providers are stored in Browser Pod, not in page JS |
| 6 | Multi-turn conversation works (history maintained) |
| 7 | Token counter shows usage per session |
| 8 | Context viewer shows exactly what markdown was loaded |
| 9 | Changing agent reloads context |
| 10 | Provider status shows whether the service is reachable |

---

This document is released under the Creative Commons Attribution 4.0 International licence (CC BY 4.0). You are free to share and adapt this material for any purpose, including commercially, as long as you give appropriate credit.
