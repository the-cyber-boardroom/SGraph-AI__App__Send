# Villager Team: Daily Brief — 16 February 2026

**version** v0.4.4  
**date** 16 Feb 2026  
**from** Human (project lead)  
**to** Villager team via Conductor  

---

## The Good News

We have shipped our first proper MVP. It's looking professional — design guidelines applied, nice colour scheme, end-to-end flow working. This is a real product that real people can use.

Now we need to productise it properly. That's today's focus.

---

## Production Environment Setup

### Subdomain Architecture

At the moment, dev is wired directly into production. That needs to change. We need three distinct environments with their own subdomains:

```
dev.send.sgraph.com     → Development (Explorer team pushes here)
qa.send.sgraph.com      → QA / Staging (pre-production verification)
send.sgraph.com         → Production (what users see)
```

This requires CloudFront distribution configuration and Route 53 DNS records for each subdomain.

**DevOps**: create a mapping document that details the CloudFront distribution settings, Route 53 records, S3 bucket mappings, and Lambda/API Gateway configurations for each environment. This document is both the plan and the checklist.

### The Production Discipline

Now that we're touching production, every change follows a strict workflow:

```
1. IDENTIFY    → Document the problem or change needed
2. TEST FIRST  → Write tests that confirm the problem exists (passing tests 
                 in the broken environment, failing in the fixed one)
3. PLAN        → Design the fix, consider second-order effects
4. RISK ASSESS → What could go wrong? What's the rollback plan?
5. MIGRATE     → Consider before/after state, transition period
6. DEPLOY      → Execute the change (dev → QA → prod)
7. VERIFY      → Run post-deployment tests, confirm the fix
```

This is especially important for infrastructure changes. Example: when I changed DNS entries from one distribution to another, there was a moment where production could have broken — the old distribution was gone but the new one might not have propagated. We need **transition tests** that verify the change at every stage:

- Test the current state (before change)
- Test in a quasi-production environment (simulated change)
- Execute the change
- Test the new state (after change)
- Have a rollback plan if the after-state tests fail

**QA**: start thinking about integration tests and end-to-end tests for the production environment. We need automated ways to verify that the website is up, the upload/download flow works, the API responds correctly, and the SSL certificates are valid. These tests run before and after every deployment.

**CISO + AppSec**: for every change that touches production, assess the risk. What could break? What's the blast radius? What's the rollback plan?

---

## Server Analytics: The LETS Pipeline in Detail

We now have four S3 buckets receiving data from various AWS services. There's a lot of information flowing in that we need to process. Today we build the first end-to-end pipeline, using **CloudFront logs** as the worked example.

### Two Types of Data (Recap)

**Data we receive** (event-driven): logs that AWS generates automatically — CloudFront access logs, CloudWatch logs, CloudTrail audit logs, S3 access logs. Raw, high-volume, needs transformation.

**Data we collect** (request-driven): information we actively fetch from AWS APIs — Lambda configurations, Route 53 settings, CloudFront distribution settings. Point-in-time snapshots of infrastructure state.

### The Digital Twin: Collect Infrastructure State

Before processing logs, we need to know what our infrastructure looks like. This is the "digital twin" — a cached representation of our AWS setup.

For the CloudFront pipeline, we need to collect (via boto3 API calls):

1. **Route 53** — DNS records, hosted zones, domain mappings
2. **CloudFront** — distribution settings, origins, behaviours, cache policies
3. **Firehose** — data stream configuration, delivery settings
4. **Lambda** — function configurations for any processing functions
5. **S3** — bucket configurations for log storage

Each collection saves to the cache service using the **latest + temporal** model. This gives us current state and historical change tracking.

Build a **graph** that connects these components — Route 53 → CloudFront → Firehose → S3, with Lambda processing in between. This graph IS the digital twin. Save it. Visualise it.

**Dev + Architect**: implement the collectors for each AWS service. One boto3 call per service, save the relevant configuration (not the full AWS response — only the ~20 fields we care about) to the cache service.

### CloudFront Log Pipeline: Step by Step

This is our first complete LETS pipeline. Every step produces a typed, saved artefact.

#### Step 1: Raw Ingestion

CloudFront logs arrive via Firehose into an S3 bucket. The current format is tab-delimited text files. 

First transformation: **convert raw tab-delimited data into typed JSON structures.** Each field gets a proper name and type. This is a pure format conversion — no data is dropped yet.

Save the typed JSON to the cache service under a `raw/` prefix. This is the first transformation checkpoint — everything downstream works from this typed version, never from the original tab-delimited data.

```
S3 (tab-delimited) → [Transform] → Cache Service (typed JSON, raw/)
```

#### Step 2: Consolidation

Take the typed JSON and consolidate it. This is where the 90% compression happens:

- Group identical requests (same URL, same method, same response code) into counts
- Extract unique values (unique IPs — wait, we're not capturing IPs. Unique user agents, unique paths, etc.)
- Remove redundant structure (log format boilerplate that repeats on every line)
- Produce a compressed, meaningful summary

Save the consolidated data to the cache service under a `consolidated/` prefix.

```
Cache Service (raw/) → [Consolidate] → Cache Service (consolidated/)
```

#### Step 3: Temporal Aggregation

Aggregate the consolidated data into time-based buckets:

```
30 minutes → hourly → daily → monthly
```

Each aggregation level is computed from the level below it. Each is saved to the cache service with temporal paths.

**The finality concept**: a time bucket is either **final** or **partial**:

- **Final**: the time period has completed. All data is in. This aggregation will never change. Cache it permanently.
- **Partial**: the time period is still in progress (e.g., it's 10 minutes into the current hour). This aggregation is provisional. Next time it's requested, recalculate by loading what we have and adding new data.

When a partial bucket becomes final (the hour/day/month completes), compute the final aggregation and mark it as final. From that point on, it's just a file read — no computation needed.

This applies fractally: a day aggregation is partial until all 24 hourly aggregations are final. A monthly aggregation is partial until all daily aggregations are final.

```
30-min (final) + 30-min (final) → hourly (final)
hourly × 24 (all final)         → daily (final)
daily × 28-31 (all final)       → monthly (final)
```

**The key optimisation**: only calculate what's missing. If I ask for the last 24 hours and I already have 20 hours cached as final aggregations, I only need to compute the remaining 4 hours (of which the current hour is partial). The manifest tracks what's been computed and what's final.

#### Step 4: Archive and Clean

Once raw data has been processed:
1. Move the original S3 log files to an **archive bucket** (cheaper storage class — S3 Glacier or Infrequent Access)
2. Delete from the live logging bucket
3. The live bucket stays clean — only unprocessed files remain

This means the live logging bucket is essentially a queue: files arrive, get processed, get archived, get deleted. At any given time, it should contain only recent, unprocessed data.

If we ever need to reprocess historical data (e.g., we add a new metric we want to extract), we rerun the pipeline against the archive. The pipeline is programmatic — it can always be re-executed.

#### Step 5: Event-Driven Automation

Once the manual pipeline is proven, automate it:
- S3 event triggers a Lambda function whenever a new log file lands
- The Lambda runs Steps 1–3 automatically
- Archive/cleanup runs on a schedule (daily)

Eventually, the entire pipeline runs without human intervention. Data arrives, gets processed, gets aggregated, gets archived. The admin dashboard shows the results in near-real-time.

### Issues FS Extension: S3 Storage

We need an extension to Issues FS that saves to S3 instead of the local filesystem. Issues FS already supports memory FS, so the jump to S3 should be manageable — it's a new storage adapter for the same data model.

This lets us store the SA pipeline metadata (manifests, processing state, aggregation status) using Issues FS patterns, but persisted in S3.

**Dev**: implement the S3 storage adapter for Issues FS.

---

## API Design: Expose Everything

Every operation in the SA pipeline needs a REST API endpoint. The principle: **the API exposes all capabilities and all workflow steps.** Each step described above becomes an API call:

```
# Digital twin / infrastructure collection
GET  /api/sa/collect/route53           → collect Route 53 config
GET  /api/sa/collect/cloudfront        → collect CloudFront config
GET  /api/sa/collect/lambda            → collect Lambda configs
GET  /api/sa/collect/s3                → collect S3 bucket configs
GET  /api/sa/collect/firehose          → collect Firehose config
GET  /api/sa/graph                     → get the infrastructure graph

# Log processing pipeline
POST /api/sa/logs/cloudfront/ingest    → transform raw logs to typed JSON
POST /api/sa/logs/cloudfront/consolidate → consolidate typed logs
GET  /api/sa/logs/cloudfront/aggregate/{period} → get aggregated data
POST /api/sa/logs/cloudfront/archive   → archive processed logs
DELETE /api/sa/logs/cloudfront/clean   → clean processed logs from live bucket

# Cache service browsing
GET  /api/sa/cache/{cache-id}          → get cache entry
GET  /api/sa/cache/{cache-id}/latest   → get latest version
GET  /api/sa/cache/{cache-id}/temporal/{date} → get temporal version

# Visualization data
GET  /api/sa/traffic/realtime          → last 5 minutes of traffic
GET  /api/sa/traffic/{date}            → traffic for a specific day
GET  /api/sa/traffic/range/{from}/{to} → traffic for a date range
```

The APIs are thin layers on top of the services — the service contains the logic, the API just exposes it. This means every operation can be triggered from the admin UI, from a script, from a cron job, or from an S3 event handler.

**Dev + Architect**: design and implement the API layer. Every pipeline step = one API endpoint.

---

## Admin UI: Visualise Everything

### What Success Looks Like

By the end of this phase, I can go to the admin panel and:

1. **See the digital twin** — a visualisation of the CloudFront → Firehose → S3 → Lambda pipeline, with real configuration data from our AWS environment
2. **Browse every data file** — raw logs, typed JSON, consolidated data, aggregated data. Navigate into each, inspect the contents.
3. **See the traffic** — answer "what happened yesterday?" and "what's happening right now?" using just the CloudFront log data
4. **Navigate via URLs** — SPA with path-based routing. Every view has a direct URL. Every component loads independently.

### Path-Based Navigation (Reminder)

```
/admin/dashboard/                               → SA overview
/admin/dashboard/twin/                          → digital twin visualisation
/admin/dashboard/twin/cloudfront/               → CloudFront config detail
/admin/dashboard/twin/route53/                  → Route 53 config detail
/admin/dashboard/logs/cloudfront/               → CloudFront log overview
/admin/dashboard/logs/cloudfront/raw/           → raw typed JSON browser
/admin/dashboard/logs/cloudfront/consolidated/  → consolidated data browser
/admin/dashboard/logs/cloudfront/aggregate/day/ → daily aggregation view
/admin/dashboard/traffic/                       → traffic dashboard (the GA replacement)
/admin/dashboard/traffic/realtime/              → real-time traffic view
/admin/dashboard/cache/                         → cache service browser
```

Each route = a self-contained HTML web component. Develop each in isolation, compose in the dashboard.

**Dev + Designer**: build the admin views. Start with the cache service browser (unblocks debugging everything else), then the digital twin visualisation, then the traffic dashboard.

---

## Journalist and Historian: Automated Daily Reports

I want to wake up every morning to a published report on what happened on the website yesterday. Two articles:

### Technical Debrief (Historian)

A detailed, technical summary published to the docs site:
- Traffic volumes (requests, unique sessions, bandwidth)
- Performance metrics (response times, Lambda execution times, error rates)
- Spikes and anomalies (unusual traffic patterns, potential attacks)
- Uptime/downtime (any outage periods, degraded performance)
- Page-level analytics (which pages got traffic, which API endpoints were hit)
- Infrastructure changes (anything that changed in the digital twin)
- Timing and performance trends (is the site getting faster or slower?)

### Commentary Article (Journalist)

A more narrative, insight-driven piece:
- Number of users, growth trends
- What users are doing (upload patterns, download patterns)
- Interesting observations (feeding from the Sherpa's trail analysis and the Ambassador's market perspective)
- How this compares to previous periods (feeding from the Historian's trend data)
- Key takeaways for the team

### Workflow

Initially manual: trigger the journalist and historian roles, provide them the day's SA data, they produce the articles.

Eventually automated: a scheduled process (daily, early morning) that:
1. Aggregates the previous day's SA data (should already be final by morning)
2. Triggers the journalist and historian roles via the API
3. Publishes the articles to the docs site
4. Notifies the human (email or Slack)

**Journalist + Historian**: define the article templates. What data do you need? What format should the articles take? **Sherpa + Ambassador**: feed into the journalist with trail analysis and market context.

---

## User Acquisition: Beta Programme Workflow

### The Invite Workflow

When I want to invite someone to the beta programme, I need a smooth end-to-end flow:

1. **Create a token** (admin UI or API)
2. **Distribute the token** — via LinkedIn message, email, text message, or physical card
3. **Track the invite** — who was invited, when, which token, have they used it?
4. **Monitor their usage** — once they start using it, what's their traffic? What feedback do they give?
5. **Close the loop** — follow up with them, get feedback, iterate

### Marketing Copy (Ambassador)

The Ambassador should produce ready-to-use text for multiple channels:

- **LinkedIn post** — a short, compelling post I can share to invite people to try the beta. Multiple variants (technical audience, business audience, security-focused audience).
- **LinkedIn DM** — a personal message template for direct outreach
- **Email** — a more detailed invitation with context
- **Text/WhatsApp message** — short and casual, with link and access code
- **Tweet/X post** — ultra-short

**Ambassador**: produce these. Multiple variants per channel. Include the URL, a one-line value prop, and instructions for using the access code.

### Physical Cards

I want to be able to hand someone a physical card — at a café, at a meetup, at a conference — that gives them everything they need to try the product:

- The URL (send.sgraph.com)
- A one-line explanation of what it does
- A unique access code (token) printed on the card
- QR code that goes directly to the site (possibly with the token pre-loaded in the URL)
- Clean, professional design consistent with our brand identity

**Designer**: create a card design template. It should work for printing at home (A4 sheet with multiple cards, cut lines) and for professional printing (standard business card size, 85mm × 55mm). The token/QR code area should be blank so we can print batches with different tokens.

**Dev**: can we generate QR codes that pre-load the token? Something like `send.sgraph.com/?token=community-beta-001` encoded as a QR code.

### User Feedback Platform (Research)

We need a way to capture feedback from beta users. Options to research:

- **GitHub Issues** — simple, free, already part of our workflow. But requires users to have a GitHub account.
- **In-app feedback widget** — a form on every page of the website that submits feedback
- **GPT/Claude bot** — a conversational feedback collector that can ask follow-up questions
- **Dedicated platforms** — Canny, UserVoice, Hotjar, etc. — evaluate what exists before building our own
- **Using SGraph Send itself** — a feedback token (e.g., `customer-support`) that allows users to submit encrypted messages through our own product. Dogfooding at its finest.

**Sherpa + Ambassador**: research existing feedback platforms. What's the simplest, lowest-friction option? What do other early-stage products use?

### Using SGraph Send for Feedback (Exploring)

This is an interesting idea worth developing: use our own product for customer feedback.

The concept:
- A special token (e.g., `feedback` or `support`) is available on every page
- The user writes a message or uploads a file
- It's submitted through our normal upload flow — encrypted, stored, tracked
- We see it in the admin panel as a new upload linked to the `support` token
- We can monitor all submissions, track response times, close the loop

**Encryption option**: the access token itself could serve as the encryption key (or derive one from it). This means we can decrypt submissions using the same token that authorised them.

**Future: identity-linked encryption**: when we add authentication (e.g., Cognito), the user's unique ID (Cognito `sub`) could be used to derive the encryption key. This proves that only that authenticated user could have sent the message — it's signed by identity. Even more powerful with PKI: a public key on the website encrypts the message, only the recipient with the private key can read it. Data is encrypted at the gate, decrypted only by the intended recipient. This is the PCI pattern applied to customer communication.

This is all roadmap — don't build it today. But the architecture should be aware that access tokens and user identity will eventually participate in the encryption model.

**Architect**: capture this pattern. Map out how token-based and identity-based encryption could work for customer feedback submissions.

### Server-Side Workflows on File Upload

Related to the feedback concept: we should have **server-side workflows that trigger when a file is uploaded**. The trigger is the S3 event (file written), and the workflow depends on the access token used:

- `support` token → route to customer support queue
- `beta-feedback` token → route to feedback analysis
- Standard token → no special workflow (just normal file transfer)
- Future: custom per-token workflows (configurable via admin)

This is the beginning of **token-based routing** — the access token doesn't just control access, it controls what happens to the file after upload.

**Architect**: design the server-side workflow trigger model. Start simple (S3 event → Lambda → check token → route), plan for extensibility.

---

## New Role: The Translator (Villager-Exclusive)

This is a milestone: **the first role that exists only in the Villager team.** The Explorer team doesn't need this — they communicate in English internally and nothing they produce goes directly to users. But the Villager team ships to production, and everything that reaches users must be translated.

The Translator handles two types of translation:

### Language and Cultural Translation

All user-facing content must exist in all supported languages. Current targets:
- **English (en)** — the internal production language. All content is created in English first.
- **Brazilian Portuguese (pt-BR)** — first translation target.
- **European Portuguese (pt-PT)** — second translation target. Already in the language selector.
- **Klingon (tlhIngan Hol)** — an Easter egg for the tech community. In the language picker as a nod to our audience. No full content needed — it does its job just by being there.

These are deliberately three different cultures (not just three languages). English, Brazilian Portuguese, and European Portuguese each have different idioms, different levels of formality, different expectations for how software should talk to you. A Brazilian reading pt-PT text (or vice versa) will notice immediately — "ficheiro" vs "arquivo" is just the start.

The pipeline: content is created in English → Translator produces initial translation (using LLM) → **human native speaker reviews and corrects** → corrections feed back into translation memory → approved version goes live.

**The human reviewer step is non-negotiable.** LLMs are English-centric. They produce translations that are technically correct but often miss cultural nuance, current slang, regional tone. The human reviewer catches what the machine cannot. The Translator's job is not to replace human translators — it's to give them a 90%-done draft so they can focus on the 10% that requires native intuition.

**Immediate task**: the current MVP website has untranslated English content on pages that should be multilingual. Audit and fix this now.

### Audience Translation

The same information needs to reach different audiences in different forms:
- A developer needs commit hashes and latency numbers
- A business stakeholder needs "uploads are 40% faster"
- An end user needs "we made sending files faster"
- The project lead needs all three, at different times

The Translator takes content produced by other roles and reframes it for each audience. Like the Designer (everything needs design), the Translator is cross-cutting — everything that reaches an audience needs translation.

### Research: Language-Specific Models

Part of the Translator's remit: find and evaluate LLM models that are natively trained in Portuguese/Brazilian Portuguese. The major LLMs are English-centric. There may be models that produce more natural translations. Research, evaluate, propose.

### Research: Language-Specific Models — Practical Access

Part of the Translator's remit: find and evaluate LLM models natively trained in Portuguese/Brazilian Portuguese. But finding them isn't enough — we need to know: **can we use them today, via API?** Are they on OpenRouter, HuggingFace Inference API, Replicate, Together AI? Can we call them from Lambda or Node.js? Cost is not the blocker (we already pay for Claude and ChatGPT). Access is the question.

### Using SG/Send for Translator Workflows

The human reviewer process should use our own product. The Translator sends a structured translation file (original + proposed translation per string) to the reviewer via SG/Send. The reviewer opens it, reviews, corrects, sends back. Every review is tracked, encrypted in transit, and generates real usage data for our SA pipeline. Dogfooding that improves the product.

### The Non-English Market Hypothesis

There's likely a bigger market for this solution in non-English-speaking countries. English-speaking markets are saturated with file sharing options. Non-English markets are underserved — especially for products that feel culturally native, not translated as an afterthought. Every language we add is a market we can credibly enter. The Translator is not just a quality role — it's a market strategy.

Once the translation pipeline is proven for en → pt-BR and en → pt-PT, the plan is to scale to many languages rapidly. The pipeline is the same; only the human reviewers and language-specific models change. In principle, we can support any language the LLMs support.

**The enterprise play**: as we commercialise SG/Send for corporate customers, the Translator capability becomes a major differentiator. A multinational doesn't have one language — it has dozens. The mothership, each subsidiary, and crucially each **acquired company** (who want to retain their brand identity) all need content in their specific language, culture, and brand voice. We can offer **translation profiles** per corporate entity — same information, translated into each entity's specific identity. This is high-value enterprise pricing territory, and the infrastructure we're building today for our own en → pt-BR pipeline is the exact same pipeline that powers it. See the Translator role definition for the full vision.

See the full Translator role definition for the complete specification, workflow details, and the three-phase evolution from internal role to customer-facing product.

**Note**: the more innovative ideas that emerged from this discussion — encrypted JSON forms with dynamic UIs, client-side LLM translation as a product feature, LLM-generated interactive workflows — have been captured in a separate Explorer brief (`v0.4.3__explorer-brief__interactive-encrypted-data-workflows.md`). Those are Explorer territory. The Villager team should be aware that these features are coming, because the translation pipeline being built today is the foundation they'll build on.

---

## Task Summary

| Priority | Task | Owner(s) |
|----------|------|----------|
| **P1** | Set up dev/QA/prod subdomain architecture (CloudFront + Route 53) | DevOps |
| **P1** | Create environment mapping document (settings, checklist, rollback plan) | DevOps |
| **P1** | Build CloudFront log pipeline: raw → typed JSON → consolidated → aggregated | Dev, Architect |
| **P1** | Implement digital twin collectors (Route 53, CloudFront, Firehose, Lambda, S3) | Dev |
| **P1** | Design and implement SA REST API (all pipeline steps as endpoints) | Dev, Architect |
| **P2** | Build admin UI views: cache browser, digital twin, traffic dashboard | Dev, Designer |
| **P2** | Implement temporal aggregation with finality tracking | Dev |
| **P2** | Implement log archive and cleanup workflow | DevOps |
| **P2** | Write integration tests for production environment (uptime, flow, SSL) | QA |
| **P2** | Implement Issues FS S3 storage adapter | Dev |
| **P2** | Produce daily report templates (journalist: commentary, historian: technical) | Journalist, Historian |
| **P1** | Audit current website for untranslated English content | Translator |
| **P1** | Produce pt-BR and pt-PT translations for all user-facing website text | Translator, Dev |
| **P2** | Establish translation pipeline (trigger, review, approve, deploy) | Translator, Architect, DevOps |
| **P2** | Create glossary of key terms with approved pt-BR translations | Translator |
| **P2** | Find native speakers for review (Brazilian Portuguese + European Portuguese) | Translator, Ambassador |
| **P2** | Research language-specific LLM models: find, evaluate, check API access (OpenRouter, HuggingFace, etc.) | Translator |
| **P2** | Set up SG/Send workflow for translator ↔ human reviewer file exchange | Translator, Dev |
| **P2** | Produce marketing copy for beta invites (LinkedIn, email, DM, text) | Ambassador |
| **P2** | Design physical/digital invite card template | Designer |
| **P3** | Research user feedback platforms (GitHub Issues, Canny, in-app widget, etc.) | Sherpa, Ambassador |
| **P3** | Explore using SGraph Send for customer feedback (token-based submissions) | Architect |
| **P3** | Design server-side workflow trigger model (S3 event → token-based routing) | Architect |
| **P3** | Research identity-linked encryption for feedback (Cognito sub, PKI) | Architect, AppSec |
| **P3** | Automate daily journalist/historian reports | Dev, Journalist, Historian |

---

## Process Update: Daily Memo Workflow (Add to .claude.md)

**Action for Conductor**: add the following workflow to the Villager team's `.claude.md` configuration file. This defines what happens every time the human provides a voice memo or daily brief.

### The Workflow

Every time the human provides a memo or brief:

1. **Architect + Librarian read first** — they are always the first to process any new input from the human.

2. **Librarian catalogues** — copies files where they need to go, updates cross-links, updates indexes, ensures nothing is lost or orphaned.

3. **They produce a debrief document** — a structured index of everything that was said, stored in the Librarian's domain. This debrief contains:
   - Summary of what the human communicated
   - Which roles/agents are impacted
   - Specific tasks or to-dos extracted for each impacted role
   - Questions for the human (anything ambiguous or needing clarification)
   - What teams should be working on next (if nothing new, confirm the existing priorities)

4. **Impacted roles update themselves** — every role mentioned or affected by the memo:
   - Updates their own `.issues` file with new tasks
   - Updates their own knowledge documents and internal guidance
   - Updates their `ROLE.md` if the memo changes their responsibilities
   - Adds roadmap items, future considerations, or notes as appropriate

5. **Human reviews the debrief** — the debrief is a "debrief on the brief." It shows the human:
   - How their input was consumed and understood
   - Whether anything was missed or misinterpreted
   - What questions the team has
   - What each team is working on next (with phases mapped by the Architect)
   - Cost-effectiveness assessment: is this the most valuable work to be doing right now?

6. **Clarifications flow back** — if the human answers questions or provides corrections, the cycle repeats (smaller this time).

### The Principle

Not every memo impacts every agent. If only one role is mentioned, only that role processes. But the Librarian and Architect ALWAYS process — they are the information routing layer.

### Cost-Effectiveness Check

The debrief should include an assessment: **what is the most cost-effective work each agent should be doing right now?** Not every agent needs to be active at all times. If an agent has no current tasks, they don't run. The Conductor manages this — spinning up agents when there's work, standing them down when there isn't.

Also see: `v0.4.3__briefs__briefing-packs-for-agents.md` — the full specification for how to package work into briefing packs for parallel agent execution.

---

## For the Conductor

Three priorities today, plus a new role:

1. **Production environment** — get dev/QA/prod subdomains set up properly. Every change follows the new production discipline: test first, risk assess, migrate carefully, verify after. This is now the permanent way of working.

2. **CloudFront log pipeline as the first end-to-end SA example** — raw ingestion → typed JSON → consolidation → temporal aggregation → visualisation. When this pipeline works for CloudFront, we apply the same pattern to CloudWatch, CloudTrail, S3 access logs, and everything else. Build it once, replicate it.

3. **User acquisition infrastructure** — the Ambassador produces copy, the Designer produces cards, we define the invite workflow. I want to be handing out access codes to every person I meet. Every new user generates data, feedback, and insight.

4. **The Translator is live.** Our first Villager-exclusive role. Immediate priority: audit and translate the current website (English content on multilingual pages is a bug). Establish the translation pipeline and the human reviewer workflow. This role is cross-cutting — from now on, every piece of content that reaches users must pass through the Translator.

The journalist and historian daily reports are medium-term but start the template design now — when the SA pipeline starts producing data, we want to be ready to turn it into daily narratives immediately.

Remember: no new features in the Villager team. The functionality exists. We're productising, deploying, monitoring, and optimising. The SA pipeline is infrastructure that helps us understand and operate the product — it's not a user-facing feature.

Let's ship it properly.
