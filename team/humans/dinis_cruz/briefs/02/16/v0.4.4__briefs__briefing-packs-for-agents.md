# Briefing Packs: Packaging Work for Agents

**version** v0.4.4  
**date** 16 Feb 2026  
**from** Human (project lead)  
**to** All teams — Architect, Librarian, Conductor, all roles  
**status** Process definition — implement immediately  

---

## The Problem

We have a growing body of documentation, source code, architecture decisions, and context. When an LLM agent needs to tackle a specific task — whether it's a bug fix, a feature implementation, or an architectural exploration — it needs the right context. Not all context. Not no context. The RIGHT context.

Right now, either:
- The agent has full repo access (Claude Code Web) and might drown in irrelevant code
- The agent has no repo access and I have to manually explain everything
- The agent gets a massive context dump (100k+ tokens) where most of it isn't relevant to the task

The solution: **briefing packs** — curated, compressed packages of exactly the context an agent needs to execute a specific piece of work.

---

## What Is a Briefing Pack?

A briefing pack is a set of documents (and optionally source code extracts) that gives an LLM agent everything it needs to perform a specific task. Nothing more, nothing less.

### Two Dimensions

**Size of work:**

| Size | Description | Pack Complexity |
|---|---|---|
| **Bug** | A single, specific bug to fix | Small — bug description, relevant code excerpt, test expectations |
| **Task** | A defined unit of work | Medium — task description, relevant architecture, code context, acceptance criteria |
| **Iteration** | A set of related tasks | Larger — iteration goals, multiple code areas, cross-cutting concerns |
| **Workstream** | A major area of work (e.g., CloudFront log pipeline) | Full pack — architecture briefs, multiple role inputs, code extracts, supporting documents |

**Agent access level:**

| Access | Context Needed | Example |
|---|---|---|
| **Full repo access** | Guidance on where to look, what matters, what to ignore | Claude Code Web session with repo cloned |
| **Packaged repo access** | Source code extracts included in the pack (relevant files/methods only) | Claude session where I drop files into context |
| **No repo access** | Everything in the documents — code quoted inline, line references, architecture explained from scratch | Claude.ai conversation, ChatGPT, any LLM without file access |

The pack is tailored to both dimensions. A workstream-level pack for an agent with no repo access is the most comprehensive. A bug-level pack for an agent with full repo access is the most minimal.

---

## How to Build a Briefing Pack

### Step 1: Librarian and Architect Lead

Every briefing pack starts with the **Librarian** and the **Architect**. They:

1. Understand the scope of the work (bug, task, iteration, workstream)
2. Identify what context is needed (which documents, which code, which decisions)
3. Decide which other roles should contribute a section
4. Build the initial pack structure

### Step 2: Decide Which Roles Contribute

Not every role comments on every pack. The Librarian and Architect assess:

- Does this have **security implications**? → AppSec contributes a section
- Does this involve **personal data**? → DPO contributes a section
- Does this involve **deployment or infrastructure**? → DevOps contributes a section
- Does this involve **user-facing changes**? → Designer, Advocate contribute
- Is there **historical context** that matters? → Historian contributes
- Are there **compliance requirements**? → GRC contributes

For a simple bug fix, maybe only the Developer contributes (where the bug is, what the fix should be). For a workstream-level pack like the CloudFront log pipeline, you might have sections from: Developer, DevOps, Architect, AppSec, DPO, Historian, and the Sherpa.

### Step 3: Build the Documents

The pack contains:

**Core document (always present):**
- What is the work? (Clear problem statement)
- What does success look like? (Acceptance criteria)
- What is the scope? (What's in, what's out)
- Architecture context (how this fits into the larger system)
- Known constraints and decisions (why things are the way they are)

**Role-specific addenda (when relevant):**
- Each contributing role gets its own section or separate document
- Focused on their perspective — security concerns, deployment notes, historical context, etc.
- Could be a couple of paragraphs (inline in the main doc) or a full separate document

**Code context (critical — see below):**
- Relevant source code, quoted directly with file paths and line numbers
- NOT generic descriptions of how things work — actual code from the actual codebase

**Issues FS (always present):**
- Every briefing pack has its own `.issues/` folder containing issue files
- Uses the simplified format
- Tracks the specific tasks, bugs, and to-dos for this pack
- Can be consolidated into the main project issues later

### The Iron Rule: One Folder Per Pack

Every single pack — no matter how small, even a one-line bug fix — lives in its own **dedicated folder**. No exceptions.

The minimum viable pack is two folders and two files:

```
token-fetch-bug/                  # ← The pack folder (unique, dedicated)
├── BRIEF.md                      # ← The pack's briefing document
└── .issues/                      # ← Issues folder
    └── tasks.issues              # ← At least one issues file
```

A larger pack adds more files, but the structure is the same:

```
cloudfront-log-pipeline/
├── BRIEF.md
├── architecture.md
├── code-context.md
├── .issues/
│   └── tasks.issues
└── addenda/
    ├── devops.md
    ├── appsec.md
    └── dpo.md
```

**Why this matters:**

1. **Git-friendly**: every pack is a clean directory that can be committed, branched, and tracked independently. The pack's history is its folder's git history.
2. **Shareable**: to hand a pack to an agent (or a human), zip the folder. That's it. One zip = one complete unit of work with all context, all issues, all addenda. Drop it into any context window and the recipient has everything they need.
3. **Discoverable**: listing the packs directory shows every piece of work that's been packaged. Each folder name tells you what it is.
4. **Self-contained**: no pack references files outside its folder for its core content (reference summaries are copied in, not linked). The pack is a complete, portable unit.

The `.issues/` folder is always a subfolder of the pack, never a file at the pack root. This keeps the structure consistent and means `tasks.issues` (or `bugs.issues`, `research.issues`, etc.) are always in the same predictable location.

### Step 4: Compress Knowledge

The pack should reference, not duplicate. The principle: **include what's needed, point to where more lives.**

```
WRONG: Copy the entire caching service documentation into the pack
RIGHT: "The caching service uses latest + temporal storage. For this task, 
        the relevant pattern is [specific pattern, quoted]. For full caching 
        service documentation, see /docs/caching-service.md"
```

Think of it as a zoomed-in view:
- **High level**: what is this system, how does it fit together (1 paragraph)
- **Medium detail**: how does the specific part we're working on function (1-2 pages)
- **Deep detail**: the actual code, actual config, actual data structures (quoted, with line references)

The agent reads the pack from top to bottom, going from context to specifics. If they need more detail on something, the pack tells them where to find it.

---

## Code Context: The Method Streams Principle

This is critical and non-negotiable: **briefing packs must reference actual code, not descriptions of code.**

### What "Method Streams" Means

When documenting a code path for a briefing pack, extract the relevant methods in call order — every method called from the entry point, in sequence, as one readable document. This was inspired by a technique used for code reviews and security analysis: take a method, extract its AST and call tree, and create a single file containing every method called from the first one.

### How to Apply It

For a briefing pack about, say, the CloudFront log ingestion:

```
WRONG:
"The log ingestion process reads files from S3, transforms them, 
and saves them to the cache service."

RIGHT:
"The log ingestion starts at `handle_cloudfront_log()` in 
`/src/services/log_processor.py` (line 42-68):

```python
def handle_cloudfront_log(event):
    bucket = event['Records'][0]['s3']['bucket']['name']
    key = event['Records'][0]['s3']['object']['key']
    raw_data = read_s3_object(bucket, key)          # line 47
    typed_records = parse_cloudfront_log(raw_data)    # line 48  
    cache_id = generate_cache_id('cloudfront', key)   # line 49
    save_to_cache(cache_id, typed_records, 'raw')     # line 50
```

This calls `parse_cloudfront_log()` at `/src/parsers/cloudfront.py` (line 15-38):

```python
def parse_cloudfront_log(raw_data):
    lines = raw_data.decode('utf-8').split('\n')
    records = []
    for line in lines[2:]:  # Skip header lines
        fields = line.split('\t')
        ...
```
"
```

The pack quotes actual code with actual file paths and actual line numbers. This means:
- The agent knows exactly what the code does (not an approximation)
- The agent can modify the right files and the right lines
- The pack can be verified against the source code (is this still accurate?)
- If the code changes, the pack can be updated by re-extracting

### When the Agent Has No Repo Access

This is where method streams are most valuable. The agent can't look at the code, so the code must come to the agent. The briefing pack becomes the agent's only view of the codebase. It must be accurate, specific, and complete for the task at hand.

### When the Agent Has Full Repo Access

Even with full access, method streams are valuable as a reading guide: "start here, follow this path, these are the files that matter for this task." The agent doesn't have to discover the relevant code — the pack points them directly to it.

---

## Example: CloudFront Log Pipeline Briefing Pack

This is the first full briefing pack we should create. It packages everything needed to implement the CloudFront log pipeline described in today's Villager brief.

### Pack Structure

```
cloudfront-log-pipeline/
├── BRIEF.md                    # Core document — what, why, scope, success criteria
├── architecture.md             # How the pipeline fits into the SA architecture  
├── code-context.md             # Method streams — relevant code with file/line refs
├── .issues/
│   └── tasks.issues            # Issues FS for this workstream
│
├── addenda/
│   ├── devops.md               # DevOps: S3 bucket config, Firehose setup, Lambda triggers
│   ├── developer.md            # Developer: current bugs, code structure, where to start
│   ├── appsec.md               # AppSec: confirm IP addresses NOT stored, security review
│   ├── dpo.md                  # DPO: privacy by design, GDPR implications of log storage
│   ├── architect.md            # Architect: cache service patterns, type-safe structures
│   └── historian.md            # Historian: how we got here, why this design, prior decisions
│
└── reference/
    ├── caching-service-summary.md   # Compressed summary of relevant cache patterns
    └── lets-pipeline-summary.md     # Compressed summary of LETS principle
```

### What the Core BRIEF.md Contains

1. **Problem**: CloudFront logs arrive via Firehose → S3 as tab-delimited text. We need to transform, consolidate, aggregate, and visualise them.
2. **Success criteria**: I can see yesterday's traffic in the admin dashboard. The pipeline processes automatically. Old logs are archived.
3. **Scope**: CloudFront logs only (this is the first pipeline — other log sources follow the same pattern later).
4. **Architecture**: digital twin collectors → raw ingestion → typed JSON → consolidation → temporal aggregation → visualisation. Specific to CloudFront. Diagram included.
5. **Key decisions**: why the cache service is used (latest + temporal), why type-safe structures matter, why finality tracking exists, why the archive/cleanup step matters.
6. **Known bugs**: current issues with the caching service implementation in the logging element (specific bugs described, with code references).
7. **Code context**: method streams for the relevant code paths.
8. **Questions for the human**: anything the Architect or Librarian couldn't answer — flagged for my input before the agent starts work.

### What the AppSec/DPO Addendum Contains

A specific, critical question: **are IP addresses actually not being stored in the CloudFront logs?** This needs to be confirmed empirically, not assumed. The DPO requires evidence. The AppSec review should:
1. Examine the Firehose configuration — what fields are being captured?
2. Examine a sample of actual log data — are there IP addresses in it?
3. Confirm that the transformation step (raw → typed JSON) does NOT include IP-like fields
4. Document the finding as a compliance artefact

This is a perfect example of why role-specific addenda exist: the developer might not think to check this, but the DPO and AppSec absolutely will.

---

## Small Packs: Bugs and Tasks

Not every pack is a workstream. Most are small: a bug fix, a single task, a UI tweak.

### Bug-Level Pack

```
Bug: Token fetch is slow on admin Lambda

DESCRIPTION (Sherpa/QA):
Fetching existing tokens takes 3+ seconds. Should be <500ms.
Reproduced: go to admin panel → tokens tab → observe load time.

DEVELOPER NOTES:
Suspect unnecessary S3 reads. The cache service should have 
tokens in the latest/ folder, but the code might be iterating 
temporal/ entries. See `/src/api/tokens.py` line 23:

```python
def get_all_tokens():
    # BUG: This reads ALL temporal entries, not just latest
    entries = cache_service.list_entries('tokens')  
    # Should be: cache_service.get_latest('tokens')
```

ARCHITECT NOTES:
Confirmed — the cache service usage mapping shows tokens are 
stored per-token as separate cache entries. The list_entries() 
call reads every version of every token. Use get_latest() to 
read only current state.

APPSEC: No security implications for this fix.

.issues/bug.issues:
Bug-1 | in-progress | Token fetch slow — reads temporal instead of latest
  -> Architect cache mapping complete, fix identified
  -> Dev to implement get_latest() call
  -> QA to verify <500ms after fix
```

Even this tiny bug pack lives in its own folder:

```
token-fetch-bug/
├── BRIEF.md              # The bug description above
└── .issues/
    └── bug.issues        # The single issue tracking this fix
```

That's a complete bug-level briefing pack. Concise, specific, actionable. An agent with this context can fix the bug without needing anything else.

---

## Parallel Agent Execution

A key reason for briefing packs: **multiple agents can work in parallel, each with their own pack, each isolated.**

```
AGENT A                    AGENT B                    AGENT C
─────────                  ─────────                  ─────────
Pack: Token fetch bug      Pack: Upload progress bar  Pack: Analytics pulse fix
Has: bug description,      Has: UI code context,      Has: admin Lambda code,
     cache service code,        event system docs,          pulse feature spec,
     fix direction               progress stages            current broken code
Issues: bug-1.issues       Issues: feature-1.issues   Issues: bug-2.issues
```

Each agent:
- Has its own pack folder with all relevant context
- Has its own `.issues/` folder tracking progress
- Can work without knowing about the other agents' work
- Produces output that can be reviewed and merged independently

The briefing pack folder is what makes this parallelism possible. Without it, every agent would need the full repo context and would potentially interfere with each other. With it, you zip a folder, drop it into a context, and the agent has everything it needs.

### Cost-Effectiveness Principle

Every agent has a cost (API tokens, time, attention). The briefing pack should answer: **what is the most cost-effective work this agent should be doing right now?**

If a task needs a massive pack with 50k tokens of context, that's expensive. If the same task can be broken into three smaller packs of 10k tokens each, that might be more cost-effective (smaller context = faster inference, fewer errors, cheaper per run). The Conductor and Architect should consider this when sizing packs.

---

## Process: Who Creates the Packs?

### For Workstream-Level Packs

1. **Human** identifies the workstream (via daily brief or direct instruction)
2. **Librarian** + **Architect** lead the pack creation:
   - Librarian identifies what documentation exists, what needs to be written
   - Architect identifies the technical context, code paths, architecture decisions
   - Together they decide which roles should contribute addenda
3. **Contributing roles** write their sections
4. **Developer** extracts method streams (actual code with file/line references)
5. **Librarian** assembles the final pack, cross-links everything, verifies completeness
6. **Pack is reviewed** — does the agent have everything it needs? Any questions for the human?
7. **Human reviews** the pack, answers questions, approves
8. **Agent picks up the pack** and starts work

### For Bug/Task-Level Packs

Simpler and faster:
1. **Bug/task** is identified (by QA, Sherpa, human, or another agent)
2. **Developer** or **Architect** writes a focused pack (description + code context + fix direction)
3. **Relevant roles** add a line or two if needed (AppSec: "no security impact", DPO: "no privacy concern")
4. **Agent picks it up** and executes

### Librarian's Maintenance Role

As packs are created and used:
- The Librarian catalogues all packs (what exists, what's current, what's obsolete)
- Packs that reference code should be verified periodically (has the code changed since the pack was written?)
- Successful packs become templates for similar future work
- The Librarian maintains a "pack library" — reusable context summaries (caching service overview, LETS pipeline overview, etc.) that can be dropped into multiple packs

---

## Reference Summaries: Reusable Context Blocks

Some context appears in many packs. Rather than rewriting it each time, maintain **reference summaries** — compressed, accurate descriptions of key systems:

| Reference | What It Covers | Approximate Size |
|---|---|---|
| `caching-service-summary.md` | Latest + temporal model, cache IDs, folder structure, API | ~500 words |
| `lets-pipeline-summary.md` | Log → Extract → Transform → Store principle | ~300 words |
| `issues-fs-summary.md` | Simplified .issues format, usage patterns | ~200 words |
| `ifd-versioning-summary.md` | Minor versions, major versions, Explorer→Villager handover | ~300 words |
| `type-safe-structures-summary.md` | How we use type-safe JSON in the caching service | ~400 words |
| `admin-ui-patterns-summary.md` | Path-based routing, web components, SPA structure | ~300 words |

These are maintained by the Librarian and updated when the underlying systems change. A briefing pack includes the relevant summaries rather than explaining everything from scratch.

---

## Task Summary

| Priority | Task | Owner(s) |
|---|---|---|
| **P1** | Create the first full briefing pack: CloudFront log pipeline | Librarian, Architect |
| **P1** | Extract method streams for CloudFront-relevant code paths | Developer |
| **P1** | Write AppSec/DPO addendum: confirm IP addresses not stored | AppSec, DPO |
| **P1** | Define the briefing pack template (reusable structure) | Librarian, Architect |
| **P2** | Create reference summaries for key systems (caching service, LETS, IFD, etc.) | Librarian |
| **P2** | Build the pack library catalogue (what packs exist, status, currency) | Librarian |
| **P2** | Define the parallel agent execution model (how packs are assigned, tracked, merged) | Conductor, Architect |
| **P3** | Create small packs for current bug backlog (token fetch, analytics pulse, etc.) | Developer, relevant roles |

---

## For the Conductor

This is a process improvement that affects everything we do. The immediate action is to build the CloudFront log pipeline pack as the first real example — it's the right size (workstream-level, multiple roles involved), it's the right urgency (we need to start processing those logs), and it'll teach us what works and what doesn't in the pack creation process.

Once the first pack is done, review it: was it enough? Did the agent need to ask questions that should have been in the pack? Did any section turn out to be unnecessary? Use those learnings to refine the template before creating more packs.

The long-term goal: any piece of work in this project can be packaged into a briefing pack and handed to an agent (or a human) with confidence that they have everything they need to execute. That's the productivity multiplier.
