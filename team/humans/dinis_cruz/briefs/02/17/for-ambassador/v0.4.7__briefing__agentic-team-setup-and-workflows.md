# Running GenAI Agentic Teams in Production: A Working Example

**date** 17 February 2026  
**author** Dinis Cruz  
**status** Active — this is a live project, not a retrospective  

---

## What This Document Is

This is a detailed account of how a single person is using GenAI (specifically Claude via Claude Code Web) to run two AI agent teams that are building, shipping, and operating a real product. It covers the setup, the workflow, the team structure, the communication model, and what we've learned.

If you're interested in how agentic AI teams actually work in practice — not in theory, not as a demo — this is a real-world example from February 2026.

---

## The Project: SG/Send

**SG/Send** (send.sgraph.ai) is a zero-knowledge encrypted file and message transfer service. The core proposition: a sender encrypts a file in their browser, sends the link via one channel and the decryption key via another. The recipient decrypts in their browser. The server never sees the plaintext — not the file, not the key, not the content.

The product is live, open source, and being used by real beta users. It's built on AWS serverless infrastructure (Lambda, S3, CloudFront, API Gateway) with a Python backend and vanilla JavaScript frontend (no frameworks — pure JS with Web Components).

### Why SG/Send Is a Good Learning Vehicle

SG/Send is deliberately chosen as the project for learning about agentic teams because it has the right properties:

- **Real product with real users** — not a toy project. Real security concerns, real deployments, real user feedback.
- **Multi-disciplinary** — requires security expertise (encryption, zero-knowledge architecture), infrastructure (AWS, serverless), frontend development, design, documentation, compliance (GDPR, DPO), marketing, user research, and translation. No single agent or role can cover everything.
- **Open source** — the code is publicly reviewable. When a security researcher reviewed the code and sent a vulnerability report via SG/Send itself, that was the system working as designed.
- **Small enough to understand** — the entire codebase fits in an agent's context window. The architecture is simple (Lambda + S3 + CloudFront). But the product surface is large enough to require real coordination.
- **Fast iteration** — serverless deployment means changes go live in minutes. The feedback loop is tight enough that you can see the impact of agentic decisions quickly.

---

## The Human-Agent Model

### One Human, Many Agents

The project is led by one human (the project lead). The human:

- Sets direction via voice memos (recorded in Otter.ai, transcribed, shared with Claude)
- Makes all final decisions (architecture, priorities, risk acceptance, deployments)
- Reviews all outputs (briefs, code, documentation)
- Provides context that agents can't access (business relationships, market intuition, user conversations)
- Manages the external world (beta users, investors, community)

The agents (all instantiated as Claude via Claude Code Web):

- Analyse, research, plan, write, and produce structured outputs
- Maintain institutional knowledge across sessions via the file system
- Follow defined roles with specific responsibilities and boundaries
- Communicate with the human and with each other through documents, not conversations

### Claude Code Web as the Platform

The entire system runs through **Claude Code Web** — the web-based interface to Claude that includes file creation, editing, and a Linux compute environment. This is the key enabler:

- Claude can create, read, and edit files
- Files persist between messages within a conversation
- The human can download files and commit them to the git repository
- The repository IS the communication channel — agents read from it and write to it

There is no custom tooling, no orchestration framework, no agent-to-agent API. It's just: human talks to Claude, Claude reads/writes files, files go into git, and the next session picks up where the last one left off via the accumulated file system.

---

## The Two Teams

The agents are organised into two teams, each with a distinct mission and a shared set of roles:

### Explorer Team

**Mission**: experiment, research, build new capabilities, explore possibilities.

The Explorer team is where new ideas are born, prototyped, and tested. Nothing the Explorer team produces goes directly to users. Their output is code, architecture decisions, research findings, and briefs.

**Roles** (17 total):

| Role | Responsibility |
|---|---|
| **Conductor** | Team coordinator — routes work, manages priorities, ensures roles collaborate |
| **Architect** | System design, technical decisions, architecture documentation |
| **Developer** | Writes code, implements features, fixes bugs |
| **DevOps** | Infrastructure, deployment, CI/CD, AWS configuration |
| **AppSec** | Application security — reviews code, identifies vulnerabilities, designs security controls |
| **DPO** | Data Protection Officer — privacy by design, GDPR compliance, data handling policies |
| **GRC** | Governance, Risk, Compliance — risk assessment, incident classification, policy frameworks |
| **QA** | Quality assurance — testing strategy, test writing, verification |
| **Designer** | UI/UX design, visual identity, design guidelines |
| **Journalist** | Writes narrative articles about the project — user-facing, engaging, publishable |
| **Historian** | Records technical history — decisions, changes, timelines, evidence |
| **Librarian** | Catalogues and cross-links all documents — maintains the knowledge base |
| **Ambassador** | External communications — marketing copy, investor documents, partnership outreach |
| **Advocate** | User documentation, onboarding guides, user-facing help content |
| **Sherpa** | User research — tracks user journeys, analyses feedback, maps pain points |
| **Cartographer** | Maps the landscape — Wardley maps, competitive analysis, strategic positioning |

### Villager Team

**Mission**: productise, deploy, operate, and maintain. Ship to production. Ensure quality, security, and reliability for real users.

The Villager team takes what the Explorer team builds and makes it production-ready. They own the live product.

**Roles** (18 total — all Explorer roles plus one exclusive):

The Villager team has all the same roles as the Explorer team, plus:

| Role | Responsibility |
|---|---|
| **Translator** | Language translation (en → pt-BR, pt-PT) and audience translation (developer → business → user). The first role that exists exclusively in the Villager team. |

The Translator is Villager-exclusive because translation is a production concern — internal Explorer communication doesn't need translation, but everything that reaches users does.

### Why Two Teams?

The separation enforces a critical discipline: **exploration and production are different activities with different priorities.**

- The Explorer team optimises for speed, experimentation, and learning. It's okay to break things.
- The Villager team optimises for reliability, quality, and user experience. Breaking things is not okay.

A feature starts in the Explorer team (researched, prototyped, tested). When it's ready, the Villager team picks it up, applies production discipline (testing, risk assessment, deployment pipeline), and ships it.

This mirrors how good engineering organisations work — except the "teams" are AI agent roles coordinated by a single human.

---

## The Communication Model: Files, Not Conversations

This is the most important design decision in the entire setup: **agents communicate through the file system (git), not through conversations.**

### Why Files?

- **Persistence**: conversations are ephemeral. Files live in git. Every document ever produced is versioned, searchable, and recoverable.
- **Context**: a new agent session can read the entire project history from the file system. It doesn't need the previous conversation — it needs the accumulated knowledge.
- **Coordination**: two agent sessions can't talk to each other, but they can both read and write to the same file system. Documents are the coordination mechanism.
- **Auditability**: every decision, every brief, every analysis is a committed file. You can git blame any document and trace it back to the session that created it.
- **Scalability**: conversations have context window limits. The file system doesn't. Complex analyses can span multiple documents that reference each other.

### The Daily Workflow

```
1. HUMAN records voice memo (Otter.ai)
   ↓
2. HUMAN shares transcript with Claude (Claude Code Web)
   ↓
3. CLAUDE (as Architect + Librarian) reads the memo
   ↓
4. CLAUDE produces a "daily brief" document
   - Captures all ideas, decisions, and tasks from the memo
   - Assigns to relevant roles
   - Cross-links to existing documents
   ↓
5. HUMAN reviews the brief
   ↓
6. HUMAN says "go" — CLAUDE produces role-specific outputs
   - Briefs for specific roles
   - Role definitions
   - Research documents
   - Architecture decisions
   - Code context packages
   ↓
7. HUMAN downloads all files → commits to git
   ↓
8. Next session: CLAUDE reads the accumulated file system
   to understand project state
```

### The File System IS the Knowledge Base

The project's git repository has a specific folder structure that serves as both the organisational chart and the institutional memory:

```
team/
├── humans/
│   └── dinis_cruz/
│       ├── briefs/           # Voice memo transcripts and daily briefs
│       ├── claude-code-web/  # Session transcripts and notes
│       ├── code-example/     # Code snippets and examples
│       ├── debriefs/         # Debrief documents (summaries of what happened)
│       └── dev_sessions/     # Development session records
│
├── roles/                    # Shared role definitions (Explorer team)
│   ├── advocate/
│   ├── ambassador/
│   ├── appsec/
│   ├── architect/
│   ├── cartographer/
│   ├── conductor/
│   ├── designer/
│   ├── dev/
│   ├── devops/
│   ├── dpo/
│   ├── grc/
│   ├── historian/
│   ├── journalist/
│   ├── librarian/
│   ├── qa/
│   └── sherpa/
│
└── villager/                 # Villager team (production)
    └── roles/
        ├── (all shared roles)
        └── translator/       # Villager-exclusive role
```

Each role folder contains:
- **ROLE.md** — the role definition (responsibilities, boundaries, how it collaborates with other roles)
- **.claude.md** — Claude-specific instructions for when an agent is instantiated as that role
- **.issues/** — task tracking for that role
- **Knowledge documents** — accumulated expertise specific to that role

### The Brief → Debrief Cycle

The human communicates with the agents primarily through **briefs** (input) and receives **debriefs** (output):

**Briefs** (human → agents):
- Voice memos transcribed and processed into structured daily briefs
- Each brief identifies which roles are impacted and what tasks are assigned
- Briefs are versioned (v0.1.2, v0.2.16, v0.3.2, v0.4.7, etc.)

**Debriefs** (agents → human):
- Each impacted role updates its .issues, knowledge documents, and ROLE.md
- The Conductor produces a debrief document summarising how the brief was consumed
- The human reviews the debrief ("debrief on brief") and provides clarifications

This cycle — brief → process → debrief → clarify → next brief — is the heartbeat of the project.

### Briefing Packs for Agent Tasks

When a task is complex enough to need its own context package, it gets a **briefing pack** — a dedicated folder containing everything an agent needs:

```
task-folder/
├── BRIEF.md            # What to do, why, scope, success criteria
├── .issues/
│   └── tasks.issues    # Task tracking
├── architecture.md     # Technical context
├── code-context.md     # Actual code with file/line references
└── addenda/            # Role-specific context
    ├── appsec.md
    ├── dpo.md
    └── devops.md
```

The key principle: **method streams**. Code context must reference actual code with file paths and line numbers, not descriptions. When an agent doesn't have repo access, the code comes to the agent via the pack.

---

## What We've Produced

### By the Numbers (as of 17 February 2026)

Over approximately 5 days of active work (13–17 February 2026):

| Metric | Count |
|---|---|
| **Documents produced** | 75+ markdown files |
| **Total documentation volume** | ~950KB of structured content |
| **Role definitions created** | 18+ (full specifications with responsibilities, boundaries, workflows) |
| **Daily briefs produced** | 8+ (covering both teams across multiple days) |
| **Architecture briefs** | 10+ (file transfer engine, interactive encrypted workflows, briefing packs, etc.) |
| **Commercial documents** | 6 (3 investor documents, 1 accountant document, coming soon) |
| **Incident response** | 1 full incident (29 security findings processed, classified, assigned) |
| **Version iterations** | v0.1.2 → v0.4.7 (continuous versioning across all documents) |

### What the Documents Cover

**Role definitions**: full specifications for all 18 roles — Conductor, Architect, Developer, DevOps, AppSec, DPO, GRC, QA, Designer, Journalist, Historian, Librarian, Ambassador, Advocate, Sherpa, Cartographer, Translator. Each with responsibilities, collaboration patterns, metrics, and immediate tasks.

**Architecture and research**: file transfer engine architecture (chunked uploads, resume capability, integrity verification), JS project bootstrap (pure JS, no frameworks, Web Components), interactive encrypted data workflows (JSON in → JSON out via browser forms), briefing pack specifications.

**Product operations**: production deployment discipline (7-step workflow), CloudFront log pipeline (LETS — raw → typed → consolidated → aggregated), server analytics architecture, admin UI design, digital twin infrastructure collection.

**Commercial**: investor pitch documents (3 — what works today, what's coming soon, future vision), accountant pitch document (5 use cases for SME accounting practices).

**Security**: full processing of an external security review (29 findings reclassified on our P0–P10 scale, pack-of-packs incident structure, remediation plan), DPO/AppSec/GRC evidence collection (11 documented examples of security roles influencing decisions).

**User feedback**: two beta users' feedback captured, classified, and turned into actionable tasks with stakeholder management plans.

### The Production Pipeline

The project has a live product at send.sgraph.ai with:
- Zero-knowledge encryption (AES-256-GCM, Web Crypto API)
- Serverless architecture (AWS Lambda, S3, CloudFront, API Gateway)
- Multiple IFD versions (Immutable File Deployments — v0.1.0 through v0.1.6)
- Portuguese language support (pt-PT live, pt-BR in progress)
- Access token system for beta distribution
- Admin panel for monitoring
- Docs site (docs.send.sgraph.ai) maintained by the Journalist agent

---

## How a Typical Session Works

Here's what an actual working session looks like:

### Morning (Human)
1. Human records a voice memo while walking, commuting, or thinking. Stream of consciousness — ideas, decisions, reactions to yesterday's work, new directions. Usually 5–15 minutes.
2. Otter.ai transcribes it (imperfectly — but good enough for Claude to parse).
3. Human opens Claude Code Web and pastes the transcript.

### Processing (Claude)
4. Claude reads the transcript and any uploaded files (screenshots, user feedback, code).
5. Claude identifies the key themes, decisions, and tasks.
6. Claude produces a daily brief — a structured document capturing everything, with tasks assigned to roles and cross-references to existing documents.
7. Human reviews: "Does this capture it all?" — often with corrections, additions, clarifications.
8. Claude iterates based on feedback.

### Production (Claude)
9. Once aligned, Claude produces the role-specific outputs — role definitions, architecture briefs, research documents, commercial documents, incident response plans.
10. Each document follows the project's conventions: versioned, with metadata (version, date, from, to), structured sections, and explicit task assignments.
11. Human downloads all files.

### Integration (Human)
12. Human commits files to the git repository.
13. The repository is the source of truth. Next session, Claude reads it.

### Key Observation: The Human Is the Bottleneck (and That's Correct)

The human reviews every output. The human makes every decision. The human commits every file. This is by design — the agents produce, the human decides. The system is designed so that the human's judgment is the quality gate, not a bureaucratic checkpoint.

What the agents eliminate is not the human's judgment — it's the busywork between the judgment calls. The human says "we need a Translator role" and gets a 28KB role definition with pipeline specifications, glossary templates, and task lists in minutes, not days.

---

## Key Design Decisions and Principles

### Immutable File Deployments (IFD)

Every version of every document is preserved. Previous versions are never modified — if we need to update yesterday's brief, we create a new version. Git history provides the full audit trail. This mirrors the encryption model: once something is committed, it's permanent.

### The File System IS the Database

We don't use a project management tool, a wiki, or a database for project state. The file system — organised into folders with known conventions — is the database. Issues are tracked in `.issues` files. Knowledge is in markdown documents. Role definitions are in `ROLE.md` files. The Librarian role exists specifically to catalogue and cross-link this knowledge base.

### Roles, Not Prompts

Each agent role has a full definition — not just "you are a security expert" but a comprehensive specification of responsibilities, boundaries, collaboration patterns, metrics, and immediate tasks. The role definition is typically 10–30KB of structured text. This is dramatically more context than a system prompt, and it's maintained in version control alongside the code.

### Voice Memos as Input

The human's primary input method is voice memos, not typed instructions. This is deliberate:
- Voice is faster than typing for complex, multi-threaded ideas
- Stream-of-consciousness captures connections that structured writing might miss
- The human can communicate while doing other things (walking, commuting)
- Claude is excellent at extracting structure from messy transcripts

The quality of the voice memo doesn't matter much — Claude can work with imperfect transcripts, half-formed thoughts, and tangential asides. What matters is that the human's ideas get captured.

### Cost-Effectiveness Principle

Not every agent needs to be active at every moment. The daily brief identifies which roles are impacted by today's input. Only those roles produce output. The question is always: "What's the most valuable work right now?" — not "How do we keep every agent busy?"

---

## What We've Learned

### What Works

- **The role model scales remarkably well.** 18 roles sounds like a lot, but each role has a clear boundary. When a security review arrives, you know immediately: AppSec leads, DPO reviews privacy aspects, GRC classifies risks, Developer plans fixes. The routing is natural.

- **Voice memos → structured briefs is a killer workflow.** A 10-minute stream-of-consciousness memo becomes a 30KB structured document with role assignments, task lists, and cross-references. The human's time is spent on high-value thinking, not on structuring that thinking.

- **The file system as communication channel eliminates coordination overhead.** No meetings, no Slack channels, no status updates. The documents ARE the status. Read the latest brief, you know what's happening.

- **Two teams with different mandates prevents the "ship it fast" / "make it good" tension.** The Explorer team can experiment freely. The Villager team enforces production discipline. The boundary is clean.

- **External validation happened naturally.** A security researcher reviewed the open-source code and sent findings via SG/Send. A beta tester confirmed the product works and requested features. Both interactions produced structured documentation that fed back into the agentic workflow. The system processes real-world input, not just the human's ideas.

### What's Hard

- **Context window limits require session management.** Long sessions accumulate context that exceeds the window. The compaction/summary mechanism helps but loses detail. The solution is the file system — important information goes into files, not conversation history.

- **The human is still the integration layer.** Files produced by Claude need to be manually committed to git. There's no automated pipeline from Claude output to repository. This is a deliberate choice (the human reviews everything) but it adds friction.

- **Agent-to-agent communication is indirect.** Two agent sessions can't talk to each other in real time. They communicate through committed files. This works but introduces latency — if the Architect needs AppSec input, the human has to facilitate.

- **Maintaining consistency across 75+ documents is non-trivial.** Version references, cross-links, terminology — keeping everything aligned requires the Librarian role, and even then, the human needs to verify.

---

## The Bigger Picture: GenAI Agentic Teams in February 2026

This project is a working example of where agentic AI stands in early 2026:

**What's possible today**: a single human can direct a team of 18 specialised AI agents to produce professional-grade documentation, architecture, security analysis, commercial materials, and incident response — at a pace that would require a team of 10+ humans working full-time. The quality is high enough to share with external stakeholders (investors, beta users, security researchers) without embarrassment.

**What requires human judgment**: every decision, every priority, every risk acceptance, every piece of external communication. The agents don't decide what to build or who to talk to. They don't negotiate with users or accept security risks. They produce options, analysis, and structured outputs — the human chooses.

**What's next**: tighter integration between the AI and the development environment (Claude Code already enables this), automated pipelines from brief to committed code, and eventually, agents that can operate semi-autonomously within defined boundaries (e.g., "fix all P6–P8 findings and submit a PR for review").

The SG/Send project is both the vehicle and the proof point. The product being built (secure encrypted communication) is useful in its own right. But the way it's being built — one human, two AI teams, 18 roles, 75+ documents, a live product with real users, all in 5 days — that's the story about the future of work.

---

## Try It Yourself

- **The product**: send.sgraph.ai
- **The code**: open source on GitHub (owasp-sbot organisation)
- **The docs**: docs.send.sgraph.ai
- **The approach**: Claude Code Web + voice memos + git + the discipline to write everything down

The setup is reproducible. The tools are available. The only requirement is a human who knows what they want to build and is willing to direct the work.
