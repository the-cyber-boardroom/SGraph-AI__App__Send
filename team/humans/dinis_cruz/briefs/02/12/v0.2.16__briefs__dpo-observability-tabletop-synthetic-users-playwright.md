# DPO, Observability, Tabletop Exercise, Synthetic Users and Playwright Automation

**version** v0.2.16  
**date** 12 Feb 2026  

## The Big Picture

We're about to start sharing SGraph Send with real users. Before we do, we need the full scaffolding in place — not just the app, but the visibility, the privacy posture, the monitoring, the synthetic users, and the incident response readiness. The philosophy: **MVP across the entire supply chain**. Don't perfect one thing and go live blind everywhere else. Have all the plumbing working — even if basic — so that as users arrive and the system scales, we have total visibility and total control.

This brief covers six connected workstreams that all need to move in parallel.

---

## 1. The DPO Role — Data Protection Officer

We need a new role: the Data Protection Officer, as defined under GDPR. This isn't a compliance checkbox — the DPO is the role that, by law, represents privacy and the customer's data rights. For our agentic team, the DPO sits alongside the advocate (who owns the customer entity) and the GRC role (who owns the risk register), but with a specific legal mandate around data protection.

### Why Now

There are claims we want to make about SGraph Send — particularly around IP addresses and what we log — that we cannot yet make honestly. The DPO is the role that forces that honesty and provides the wording for what we can and can't say to users.

### What the DPO Needs to Do First

- Analyse the current state of IP address handling. Similar documentation elsewhere talks about masking IP addresses, but **we cannot currently make that claim**. Until we've done a full review of the AWS deployment — CloudFront, Lambda, S3, every service in the chain — we don't know everywhere that IP addresses might be captured in logs. The DPO needs to map this.
- Provide the privacy wording for users. Not aspirational — current state. What are we actually capturing? What can we honestly tell users right now?
- Analyse the Google Analytics addition (see below) from a data protection perspective.
- Work with GRC to map and formally accept the risks around what we're capturing.

### The Transparency Principle

We should be transparent about where we are today:

- **What we believe is true**: we're not putting cookies on the user. We're not storing data in local storage. There's no way to identify a user back from what we capture client-side.
- **What we know we capture**: Google Analytics data (once added) and server-side logs on the Lambda functions (which we don't fully control yet).
- **What we're not sure about**: whether IP addresses appear in CloudFront logs, S3 access logs, Lambda execution logs, or other AWS infrastructure logs that we haven't fully reviewed.

The honest position is: "Our objective is not to log IP addresses or retain them any longer than necessary. At the moment, there are multiple places in the AWS infrastructure where those logs could be captured. We are actively reviewing this." The DPO writes this. The journalist publishes it. The advocate validates it serves users.

---

## 2. Google Analytics — Add It, But Eyes Wide Open

We want to add Google Analytics. The code is already wired up and ready to deploy. But this needs to be done properly — not just technically, but from a privacy and risk perspective.

### The Plan

- **Dev / Architect**: map out a small iteration to integrate Google Analytics. This is a straightforward technical task.
- **DPO**: analyse the data protection implications. What does GA actually capture? What gets sent to Google? What jurisdiction does that data end up in?
- **GRC**: analyse and formally accept the risks. Document them in the risk register.
- **Advocate**: review from the user's perspective. Is this transparent enough? Would our personas be comfortable with this?

### What We Can Say to Users

Based on current knowledge:

- No cookies are placed on the user (verify this — GA4 behaviour may differ from classic GA)
- No local storage is used
- No way to identify a specific user back from the data
- GA provides aggregate behavioural analytics — page views, flow patterns, time on page, device/browser info

The DPO provides the exact wording. We publish it. We're transparent.

---

## 3. Maximum Observability — Enable Everything

This is the DevOps workstream, and it's driven by a simple insight: **we don't have a lot of traffic right now, so we can go to town**. Enable everything AWS offers. See what it costs. See what it tells us. Scale back later if needed.

### What to Enable

- CloudFront access logs (detailed)
- S3 object access logs
- Lambda execution logs (full detail)
- AWS X-Ray tracing
- AWS WAF (Web Application Firewall) — both for protection and for visibility into what's being blocked
- Any other AWS observability feature that's available — just turn it all on

### Deliverables from DevOps

This is critical: **DevOps should deliver scripts and API/test checks, not just documentation**.

- Scripts to enable each logging/monitoring feature
- Scripts to verify that logging is active and working (pre-deployment checks)
- Guidance on how to access and query the logs
- Cost estimates for running everything at current (low) traffic levels
- A brief on what each log source captures and where it stores data (the DPO needs this)

### Dashboards

We need dashboards for all this AWS traffic data. There's already example code that can do this effectively — it will be shared as a starting point. The dashboards should give us:

- Real-time traffic visibility
- Error rates and types
- Access patterns (which the sherpa will use for trail observation)
- Security events (which the CISO/AppSec will use)
- Performance metrics (which QA will use for regression detection)

### Why This Matters for the Sherpa

This is the observability infrastructure that makes the sherpa's trail observation possible. Without it, the sherpa has nothing to read. With it, the sherpa can start answering: "what are the trails being followed?" — which is exactly the question we want answered as soon as real users arrive.

---

## 4. Tabletop Exercise — Total AWS Compromise

Now that we have all these roles in play, it's time to test them. We run a tabletop exercise simulating a catastrophic scenario: **total AWS compromise**.

### The Scenario (Pick One or All)

- AWS has been compromised from the inside (insider threat)
- A vulnerability in AWS allows cross-domain access (there have been real cases where this was plausible)
- Our AWS access key has been compromised

All three lead to the same assumption: **the attacker has full access to our AWS infrastructure**.

### What the Exercise Must Cover

- **Detection**: how would we know this happened? What signals would we see? What wouldn't we see?
- **Impact assessment**: what data is exposed? Remember — SGraph Send is zero-knowledge, so file contents should be safe even in total compromise. But what about metadata? Access logs? Transfer records? IP addresses?
- **Consequences**: what's the real-world impact on users? What promises have we broken?
- **Communications**: who do we tell? In what order? What do we say? (The DPO and advocate are critical here — GDPR has specific notification requirements)
- **Runbooks**: what's the step-by-step response? (We almost certainly don't have these yet — the exercise will reveal the gaps)
- **Escalation**: how do I get dragged out of bed at 2am when this happens? How do the agents get triggered? (We don't have good answers for this yet — that's the point)
- **Lessons learned**: what runbooks are missing? What processes don't exist? What questions haven't been asked?

### How to Run It — Issues FS as the Exercise Database

This is a key methodological decision: **run the entire exercise inside an Issues FS database**. Every action, every question, every response, every decision — captured as issues with relationships.

Think of it as a slow game of tennis. One role serves a question. Another role returns an answer. Every volley is an issue in the graph. This gives us:

- Complete replay capability — every action, every sequence, every timing
- Full traceability — which role did what, when, and why
- A reusable template — the exercise itself becomes a playbook for future exercises
- Gap identification — missing issues, missing links, missing roles in the response chain

For now, don't worry about realistic timing. The answers can come at whatever speed they come. In future iterations, we can layer in realistic constraints ("that's a great answer, but there's no way it would have happened in 5 seconds — that's a 2-hour investigation").

### Roles Involved

Every role touches this exercise, but the primary drivers are:

- **CISO**: leads the scenario, defines the threat model
- **DevOps**: what's technically exposed, what can be contained
- **DPO**: what are the GDPR notification obligations
- **GRC**: risk assessment, regulatory consequences
- **AppSec**: vulnerability analysis, containment options
- **Advocate**: user impact, communication tone and content
- **Journalist**: draft the public communications
- **Conductor**: coordinate the response sequence

---

## 5. Synthetic Users — Browser Automation as Bellwether

We need to create a new category of entity in our system: **synthetic users**. These sit alongside the existing structure — same level as humans and roles, a new top-level folder called `users`.

### What Synthetic Users Are

Each synthetic user is a role definition that represents a specific user persona navigating the product via automated browser sessions. They're not test scripts — they're characters with realistic behaviour patterns, including getting lost, clicking wrong things, and taking happy paths.

### Naming Convention

Each user variant captures the persona and their specific access characteristics:

- `user_friendly_en` — a friendly/beta user, English language
- `user_friendly_pt` — same persona, Portuguese language
- `user_friendly_colourblind` — same persona, high-contrast/colour-blind accessibility mode
- (Future: more personas, more languages, more accessibility variants)

Don't name them after real people. Name them by type and variant.

### What They Do

The QA team provides the Playwright scripts. The scripts simulate realistic user journeys:

- Come to the page
- Get lost (realistic confusion paths)
- Try different things
- Click wrong things
- Eventually find the happy path
- Complete a transfer
- Take screenshots at every step

These run both locally and against the live server. They run on a schedule — every hour, every few hours, daily, whatever makes sense. They are the **bellwether**.

### Why This Matters

These synthetic users serve multiple purposes simultaneously:

- **Bellwether / canary**: they verify the website is still working. If a scheduled run fails, something changed. If JavaScript got injected, if a social engineering message appeared, if a page broke — a synthetic user might catch it before a real user does.
- **Performance benchmarking**: every run produces timing data. When we push a new version, we compare against the baseline. Regressions show up immediately.
- **Sherpa trail data**: every synthetic user run produces logs and observability data, just like a real user would. The sherpa can practice reading trails before real users arrive.
- **Tabletop detection**: in a compromise scenario, the synthetic users are running continuously. They'd be among the first to encounter whatever an attacker changed.

### The Advocate's Role

Talk to the advocate first. The advocate owns the persona definitions. The synthetic users need to be based on real personas — the same ones the advocate maintains. The advocate defines who these users are; QA defines what they do; the sherpa reads the trails they leave.

---

## 6. Playwright as a Service — Issues FS In, Issues FS Out

We need a hosted Playwright browser automation capability. The cloud coding environments the agents use don't have desktop/screenshot capabilities, so this needs to be a separate hosted service.

### Architecture

The core idea: **send an Issues FS database (as a zip file) containing the steps to execute. Get back the same Issues FS database, now populated with results, screenshots, and metadata.**

```
INPUT                              OUTPUT
┌────────────────────┐             ┌────────────────────┐
│ issues-fs.zip      │             │ issues-fs.zip      │
│                    │             │                    │
│ step_001: goto URL │             │ step_001: goto URL │
│ step_002: screenshot│  ──────▶  │   result: ✓        │
│ step_003: click X  │  Playwright │   screenshot: .png │
│ step_004: wait 2s  │  Service   │ step_002: ...      │
│ step_005: screenshot│            │   result: ✓        │
│ ...                │             │   screenshot: .png │
└────────────────────┘             │ step_003: ...      │
                                   │   result: ✓        │
                                   │   timing: 340ms    │
                                   └────────────────────┘
```

### The DSL Layer

Most user actions are simple and repetitive: go to page, take screenshot, click element, wait, type text, take screenshot. We should have a strongly-typed DSL that describes these steps — this is what generates the Issues FS input database. The DSL prevents the need for raw Python on the server side and makes the step definitions safe, portable, and replayable.

But the DSL is just the creation layer. The actual communication format is always zip-to-zip (Issues FS to Issues FS). The DSL generates the input; the service populates the output.

### Deployment

- Docker container running Playwright (headful, with screenshot capability)
- Deployable as a Lambda function or ECS task
- Already have access to Docker in CI pipeline and local development
- Once the Docker image is built, the Lambda deployment is straightforward
- Can iterate locally, deploy to Lambda when stable

### Issues FS as Zip Databases

Related feature request: we should start using Issues FS databases as zip files more formally. We can already do this manually (create an Issues FS, zip it, share it, unzip to analyse). But direct support for zip-as-database would be cleaner, and there are performance benefits — the memory FS already supports it, and SQLite-backed storage might perform better than individual files for large datasets.

This becomes the communication format: Playwright service receives a zip, returns a zip. The tabletop exercise lives in a zip. Synthetic user run histories archive as zips. It's zips all the way down.

---

## The Connections Between All Six Workstreams

These aren't independent projects. They feed each other:

```
         DPO                    OBSERVABILITY
          │                          │
          │ privacy analysis         │ enables logging
          │ of what we capture       │ that sherpa reads
          │                          │
          ▼                          ▼
    GOOGLE ANALYTICS ──────▶ DASHBOARDS + TRAILS
                                     │
                                     │ data flows into
                                     ▼
                            SYNTHETIC USERS
                              │         │
                    run on    │         │  produce trails
                    schedule  │         │  that feed
                              ▼         ▼
                         PLAYWRIGHT    SHERPA
                         SERVICE       OBSERVATION
                              │
                              │  also powers
                              ▼
                      TABLETOP EXERCISE
                      (run inside Issues FS)
```

The DPO reviews what we capture. Observability enables the capture. Synthetic users generate the traffic. Playwright runs the automation. The sherpa reads the trails. The tabletop tests the whole system under stress. Everything connects.

---

## For the Agents

- **Architect**: design the Playwright-as-a-service architecture. Issues FS zip in, Issues FS zip out. Define the DSL schema for user action steps. Explore zip/SQLite storage for Issues FS databases.
- **Dev**: implement the Playwright service (Docker + Lambda). Implement the DSL parser. Wire up Google Analytics. There's existing code to be shared for both the dashboards and the Playwright setup — build on it.
- **DevOps**: enable maximum AWS observability. Deliver scripts, not docs. CloudFront logs, S3 access, Lambda logs, X-Ray, WAF — turn it all on. Provide cost estimates. Create the dashboards (starting from example code to be provided).
- **DPO** (new role): audit the AWS deployment for IP address capture points. Analyse Google Analytics data protection implications. Write the current-state privacy wording for users. Provide data protection input to the tabletop exercise (GDPR notification requirements).
- **GRC**: work with the DPO on risk analysis for Google Analytics and IP logging. Formally accept risks. Update the risk register.
- **CISO**: lead the tabletop exercise — total AWS compromise scenario. Define the threat model. Work with AppSec on containment options.
- **AppSec**: support the tabletop exercise with vulnerability analysis. Review the synthetic user scripts for security implications (these will be hitting the live server).
- **Advocate**: define the personas that synthetic users are based on. Review the privacy wording from the DPO — does it serve users? Would our personas accept this?
- **Sherpa**: prepare to read the trails. Once observability is live and synthetic users are running, this is your data. Start defining what trail patterns you want to monitor and what dashboards you need.
- **QA**: write the Playwright scripts for synthetic users. Cover confusion paths, not just happy paths. Wrong clicks, wrong keys, expired links, realistic hesitation. These need to feel like real users, not test robots.
- **Conductor**: sequence these six workstreams. DPO and observability can run in parallel. Synthetic users need observability first. Playwright service needs the architecture defined first. The tabletop exercise needs most of the other pieces in place.
- **Journalist**: publish the DPO's privacy wording on the site. The transparency panel needs to reflect current reality, not aspirations.
- **Librarian**: these six workstreams will generate a lot of artefacts — DPO analyses, DevOps scripts, tabletop exercise databases, synthetic user definitions, Playwright run archives. Catalogue them.
- **Cartographer**: the tabletop exercise will reveal gaps and dependencies. Visualise the incident response workflow as a graph. Map the blast radius of total AWS compromise.
- **Historian**: document the tabletop exercise as a version milestone. This is the first time we test our incident response — record what we learn.
