# v0.2.40 -- Dev: Cache Service Architecture Response

**Version:** v0.2.40
**Date:** 13 February 2026
**Role:** Dev
**Context:** Response to Dinis Cruz's [cache service architecture brief](../../../../humans/dinis_cruz/briefs/02/13/v0.2.40__brief__cache-service-architecture.md), which answers the 6 questions raised in my [v0.2.33 implementation plan](v0.2.33__review__cache-service-implementation-plan.md).
**Depends on:** [Architect Cache Service Schema (v0.2.33)](../../../architect/reviews/26-02-13/v0.2.33__response-to-daily-brief-2__13-feb.md) | [Cache Client How-To](../../../../library/dependencies/cache-service/v0.6.0__v0.10.1__cache_service__client__how_to_use.md) | [Html_Cache__Client reference](../../../../humans/dinis_cruz/code-example/cache-service/Html_Cache__Client.py)
**Supersedes:** [v0.2.33 Implementation Plan](v0.2.33__review__cache-service-implementation-plan.md) (the "build from scratch" plan)

---

## Executive Summary

My v0.2.33 plan designed a `Cache__Service__Send` from scratch -- 25-35 hours across 3 weeks. That was the wrong approach. The brief reveals that the **MGraph-AI Cache Service already exists** as a production-ready package (`mgraph-ai-service-cache` v0.6.0+ / `mgraph-ai-service-cache-client` v0.10.1) with exactly the capabilities I was planning to build: content-addressable storage, temporal strategies, namespace isolation, hierarchical child data, and the same `Storage_FS` / `Type_Safe` / `Cache__Hash__Generator` foundation.

The implementation pivots from "build a cache service" to "build a thin wrapper over a cache service". The reference implementation (`Html_Cache__Client.py`) shows exactly what `Send__Cache__Client` should look like. Estimated effort drops to **8-12 hours total**.

This document resolves the 6 questions from v0.2.33, presents the revised implementation plan, and identifies the 2 remaining open items.

---

## 1. Question Resolution

### Q1 (BLOCKING): "Share the reference Cache__Service code"

**Status:** RESOLVED

**Answer:** The reference is `Html_Cache__Client.py`, located at `team/humans/dinis_cruz/code-example/cache-service/`. I have read this file in full. It is a `Type_Safe` wrapper over `Cache__Service__Client` with `@type_safe` decorators on every public method. Key patterns I will replicate:

- **Class structure:** `Type_Safe` with two fields: `cache_client: Cache__Service__Client` and `hash_generator: Cache__Hash__Generator`
- **Entry operations:** `entry__store`, `entry__retrieve`, `entry__exists`, `entry__exists_by_hash`, `entry__update`, `entry__delete` -- all delegating to `cache_client.store()`, `cache_client.retrieve()`, etc.
- **Cache ID resolution:** `cache_id__from_key` hashes a `cache_key` string via `hash_generator.from_string()`, then resolves to `cache_id` via `cache_client.retrieve().retrieve__hash__cache_hash__cache_id()`
- **Data operations:** `data__store_json`, `data__store_string`, `data__retrieve_json`, `data__retrieve_string`, `data__exists`, `data__list`, `data__update_json`, `data__delete`, `data__delete_all` -- all using `cache_client.data_store()` and `cache_client.data().retrieve()`
- **Namespace operations:** `namespace__all_files` via `cache_client.admin_storage().files__all__path()`
- **Health check:** `health_check` via `cache_client.info().health()`

**Impact:** This eliminates the need to design the wrapper interface from scratch. The API surface is clear and proven.

### Q2 (BLOCKING): "Walk me through the osbot-utils cache module"

**Status:** RESOLVED

**Answer:** The cache module lives in `mgraph-ai-service-cache-client` (v0.10.1 on PyPI), not in `osbot-utils` directly. Key classes:

| Class | Package | Purpose |
|-------|---------|---------|
| `Cache__Service__Client` | `mgraph_ai_service_cache_client` | Full client with store/retrieve/data/admin sub-clients |
| `Cache__Hash__Generator` | `osbot_utils.helpers.cache` | SHA-256 hashing with `from_string()`, `from_bytes()`, `from_json()`, `from_json_field()` |
| `Cache_Id` | `osbot_utils.type_safe.primitives.domains.identifiers` | Typed identifier (extends `Random_Guid`) |
| `Safe_Str__Cache_Hash` | `osbot_utils.type_safe.primitives.domains.cryptography.safe_str` | Type-safe hash string |

The client library documentation (`library/dependencies/cache-service/v0.6.0__v0.10.1__cache_service__client__how_to_use.md`) covers all three execution modes (REMOTE, IN_MEMORY, LOCAL_SERVER), every storage strategy, the complete API reference, and testing patterns. The LLM brief (`library/dependencies/cache-service/v0.5.68__cache-service__llm-brief.md`) covers the server-side architecture.

**Impact:** I now understand the full API surface. No need to reverse-engineer from class signatures.

### Q3: "Does Storage_FS have temporal write support?"

**Status:** RESOLVED

**Answer:** Yes -- the cache service handles this natively. The `TEMPORAL` strategy writes to time-organized paths (`data/temporal/2026/02/13/14/30/{cache_id}.json`) automatically. The `TEMPORAL_LATEST` strategy adds a latest pointer. No manual dual-write needed in `Send__Cache__Client`.

From the client API:

```python
# TEMPORAL: one file per event, time-organized
client.store().store__json(strategy="temporal", namespace="analytics", body=event_data)

# TEMPORAL_LATEST: latest pointer + history
client.store().store__json__cache_key(strategy="temporal_latest", namespace="config", cache_key="pulse/latest", body=pulse_data)
```

**Impact:** Removes ~40 lines of custom path-generation code from my original plan. The cache service handles temporal path resolution, sharding, and the latest pointer internally.

### Q4: "Middleware vs hooks for analytics?"

**Status:** PARTIALLY RESOLVED

**Answer:** The brief specifies raw event recording via `TEMPORAL` strategy -- one JSON file per request. The storage mechanism is clear. The **implementation choice** (FastAPI middleware vs Starlette background task vs route-level hooks) remains a Dev decision.

**My decision:** **FastAPI middleware with background task write.** Rationale:

1. **Middleware** captures all HTTP requests, not just transfer API calls. This matches the Architect's schema which includes `page_view` events for static file requests.
2. **Background task** ensures the analytics write does not add latency to the user's response. The `TEMPORAL` store via `IN_MEMORY` mode is fast (same-process, no network), but keeping it in a background task is defensive.
3. **Lambda compatibility** -- Starlette background tasks complete before the Lambda response is returned via Mangum, so events will not be lost.

If background tasks prove problematic in Lambda (the Mangum adapter might not wait for them), I will fall back to inline middleware writes. With `IN_MEMORY` mode, the latency cost is negligible.

### Q5: "Admin authentication pattern?"

**Status:** OPEN (not addressed in brief)

**Answer:** The brief focuses on data architecture and does not specify the admin authentication pattern. The existing `SGRAPH_SEND__ACCESS_TOKEN` header is used for transfer routes in the user Lambda.

**My default:** Same `SGRAPH_SEND__ACCESS_TOKEN` pattern for admin endpoints initially. The admin Lambda already has separate deployment, so a distinct admin token can be added later without affecting the user Lambda. This is a non-blocking item -- the cache service integration does not depend on admin auth being resolved.

**Action:** Flag for Dinis in the next brief cycle. For Phase 1, the admin endpoints will check the same access token. Phase 2 can introduce a separate `SGRAPH_SEND__ADMIN_TOKEN` if needed.

### Q6: "Confirm S3 layout"

**Status:** RESOLVED

**Answer:** Same S3 bucket, prefix separation mirrors the Lambda architecture:

| Lambda | S3 Prefix | Data Domain |
|--------|-----------|-------------|
| `lambda__user` | `user/` | Transfers, encrypted payloads |
| `lambda__admin` | `admin/` | Analytics, tokens, costs, transfer event caches |

The existing `Storage_FS__S3` class supports `s3_prefix`. The current code in `Send__Config.create_storage_backend()` creates a `Storage_FS__S3(s3_bucket=self.s3_bucket)` without a prefix. This will be updated to `Storage_FS__S3(s3_bucket=self.s3_bucket, s3_prefix="user/")` for the transfer service. The cache service client gets its own `Storage_FS__S3` with `s3_prefix="admin/"`.

**Impact:** Zero new S3 buckets, zero new IAM policies. Clean prefix isolation.

---

## 2. Revised Implementation Plan

### What Changed

| Aspect | v0.2.33 Plan | Revised Plan |
|--------|-------------|--------------|
| **Approach** | Build `Cache__Service__Send` from scratch | Build `Send__Cache__Client` wrapper over `Cache__Service__Client` |
| **Core class** | Custom class with Storage_FS path generation | `Type_Safe` wrapper following `Html_Cache__Client` pattern |
| **Hash resolution** | Custom implementation with sharded directories | Built into cache service -- `cache_id__from_key()` pattern |
| **Temporal storage** | Custom dual-write to date paths + latest pointer | `TEMPORAL` and `TEMPORAL_LATEST` strategies -- automatic |
| **Namespace setup** | Custom directory tree creation | First-class cache service feature -- namespace parameter on every call |
| **Estimated effort** | 25-35 hours across 3 weeks | 8-12 hours across 2 phases |
| **New dependencies** | None (all in osbot-utils) | `mgraph-ai-service-cache`, `mgraph-ai-service-cache-client` on PyPI |

### Phase 1: Wrapper + Raw Events + Tokens (~4-6 hours, single session)

| # | Task | Approach | Test Count |
|---|------|----------|------------|
| 1 | **`Send__Cache__Client` scaffolding** | `Type_Safe` class with `cache_client: Cache__Service__Client` and `hash_generator: Cache__Hash__Generator`. Configure `IN_MEMORY` mode. Define 4 namespace constants (`NS_ANALYTICS`, `NS_TOKENS`, `NS_COSTS`, `NS_TRANSFERS`). Health check method. | +3 |
| 2 | **Analytics raw event recording** | `analytics__record_event(event)` calls `cache_client.store().store__json(strategy="temporal", namespace="analytics", body=event.json())`. FastAPI middleware writes one event per request. | +4 |
| 3 | **Pulse computation** | `analytics__pulse(window_minutes=5)` reads recent temporal files via `cache_client.admin_storage().files__all__path(path="analytics/data/temporal/...")`, filters by time window, counts, returns snapshot. Store latest via `TEMPORAL_LATEST` if desired. | +3 |
| 4 | **Token CRUD** | `token__create(name, limit)` uses `KEY_BASED` strategy with `json_field_path='cache_key'`. `token__lookup(name)` hashes the name, resolves to `cache_id`, retrieves entry. `token__use(name, ip_hash, action)` stores child data event under token's `cache_id`. `token__revoke(name)` updates entry status. `token__list()` uses namespace listing. | +8 |
| 5 | **Integration wiring** | Update `Fast_API__SGraph__App__Send__User.setup()` to create `Send__Cache__Client`. Pass to `Routes__Transfers`. Update `Send__Config` to add `s3_prefix="user/"` to transfer storage. | +2 |

**Phase 1 deliverable:** Analytics recording (every request writes a raw event), pulse endpoint ("is anyone using the site?"), token system operational (create, lookup, use, revoke, list). This answers the two critical questions from the Conductor: visibility and friendlies onboarding.

### Phase 2: LETS Aggregations + Admin Endpoints (~4-6 hours)

| # | Task | Approach | Test Count |
|---|------|----------|------------|
| 6 | **30-min + hourly aggregation** | Read temporal raw events for window, count/sum, store via `KEY_BASED` at `aggregations/hourly/2026-02-13-14.json`. Check-before-compute: if aggregation exists and window is closed, return cached. | +4 |
| 7 | **Daily/monthly cascading** | Combine 24 hourly aggregations into daily. Combine 28-31 daily into monthly. Same check-before-compute pattern. | +3 |
| 8 | **Transfer event dual-write** | When a transfer event occurs (upload, complete, download), also call `analytics__record_event()` and `transfer__record_event(transfer_id, event)` which stores child data under the transfer's `cache_id`. | +3 |
| 9 | **Admin REST endpoints** | `Routes__Tokens` with CRUD endpoints. `Routes__Analytics` with pulse + aggregation query endpoints. Both in `lambda__admin`. | +6 |

**Phase 2 deliverable:** Full LETS pipeline operational. Admin can query historical analytics. Transfer events flow to both analytics stream and per-file event cache. Admin Lambda has REST endpoints for token management and analytics queries.

### Phase 3: Admin UI + Cost Tracking (future sprint)

| # | Task |
|---|------|
| 10 | Admin UI v0.1.1: token management panel (IFD) |
| 11 | Admin UI v0.1.2: analytics dashboard (IFD) |
| 12 | AWS cost collector + cost aggregations |

---

## 3. Updated Confidence Levels

| Area | v0.2.33 | Now | Change Reason |
|------|---------|-----|---------------|
| Cache service scaffolding | HIGH | **VERY HIGH** | Following a proven reference implementation (`Html_Cache__Client`). No guesswork. |
| Hash generator integration | MEDIUM | **VERY HIGH** | Full API documented in client how-to. `from_string()`, `from_bytes()`, `from_json()`, `from_json_field()` all documented with examples. |
| Raw event recording | HIGH | **VERY HIGH** | `TEMPORAL` strategy is a single API call. The cache service handles path generation. |
| Pulse computation | HIGH | **HIGH** | Requires reading temporal files by time prefix via admin storage. Clear API (`files__all__path`), but time-window filtering needs care. |
| Aggregation cascade (LETS) | MEDIUM | **HIGH** | Logic unchanged from v0.2.33, but now operating on cache service API calls instead of raw Storage_FS paths. Less surface area for path bugs. |
| Token system | HIGH | **VERY HIGH** | `KEY_BASED` + child data is exactly the pattern in `Html_Cache__Client`. Direct mapping. |
| Admin REST endpoints | HIGH | **VERY HIGH** | Standard FastAPI routes, same patterns as user Lambda. No new patterns. |
| IN_MEMORY integration | N/A | **HIGH** | Well-documented in client how-to. Need to verify exact wiring for `IN_MEMORY` mode within the Lambda process. The `_requests._app` / `_requests._setup_mode()` pattern from the docs is clear. |
| S3 prefix migration | N/A | **HIGH** | `Storage_FS__S3` already supports `s3_prefix`. Adding `s3_prefix="user/"` to `Send__Config.create_storage_backend()` is a one-line change. Need to handle migration of existing test data in the root `transfers/` folder. |

---

## 4. Testing Strategy (Revised)

### What Changed from v0.2.33

The v0.2.33 plan tested against `Storage_FS__Memory` directly, manually constructing paths and verifying file existence. The revised plan tests through the cache service API in `IN_MEMORY` mode, which is the same execution path as production (minus the S3 backend).

### Test Infrastructure Setup

```python
from mgraph_ai_service_cache.fast_api.Cache_Service__Fast_API import Cache_Service__Fast_API
from mgraph_ai_service_cache_client.client_contract.Service__Fast_API__Client import Cache__Service__Fast_API__Client
from mgraph_ai_service_cache_client.client_contract.Service__Fast_API__Client__Config import Cache__Service__Fast_API__Client__Config
from osbot_utils.helpers.cache.Cache__Hash__Generator import Cache__Hash__Generator

# Create cache service with in-memory storage
service = Cache_Service__Fast_API().setup()

# Create client in IN_MEMORY mode
config = Cache__Service__Fast_API__Client__Config()
client = Cache__Service__Fast_API__Client(config=config)
client._requests._app = service.app
client._requests._setup_mode()

# Create Send__Cache__Client wrapper
send_cache = Send__Cache__Client(cache_client=client, hash_generator=Cache__Hash__Generator())
```

### Test Categories

| Category | Tests | What They Verify |
|----------|-------|------------------|
| **Wrapper scaffolding** | 3 | Health check, namespace constants, client wiring |
| **Analytics recording** | 4 | Raw event write via TEMPORAL, event structure, temporal path correctness, high-throughput (100 events) |
| **Pulse** | 3 | Pulse from recent events, empty pulse, window filtering |
| **Token lifecycle** | 8 | Create, lookup, use (increment), exhaust (limit reached), revoke, list, duplicate name handling, lookup-after-revoke |
| **Aggregation** | 7 | 30-min window, hourly cascade, daily cascade, check-before-compute (idempotent), live-window recompute, empty-window edge case, concurrent-write safety |
| **Transfer dual-write** | 3 | Event in analytics stream, event in transfer child data, cross-reference integrity |
| **Integration** | 2 | Full FastAPI app with `Send__Cache__Client` wired in, end-to-end request produces analytics event |

**Total: ~30 tests** (vs. 35+ in v0.2.33 plan, but with higher coverage per test because the cache service handles more internally).

### Key Testing Principle

All tests use the cache service in `IN_MEMORY` mode. No mocks. No patches. The test exercises the same code path as production -- the only difference is the storage backend (memory vs S3). This matches the project's testing rules exactly.

---

## 5. Remaining Open Items

| # | Item | Status | Owner | Blocking? |
|---|------|--------|-------|-----------|
| 1 | **Admin authentication pattern (Q5)** | Open | Dinis | No -- default to same access token for Phase 1 |
| 2 | **Middleware implementation choice (Q4)** | Decided (Dev) | Dev | No -- middleware + background task, fallback to inline if Lambda issues |
| 3 | **Existing `transfers/` data migration** | New item | Dev | No -- brief says delete root `transfers/` folder (test data only), start clean with `user/transfers/` |
| 4 | **Lambda package size with cache dependencies** | Risk to monitor | DevOps | No -- monitor cold start after adding `mgraph-ai-service-cache` + client |
| 5 | **Decision D034 approval** | Pending Dinis | Dinis | No -- proceeding with `IN_MEMORY` mode as recommended |

---

## 6. New Dependencies

The following packages need to be added to `APP__SEND__USER__LAMBDA_DEPENDENCIES`:

| Package | Version | PyPI |
|---------|---------|------|
| `mgraph-ai-service-cache` | v0.6.0+ | [pypi.org/project/mgraph-ai-service-cache](https://pypi.org/project/mgraph-ai-service-cache/) |
| `mgraph-ai-service-cache-client` | v0.10.1 | [pypi.org/project/mgraph-ai-service-cache-client](https://pypi.org/project/mgraph-ai-service-cache-client/) |

These are already deployed in production for the MGraph-AI HTML service. The `IN_MEMORY` execution mode means there is no runtime network dependency -- the cache service FastAPI app runs in the same process via `TestClient`.

---

## 7. What I Will Build First

**Gating item: `Send__Cache__Client` + raw analytics recording.**

This is the foundation everything else depends on. The sequence:

1. **Create `Send__Cache__Client`** following the `Html_Cache__Client` pattern exactly. Fields: `cache_client`, `hash_generator`. Four namespace constants. Health check. Entry and data operations scoped to SGraph Send's domain (tokens, analytics, costs, transfers).

2. **Wire `IN_MEMORY` mode** in `Fast_API__SGraph__App__Send__User.setup()`. The cache service FastAPI app runs in-process. The client connects via `TestClient`. No network, no separate Lambda.

3. **Add analytics middleware** that writes one raw event per request via `TEMPORAL` strategy to the `analytics` namespace.

4. **Write tests** -- scaffolding + analytics recording + pulse. Target: 10 tests passing in the first session.

Once raw events are flowing, everything else cascades: pulse reads those events, aggregations summarise them, tokens use the same cache client with a different namespace, admin endpoints query the same data.

---

## 8. Decisions Acknowledged

| # | Decision | My Understanding | Status |
|---|----------|------------------|--------|
| D024 (updated) | Use existing MGraph-AI Cache Service via `Cache__Service__Client` in `IN_MEMORY` mode | Thin wrapper, not from-scratch build. Same-process execution. | Acknowledged, implementing |
| D025 (updated) | `Cache__Hash__Generator` already integrated in cache service | Configure `json_field_path` for field-level hashing (tokens use `cache_key` field) | Acknowledged |
| D026 (updated) | Namespace separation is a first-class cache service feature | Namespace parameter on every API call. No custom directory creation needed. | Acknowledged |
| D027 (unchanged) | On-demand LETS pipeline with cascading save | Custom logic on top of cache service API. Storage handled, compute logic is ours. ~200-300 lines. | Acknowledged, Phase 2 |
| D028-D030 (unchanged) | Human-friendly tokens, cost namespace, GA removal gate | Unchanged from Architect's design | Acknowledged |
| D034 (new) | Cache service runs in-process with `admin/` prefix, transfers get `user/` prefix | Mirrors Lambda architecture. Extraction to separate Lambda is a config change. | Acknowledged, **recommend approval** |

---

## 9. Risk Assessment (Updated)

| Risk | Severity | v0.2.33 Assessment | Current Assessment | Notes |
|------|----------|--------------------|--------------------|-------|
| RF-15: Cache service schema delay | Medium | Open | **Closed** | No custom schema to design -- cache service exists |
| RF-18: LETS aggregation correctness | Medium | Open | **Open** | Still the main custom logic. Edge cases: partial windows, timezone boundaries, concurrent writes. Tests will cover. |
| RF-21: Scope expansion (5 cache consumers) | Medium | Open | **Reduced** | Cache service handles complexity. Wrapper is thin. Each namespace is isolated. |
| RF-23: Zero analytics (P0) | High | Open | **Faster to resolve** | Phase 1 delivers raw events + pulse in one session (~4-6 hours) |
| NEW: Cache service dependency in Lambda | Medium | N/A | **Open** | Two new PyPI packages. `IN_MEMORY` mode means no network dependency. Monitor cold start times. |
| NEW: Shared S3 bucket interference | Low | N/A | **Open** | Prefix isolation (`admin/` vs `user/`). Cache service never writes outside its configured prefix. |
| NEW: `_requests._app` / `_setup_mode()` internal API | Low | N/A | **Open** | The `IN_MEMORY` wiring uses internal client APIs (`_requests._app`, `_requests._setup_mode()`). These could change in future client versions. Mitigated: pin the client version. |

---

*Dev review complete. The brief resolves both blocking questions and fundamentally simplifies the implementation. The pivot from "build from scratch" to "thin wrapper over existing service" is the right call. The `Html_Cache__Client` reference provides an exact template. I am ready to start Phase 1 immediately.*
