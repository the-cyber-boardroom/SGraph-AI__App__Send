# v0.2.33 — Dev: Cache Service Implementation Plan

**Version:** v0.2.33
**Date:** 13 February 2026
**Role:** Dev
**Context:** Cache service is the #1 priority for the next sprint. Project lead has agreed to pair on this.
**Depends on:** [Architect Cache Service Schema](../../../architect/reviews/26-02-13/v0.2.33__response-to-daily-brief-2__13-feb.md) (D024-D030, all approved)
**Companion:** [Architect Server/Admin Review](../../../architect/reviews/26-02-13/v0.2.33__review__server-admin-features-next-sprint.md)

---

## Executive Summary

The Architect has fully specified the cache service: 4 schemas, 7 decisions, Type_Safe data models, file tree layouts, and a LETS pipeline design. This document translates that specification into an implementation plan — what I (the Dev) understand, what I can build independently, and **where I need Dinis's help**.

The cache service replaces Google Analytics (now removed) with server-side analytics, powers the token system for friendlies onboarding, and provides the data backbone for the admin dashboard. It is the single most important backend component for the next sprint.

---

## 1. What I Understand

### 1.1 The Big Picture

The cache service is a **layer on top of Memory-FS** (not a replacement). It adds three things:

1. **Cache ID resolution** — every entry gets a unique ID that maps to a file tree
2. **Cache Hash lookup** — human-meaningful values (like token names) resolve to Cache IDs via SHA-256
3. **Temporal storage** — automatic date-path filing (`year/month/day/hour`) with a `latest` pointer

Everything goes through `Storage_FS`. The cache service just generates the right paths and orchestrates the reads/writes.

### 1.2 The Four Namespaces

| Namespace | Purpose | Priority |
|-----------|---------|----------|
| `analytics` | Site analytics (replaces GA) | **P0** — we're flying blind without it |
| `tokens` | Human-friendly invitation tokens with usage limits | **P1** — needed for friendlies rollout |
| `costs` | AWS cost tracking and unit economics | **P2** — useful but not blocking |
| `transfers` | Per-file event analytics (views, downloads) | **P2** — enhances existing transfer system |

### 1.3 The LETS Pipeline

Every server request writes a raw JSON event file. When someone queries aggregated data:

1. Check if the aggregation file exists
2. If the time window is complete (past), return the cached aggregation — it's immutable
3. If the time window is current (live), recompute from raw data
4. If no aggregation exists, compute from the next-lower granularity (or raw data)
5. Save the result (the "S" in LETS — save the query)

Aggregations cascade: 30-min → hourly → daily → monthly → yearly. Each level combines the level below it. You only pay the computation cost once.

### 1.4 How It Fits the Existing Code

The Transfer Service (`Transfer__Service.py`) currently:
- Creates transfers with metadata in `transfers/{transfer_id}/meta.json`
- Stores encrypted payloads in `transfers/{transfer_id}/payload`
- Logs events in the metadata dict

The cache service adds a **dual-write**: when a transfer event occurs, it also writes to:
- `cache/analytics/data/raw/{year}/{month}/{day}/{hour}/{timestamp}__{event_id}.json` (feeds site analytics)
- `cache/transfers/data/events/{cache_id}/{temporal_path}/{timestamp}__{event_id}.json` (feeds per-file analytics)

The existing transfer storage is **unchanged**. The cache layer is additive.

---

## 2. What I Can Build Independently

### 2.1 Cache Service Core Scaffolding

I can build the `Cache__Service__Send` class with:

```python
class Cache__Service__Send(Type_Safe):
    storage_fs     : Storage_FS                # Shared backend
    hash_generator : Cache__Hash__Generator     # From osbot-utils

    NS_ANALYTICS   : str = 'analytics'
    NS_TOKENS      : str = 'tokens'
    NS_COSTS       : str = 'costs'
    NS_TRANSFERS   : str = 'transfers'
```

Methods I can implement:
- `setup_namespaces()` — create the directory tree for all 4 namespaces
- `resolve_cache_hash(value: str) -> Safe_Str__Cache_Hash` — hash a value
- `create_cache_entry(namespace, source_value) -> Cache_Id` — create entry, write refs
- `lookup_by_hash(namespace, cache_hash) -> Cache_Id` — find entry from hash
- `write_temporal(namespace, cache_id, data, filename)` — write to date path + latest
- `read_latest(namespace, cache_id, filename)` — read from latest pointer

**Tests:** All use `Storage_FS__Memory`. No mocks. I can write comprehensive tests for all of the above.

### 2.2 Analytics Raw Event Recording

I can add a middleware or hook to the FastAPI app that writes a raw event file for every request:

```python
# In the request lifecycle
event = Schema__Analytics__Raw_Event(
    event_type  = classify_request(request),    # 'page_view', 'api_call', etc.
    path        = request.url.path,
    method      = request.method,
    status_code = response.status_code,
    ip_hash     = hash_ip(request.client.host),
    ...
)
cache_service.write_temporal('analytics', None, event.json(), f"{timestamp}__{event_id}.json")
```

### 2.3 Pulse Endpoint

The pulse is always recomputed (never cached). It reads the last 5 minutes of raw event files and produces a snapshot:

```python
def compute_pulse(self, window_minutes=5) -> Schema__Analytics__Pulse:
    # Read raw files from the last N minutes
    # Count requests, unique visitors, active transfers
    # Return pulse snapshot
```

This is the minimum viable GA replacement: "Is anyone using the site right now?"

### 2.4 Token CRUD

Once the cache service core exists, the token system is straightforward:

- `create_token(name, limit)` — hash name → create cache entry → write metadata
- `lookup_token(name)` — hash name → find cache entry → read metadata
- `use_token(name, ip_hash, action)` — lookup → check limits → increment → write event
- `revoke_token(name)` — lookup → set status to 'revoked'
- `list_tokens()` — scan `cache/tokens/data/direct/*/token.json`

### 2.5 Basic Aggregation

I can implement the 30-min and hourly aggregation cascade:

- Read raw event files for the time window
- Count totals (requests, unique visitors, uploads, downloads, etc.)
- Save the aggregation
- Combine two 30-min aggregations into one hourly

---

## 3. Where I Need Dinis's Help

### 3.1 Reference Cache__Service Code (CRITICAL)

**What:** The Architect references a `Cache__Service` implementation that Dinis provided in `team/humans/dinis_cruz/code-example/example-of-memory-fs-with-s3/`. I need access to this code to understand:

- How `Cache__Hash__Generator` is configured (hash length, algorithm options)
- How `Cache__Config` wires up storage backends
- How namespace handlers are structured
- How temporal path generation works in the reference implementation
- The `Cache__Handler` interface pattern

**Why I need Dinis:** The reference code demonstrates the patterns from `osbot-utils` that I should follow. Building without it risks diverging from the established patterns.

**Ask:** Can you share the reference `Cache__Service` code (or point me to it) so I can align my implementation?

### 3.2 osbot-utils Cache Module (CRITICAL)

**What:** The Architect specifies these imports:

```python
from osbot_utils.helpers.cache.Cache__Hash__Generator import Cache__Hash__Generator
from osbot_utils.helpers.cache.schemas.Schema__Cache__Hash__Config import Schema__Cache__Hash__Config
from osbot_utils.type_safe.primitives.domains.identifiers.Cache_Id import Cache_Id
from osbot_utils.type_safe.primitives.domains.cryptography.safe_str.Safe_Str__Cache_Hash import Safe_Str__Cache_Hash
```

**Why I need Dinis:** I need to understand:
- What version of `osbot-utils` has these classes? Are they in the version we're currently using, or do we need an upgrade?
- What's the API surface of `Cache__Hash__Generator`? The Architect mentions `from_string()`, `from_bytes()`, `from_json()` — are there others?
- Are there existing tests in osbot-utils I should look at for usage patterns?
- Is `Cache_Id` a typed string or does it have special generation logic?

**Ask:** Can you walk me through the `osbot-utils` cache module, or share the test files that exercise it?

### 3.3 Storage_FS Temporal Write Pattern (IMPORTANT)

**What:** The cache service needs to write files to both a temporal path (`2026/02/13/14/`) and a `latest/` path simultaneously. Does `Storage_FS` have built-in support for this, or do I need to implement it?

**Why I need Dinis:** The `Storage_FS` API surface in `osbot-utils` may already have temporal write capabilities that I'm not aware of. If it does, I should use them rather than reimplementing.

**Ask:** Does `Storage_FS` support temporal writes, or should I implement the dual-write (temporal + latest) in `Cache__Service__Send`?

### 3.4 Integration Point: FastAPI Middleware vs Hook (IMPORTANT)

**What:** To record analytics for every request, I need to intercept the request/response cycle. Options:

1. **FastAPI middleware** — intercept every request, write event after response
2. **Starlette background task** — write event in background after response is sent
3. **Event hook in Transfer Service** — only capture transfer-related events (not all HTTP requests)

**Why I need Dinis:** The choice affects:
- Whether we capture static file requests (JS, CSS, images) — middleware captures these, hooks don't
- Whether analytics recording adds latency to responses — background tasks avoid this
- Whether the approach works with both Lambda and non-Lambda deployments

**Ask:** Should I capture ALL HTTP requests (middleware approach) or only transfer API calls (hook approach)? The Architect's schema suggests all requests (it includes `page_view` events), which means middleware.

### 3.5 Admin Lambda Wiring (MODERATE)

**What:** The admin Lambda needs new routes for token management and analytics. Currently it has only health + info routes. I need to:

1. Add `Routes__Tokens` with CRUD endpoints
2. Add `Routes__Analytics` with pulse + aggregation endpoints
3. Ensure both Lambdas share the same `Storage_FS` backend

**Why I need Dinis:** The admin Lambda's deployment configuration and access control pattern may need adjustment. The admin routes need stronger authentication than the user Lambda (admin-only access).

**Ask:** What authentication pattern should the admin endpoints use? Same `SGRAPH_SEND__ACCESS_TOKEN` or a separate admin-specific credential?

### 3.6 S3 Bucket Layout (MODERATE)

**What:** In production, the cache service writes to S3 via `Storage_FS__S3`. The Architect specified separate namespace prefixes (`cache/analytics/`, `cache/tokens/`, etc.) within the storage. But should the cache data go in:

- The **same S3 bucket** as transfers (`{account_id}--sgraph-send-transfers--{region}`)?
- A **separate S3 bucket** for cache/analytics data?

**Why I need Dinis:** Decision D019 from Brief #1 mentions a "separate observability S3 bucket." The Architect says CloudFront logs go to the observability bucket, but the cache service data lives in the main bucket. I want to confirm this with you before implementing.

**Ask:** Confirm: cache service data (analytics aggregations, token metadata, cost data) goes in the main transfers bucket under a `cache/` prefix. Raw CloudFront logs go in a separate observability bucket. Is that right?

---

## 4. Implementation Sequence (My Plan)

### Week 1: Core + Visibility

| Day | Task | Deliverable | Test Count |
|-----|------|-------------|------------|
| **1** | Cache service scaffolding: class, namespaces, hash generator wiring | `Cache__Service__Send` with `setup_namespaces()`, `resolve_cache_hash()`, `create_cache_entry()`, `lookup_by_hash()` | +8 tests |
| **1** | Temporal write/read: `write_temporal()`, `read_latest()` | Dual-write to date path + latest pointer | +4 tests |
| **2** | Analytics raw event recording: FastAPI middleware (or hook, per Dinis's answer to 3.4) | Every request writes a JSON file to `cache/analytics/data/raw/` | +4 tests |
| **2** | Pulse computation: `compute_pulse()` | "Is anyone using the site?" — reads last 5 min of raw events | +3 tests |
| **3** | 30-min + hourly aggregation with cascading save | `compute_aggregation()` for analytics namespace | +6 tests |

**Week 1 deliverable:** Server-side analytics that answers "is anyone using the site?" and "how many people today?" — the two minimum viable GA replacements identified in the Conductor's plan.

### Week 2: Tokens + Admin

| Day | Task | Deliverable | Test Count |
|-----|------|-------------|------------|
| **4** | Token CRUD: create, lookup, use, revoke, list | Full token lifecycle in cache service | +10 tests |
| **5** | Admin token REST endpoints + admin analytics endpoints | Admin Lambda serves token + analytics APIs | +8 tests |
| **6** | Transfer event dual-write integration | Transfer events flow to both transfer storage and analytics | +4 tests |

**Week 2 deliverable:** Token system operational. Admin can create tokens, share URLs, see usage. Analytics dashboard has data.

### Week 3: Admin UI + Polish

| Day | Task | Deliverable |
|-----|------|-------------|
| **7-8** | Admin UI v0.1.1: token management panel (IFD) | Create, list, revoke tokens. Share URLs. |
| **9-10** | Admin UI v0.1.2: analytics dashboard (IFD) | Pulse view, daily/hourly charts, top paths. |

---

## 5. Testing Strategy

All tests use `Storage_FS__Memory`. No mocks. No patches. The cache service is a pure logic layer on Storage_FS, so testing is clean and fast.

```python
def test_cache_service_create_and_lookup():
    cache = Cache__Service__Send(storage_fs=Storage_FS__Memory())
    cache.setup_namespaces()

    # Create a token
    cache_id = cache.create_cache_entry('tokens', 'community-x')
    assert cache_id is not None

    # Lookup by hash
    found_id = cache.lookup_by_hash('tokens', cache.resolve_cache_hash('community-x'))
    assert found_id == cache_id

def test_analytics_raw_event_write():
    cache = Cache__Service__Send(storage_fs=Storage_FS__Memory())
    cache.setup_namespaces()

    event = Schema__Analytics__Raw_Event(event_type='page_view', path='/', ...)
    cache.record_analytics_event(event)

    # Verify file was written to temporal path
    files = cache.storage_fs.files_list('cache/analytics/data/raw/')
    assert len(files) == 1

def test_lets_pulse():
    cache = Cache__Service__Send(storage_fs=Storage_FS__Memory())
    cache.setup_namespaces()

    # Write 10 events
    for i in range(10):
        cache.record_analytics_event(Schema__Analytics__Raw_Event(...))

    # Compute pulse
    pulse = cache.compute_pulse(window_minutes=5)
    assert pulse.active_requests == 10

def test_token_lifecycle():
    cache = Cache__Service__Send(storage_fs=Storage_FS__Memory())
    cache.setup_namespaces()

    # Create
    token = cache.create_token(name='beta-invite', limit=10)
    assert token.usage_count == 0
    assert token.usage_limit == 10

    # Use 3 times
    for i in range(3):
        result = cache.use_token('beta-invite', ip_hash=f'hash_{i}', action='page_opened')
        assert result.success is True

    token = cache.lookup_token('beta-invite')
    assert token.usage_count == 3

    # Exhaust the limit
    for i in range(7):
        cache.use_token('beta-invite', ip_hash=f'hash_{10+i}', action='page_opened')

    result = cache.use_token('beta-invite', ip_hash='hash_extra', action='page_opened')
    assert result.success is False
    assert result.rejection_reason == 'exhausted'
```

---

## 6. Questions Summary (For Dinis)

| # | Question | Blocking? | Default if No Answer |
|---|----------|-----------|---------------------|
| **Q1** | Share the reference `Cache__Service` code from your code-example directory | **Yes** | I'll implement from the Architect's spec alone, but may diverge from osbot-utils patterns |
| **Q2** | Walk me through the `osbot-utils` cache module (or point to tests) | **Yes** | I'll inspect the installed package and work from the class signatures |
| **Q3** | Does `Storage_FS` have built-in temporal write support? | No | I'll implement dual-write in `Cache__Service__Send` |
| **Q4** | Middleware (all requests) or hooks (transfer events only) for analytics? | No | Middleware — capture everything, including static file requests |
| **Q5** | Admin authentication pattern — same token or separate? | No | Same `SGRAPH_SEND__ACCESS_TOKEN` for now, separate admin token later |
| **Q6** | Confirm S3 layout — cache data in main bucket, CloudFront logs in observability bucket? | No | Yes, as Architect specified |

---

## 7. My Confidence Level

| Area | Confidence | Notes |
|------|-----------|-------|
| Cache service scaffolding | **HIGH** | Clear spec from Architect, straightforward Storage_FS operations |
| Hash generator integration | **MEDIUM** | Need to see the osbot-utils API to be sure about configuration options |
| Raw event recording | **HIGH** | Standard FastAPI middleware pattern |
| Pulse computation | **HIGH** | Read files from last N minutes, count, return |
| Aggregation cascade | **MEDIUM** | The logic is clear, but edge cases (partial windows, timezone handling, concurrent writes) need careful testing |
| Token system | **HIGH** | CRUD with hash lookup — well-understood pattern |
| Admin REST endpoints | **HIGH** | Standard FastAPI routes, same patterns as user Lambda |
| Admin UI | **MEDIUM** | Need to follow IFD methodology and Web Components pattern — will reference v0.1.3 user UI as template |

---

*Dev review complete. The cache service is ready to implement. The Architect's schema is comprehensive and the testing strategy is clear. The two blocking questions for Dinis are: (1) share the reference Cache__Service code, and (2) walk through the osbot-utils cache module. Everything else can proceed with sensible defaults.*
