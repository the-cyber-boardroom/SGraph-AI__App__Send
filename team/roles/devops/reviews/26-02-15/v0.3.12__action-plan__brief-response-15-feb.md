# v0.3.12 -- DevOps Action Plan: Response to Explorer Daily Brief (15 Feb)

**Version:** v0.3.12
**Date:** 15 February 2026
**Role:** DevOps (Explorer team)
**Brief:** `team/humans/dinis_cruz/briefs/02/15/v0.3.10__daily-brief__explorer-team-15-feb-2026.md`
**Previous:** `team/roles/devops/reviews/26-02-15/v0.3.8__deployment-checklist__observability-pipeline.md`
**Master index:** `team/roles/librarian/reviews/26-02-15/v0.3.10__master-index__daily-brief-15-feb.md`

---

## 1. Acknowledged Tasks

### Primary Assignment: S3-Triggered Lambda for LETS Pipeline (Task #15)

**Priority:** P2
**Co-owner:** Dev
**Brief section:** SA Deep Dive -- "S3 event triggers Lambda function (automatic)"
**Dependency:** Dev waits for this S3 event configuration before implementing the LETS transformation Lambda

The brief describes an **event-driven data pipeline** where log files landing in S3 automatically trigger a Lambda function to process them:

```
CloudFront/Lambda/S3 --> logs appear in S3
    |
    v
S3 event triggers Lambda function (automatic)
    |
    v
Lambda extracts, normalises, compresses (~90% reduction)
    |
    v
Saves curated version to cache service (temporal + latest)
```

This is the foundation of the Server Analytics event-driven pipeline. Without it, log transformation must be triggered manually or on-demand, which defeats the purpose of real-time-ish analytics.

**Downstream dependencies:**
- Task #22 (delta tracking for collected data) is blocked by this pipeline
- CloudFront log viewer (Task #13) depends on processed log data
- The "digital twin" concept depends on continuous log processing

---

## 2. Carryover: Observability Pipeline Deployment Status

### Metrics Environment Variables -- COMPLETED

The deployment checklist (`v0.3.8__deployment-checklist__observability-pipeline.md`) documented:

| Item | Status |
|------|--------|
| IAM policy for CloudWatch metrics (admin Lambda role) | Documented, ready to apply |
| 5 required env vars for `/metrics/*` endpoints | Documented in `admin__config.py` |
| `METRICS__ENABLED` conditional gate | Implemented -- graceful degradation when env vars not set |
| GitHub Actions secrets mapping | 5 new secrets documented |
| Verification smoke tests | curl commands documented |
| CloudFront us-east-1 dual-client architecture | Confirmed in Architect proposal |
| Optional CloudFront logs read permissions | Documented for future Phase 2 |

**Current deployment state:** The observability pipeline code is merged. The env vars and IAM permissions are documented and ready to be applied to the admin Lambda. Once the 5 required environment variables are set and the CloudWatch IAM policy is attached, the `/metrics/*` endpoints will activate automatically.

**Action remaining:** Apply the IAM policy and set the environment variables in AWS. This is a manual AWS Console action (or osbot-aws script). No code changes needed.

---

## 3. S3 Event Trigger Plan

### 3.1 Source Buckets and Prefixes

Based on the AWS observability setup guide (`v0.3.6__devops__aws-observability-setup-guide.md`), there are three distinct log data sources landing in S3:

| # | Source | S3 Bucket | Prefix | Format | Volume |
|---|--------|-----------|--------|--------|--------|
| 1 | CloudFront Real-Time Logs | `{account_id}--sgraph-send-cf-logs--{region}` | `cloudfront-realtime/year=YYYY/month=MM/day=DD/` | GZIP compressed (Firehose output) | High -- every request |
| 2 | S3 Access Logs (Transfers) | `{account_id}--sgraph-send-s3-access-logs--{region}` | `transfers-bucket/` | Plain text (S3 standard log format) | Medium -- S3 operations |
| 3 | S3 Access Logs (Cache) | `{account_id}--sgraph-send-s3-access-logs--{region}` | `cache-bucket/` | Plain text (S3 standard log format) | Medium -- cache operations |

**Note:** CloudWatch Logs and CloudTrail are not S3-event-triggered -- they use different collection mechanisms (API polling via the "collected data" pipeline, not the event-driven pipeline).

### 3.2 Event Types

Use **`s3:ObjectCreated:*`** for all triggers. This fires on:
- `s3:ObjectCreated:Put` -- direct object writes
- `s3:ObjectCreated:Post` -- multipart upload completion
- `s3:ObjectCreated:Copy` -- object copies
- `s3:ObjectCreated:CompleteMultipartUpload` -- large file uploads

For Firehose delivery (CloudFront logs), the event type is `s3:ObjectCreated:Put`. For S3 access logs, AWS writes them as regular `PutObject` operations. Using `s3:ObjectCreated:*` covers both without risk of missing events.

### 3.3 Trigger Configuration

There are two options for the S3 event notification target: **S3 Event Notifications to Lambda** (direct) or **S3 Event Notifications to SQS, then Lambda polls SQS**. Recommendation: start with direct S3-to-Lambda for simplicity.

#### Option A: Direct S3 --> Lambda (Recommended for Explorer)

```
S3 bucket (log file lands)
    |
    v
S3 Event Notification (filter by prefix + suffix)
    |
    v
Lambda function: sgraph-send-lets-processor
    |
    v
Reads source object from S3 --> transforms --> writes to cache service bucket
```

**Configuration per source:**

**Trigger 1: CloudFront Real-Time Logs**

| Setting | Value |
|---------|-------|
| Source bucket | `{account_id}--sgraph-send-cf-logs--{region}` |
| Event type | `s3:ObjectCreated:*` |
| Prefix filter | `cloudfront-realtime/` |
| Suffix filter | `.gz` (Firehose outputs GZIP) |
| Target | Lambda: `sgraph-send-lets-processor` |

**Trigger 2: S3 Access Logs (Transfers bucket)**

| Setting | Value |
|---------|-------|
| Source bucket | `{account_id}--sgraph-send-s3-access-logs--{region}` |
| Event type | `s3:ObjectCreated:*` |
| Prefix filter | `transfers-bucket/` |
| Suffix filter | (none -- S3 access logs have no standard extension) |
| Target | Lambda: `sgraph-send-lets-processor` |

**Trigger 3: S3 Access Logs (Cache bucket)**

| Setting | Value |
|---------|-------|
| Source bucket | `{account_id}--sgraph-send-s3-access-logs--{region}` |
| Event type | `s3:ObjectCreated:*` |
| Prefix filter | `cache-bucket/` |
| Suffix filter | (none) |
| Target | Lambda: `sgraph-send-lets-processor` |

### 3.4 Lambda Function: `sgraph-send-lets-processor`

This is a **new, third Lambda function** -- separate from the existing User and Admin Lambdas. Rationale:

1. **Separation of concerns.** The LETS processor has a different trigger mechanism (S3 event, not HTTP), different IAM needs (read from log buckets, write to cache bucket), and different performance profile (batch processing, not request-response).
2. **Independent scaling.** Log processing spikes independently of user/admin traffic. A burst of CloudFront logs should not compete with user upload requests.
3. **Independent deployment.** The LETS processor can be updated without touching the user-facing or admin Lambdas.
4. **Different timeout profile.** Log processing may need longer timeouts than HTTP request handling.

**Lambda configuration:**

| Setting | Value | Rationale |
|---------|-------|-----------|
| Function name | `sgraph-send-lets-processor` (or `sgraph-send-lets-processor-dev` for Explorer) | Descriptive, follows naming convention |
| Runtime | Python 3.12 (arm64) | Matches existing Lambdas |
| Architecture | arm64 | Cost-effective, matches existing |
| Memory | 256 MB | Log parsing + GZIP decompression needs more than 128 MB |
| Timeout | 60 seconds | Firehose batches can be up to 5 MiB compressed |
| Reserved concurrency | 5 | Low -- log processing is not latency-critical; prevents runaway costs |
| Retry attempts | 2 | S3 event notifications retry on failure; 2 retries is safe |
| Dead letter queue | SQS DLQ: `sgraph-send-lets-dlq` | Capture failed processing for investigation |
| Environment variables | See Section 3.5 | |

### 3.5 Environment Variables for LETS Processor Lambda

| Env Var | Required | Example Value | Description |
|---------|----------|---------------|-------------|
| `CACHE__SERVICE__BUCKET_NAME` | Yes | `{cache-bucket-name}` | Cache service S3 bucket (write target) |
| `URL__TARGET_SERVER__CACHE_SERVICE` | Yes | `https://cache.sgraph.ai` | Cache service base URL |
| `LETS__SOURCE_TYPE_MAP` | No | JSON map of prefix-to-source-type | Maps S3 prefixes to log source types for routing |

### 3.6 IAM Permissions

The LETS processor Lambda execution role needs:

```json
{
    "Version": "2012-10-17",
    "Statement": [
        {
            "Sid": "ReadSourceLogBuckets",
            "Effect": "Allow",
            "Action": [
                "s3:GetObject",
                "s3:GetObjectVersion"
            ],
            "Resource": [
                "arn:aws:s3:::{account_id}--sgraph-send-cf-logs--{region}/*",
                "arn:aws:s3:::{account_id}--sgraph-send-s3-access-logs--{region}/*"
            ]
        },
        {
            "Sid": "WriteCacheBucket",
            "Effect": "Allow",
            "Action": [
                "s3:PutObject",
                "s3:GetObject",
                "s3:ListBucket"
            ],
            "Resource": [
                "arn:aws:s3:::{cache-bucket-name}",
                "arn:aws:s3:::{cache-bucket-name}/*"
            ]
        },
        {
            "Sid": "CloudWatchLogs",
            "Effect": "Allow",
            "Action": [
                "logs:CreateLogGroup",
                "logs:CreateLogStream",
                "logs:PutLogEvents"
            ],
            "Resource": "arn:aws:logs:*:*:*"
        },
        {
            "Sid": "DLQAccess",
            "Effect": "Allow",
            "Action": [
                "sqs:SendMessage"
            ],
            "Resource": "arn:aws:sqs:{region}:{account_id}:sgraph-send-lets-dlq"
        }
    ]
}
```

**Additionally**, the S3 source buckets need a **resource-based policy** granting Lambda invocation permissions, OR we use Lambda's resource-based policy to allow S3 to invoke it:

```bash
# Grant each source bucket permission to invoke the LETS Lambda
aws lambda add-permission \
    --function-name sgraph-send-lets-processor \
    --statement-id s3-cf-logs-invoke \
    --action lambda:InvokeFunction \
    --principal s3.amazonaws.com \
    --source-arn arn:aws:s3:::{account_id}--sgraph-send-cf-logs--{region} \
    --source-account {account_id}

aws lambda add-permission \
    --function-name sgraph-send-lets-processor \
    --statement-id s3-access-logs-invoke \
    --action lambda:InvokeFunction \
    --principal s3.amazonaws.com \
    --source-arn arn:aws:s3:::{account_id}--sgraph-send-s3-access-logs--{region} \
    --source-account {account_id}
```

### 3.7 S3 Event Notification Configuration

For each source bucket, configure via the S3 console or osbot-aws:

**Bucket 1: CF Logs Bucket**

```json
{
    "LambdaFunctionConfigurations": [
        {
            "Id": "LETSProcessorCFLogs",
            "LambdaFunctionArn": "arn:aws:lambda:{region}:{account_id}:function:sgraph-send-lets-processor",
            "Events": ["s3:ObjectCreated:*"],
            "Filter": {
                "Key": {
                    "FilterRules": [
                        {"Name": "prefix", "Value": "cloudfront-realtime/"},
                        {"Name": "suffix", "Value": ".gz"}
                    ]
                }
            }
        }
    ]
}
```

**Bucket 2: S3 Access Logs Bucket**

```json
{
    "LambdaFunctionConfigurations": [
        {
            "Id": "LETSProcessorTransfersLogs",
            "LambdaFunctionArn": "arn:aws:lambda:{region}:{account_id}:function:sgraph-send-lets-processor",
            "Events": ["s3:ObjectCreated:*"],
            "Filter": {
                "Key": {
                    "FilterRules": [
                        {"Name": "prefix", "Value": "transfers-bucket/"}
                    ]
                }
            }
        },
        {
            "Id": "LETSProcessorCacheLogs",
            "LambdaFunctionArn": "arn:aws:lambda:{region}:{account_id}:function:sgraph-send-lets-processor",
            "Events": ["s3:ObjectCreated:*"],
            "Filter": {
                "Key": {
                    "FilterRules": [
                        {"Name": "prefix", "Value": "cache-bucket/"}
                    ]
                }
            }
        }
    ]
}
```

---

## 4. Infrastructure Concerns

### 4.1 Cost Implications

S3-triggered Lambda is one of the most cost-effective AWS patterns. At current traffic levels (thousands of requests/day), costs are negligible:

| Component | Estimated Monthly Cost | Calculation |
|-----------|----------------------|-------------|
| Lambda invocations (LETS processor) | < $0.50 | ~10,000 invocations/month at 256 MB, 500ms avg |
| Lambda compute (LETS processor) | < $0.50 | 10,000 x 0.5s x 256MB = ~1.25 GB-seconds |
| S3 GET requests (reading logs) | < $0.01 | 10,000 reads at $0.0004/1000 |
| S3 PUT requests (writing to cache) | < $0.01 | 10,000 writes at $0.005/1000 |
| SQS DLQ | $0.00 | Only charged on messages; should be near-zero in normal operation |
| CloudWatch Logs (LETS processor) | < $0.50 | Standard Lambda logging |
| **Total additional** | **< $2/month** | At current traffic levels |

**Scaling cost model:** At 10x traffic (tens of thousands of requests/day), costs remain under $10/month. The biggest variable is Firehose delivery frequency -- Firehose batches logs into 5-minute windows, so the Lambda is invoked once per batch interval (not once per request). At 100x traffic, the Lambda invocation count still stays proportional to the number of Firehose delivery batches, not the number of user requests.

### 4.2 Throttling and Concurrency

**Risk: S3 event notification bursts**

S3 event notifications are delivered at least once. During batch deliveries (e.g., Firehose delivers a batch every 5 minutes), multiple events can fire simultaneously.

**Mitigation:**
- Reserved concurrency of 5 caps parallel executions. S3 events that exceed concurrency are retried by S3 (up to the S3 retry policy).
- The LETS processor is not latency-sensitive -- a few seconds of queuing is acceptable.
- If concurrency proves too low, increase to 10 or 20. Each concurrent execution uses 256 MB against your account's regional Lambda concurrency pool.

**Risk: Lambda concurrency pool exhaustion**

The account has a default regional concurrency limit of 1,000. Current allocation:
- User Lambda: 50 reserved
- Admin Lambda: 10 reserved
- LETS processor: 5 reserved
- **Remaining unreserved pool:** 935

No risk of pool exhaustion at current scale.

### 4.3 Error Handling

**At-least-once delivery:** S3 event notifications guarantee at-least-once delivery, meaning the LETS processor must be **idempotent**. Processing the same log file twice should produce the same result without corruption.

**Error handling strategy:**

| Failure Mode | Handling |
|-------------|---------|
| Lambda invocation failure (crash, OOM) | S3 retries up to 2 times with exponential backoff |
| Log file parsing error (corrupt/unexpected format) | Log the error, write to DLQ, continue processing other files |
| Cache service write failure | Retry with exponential backoff within the Lambda; if still failing, the event goes to DLQ |
| Timeout (>60s) | Increase memory to speed up processing, or increase timeout. DLQ captures the event for replay |
| Duplicate events (at-least-once) | Use deterministic cache keys (derived from source S3 key + timestamp) so re-processing overwrites identical data |

**Dead Letter Queue (DLQ):**

Create an SQS queue `sgraph-send-lets-dlq` to capture failed events:

| Setting | Value |
|---------|-------|
| Queue name | `sgraph-send-lets-dlq` |
| Visibility timeout | 300 seconds |
| Message retention | 14 days |
| Encryption | SSE-SQS |

The admin Lambda can expose a `/admin/lets/dlq-status` endpoint that reads the DLQ message count, allowing operators to see if processing failures are accumulating.

### 4.4 Monitoring

Add these CloudWatch alarms for the LETS processor:

| Alarm | Metric | Threshold | Action |
|-------|--------|-----------|--------|
| LETS processor errors | `Errors` > 0 for 5 minutes | Warning | Check DLQ |
| LETS processor throttled | `Throttles` > 0 for 5 minutes | Warning | Increase concurrency |
| LETS DLQ messages | `ApproximateNumberOfMessagesVisible` > 0 | Warning | Investigate failed events |
| LETS processor duration | `Duration` p95 > 30s | Warning | Consider memory increase |

### 4.5 Circular Trigger Prevention

**Critical:** The LETS processor reads from log buckets and writes to the **cache service bucket**. The cache service bucket must NOT have an S3 event notification pointing back to the LETS processor, or we create an infinite loop.

Current design is safe because:
- Log buckets (CF logs, S3 access logs) trigger the LETS processor
- LETS processor writes to the cache service bucket
- The cache service bucket has NO S3 event notification configured

This must be enforced as a rule: **never add an S3 event trigger on the cache service bucket pointing to the LETS processor.**

### 4.6 Log Retention and Cleanup

Raw log files in the source buckets accumulate over time. Configure lifecycle policies:

| Bucket | Rule | Transition | Expiration |
|--------|------|------------|------------|
| CF Logs bucket | Move to IA after 30 days | S3 Standard-IA at 30 days | Delete at 90 days |
| S3 Access Logs bucket | Move to IA after 30 days | S3 Standard-IA at 30 days | Delete at 90 days |

The curated data in the cache service bucket follows the cache service's own retention policy (temporal + latest). Raw logs are disposable once transformed.

---

## 5. Recommendations

### R1: Start with CloudFront Logs Only (Phase 1)

The CloudFront real-time logs are the highest-value source for the SA dashboard (traffic patterns, request rates, response codes, popular endpoints). Start the LETS pipeline with CloudFront logs only.

**Rationale:**
- CloudFront logs are already structured (Firehose delivers consistent format)
- CloudFront logs have the clearest transformation path (parse fields, count requests, aggregate by path/status)
- The CloudFront log viewer (Task #13) is a P2 deliverable that depends on this pipeline
- S3 access logs are lower priority -- they capture internal Lambda-to-S3 operations, not user-facing traffic

**Sequence:**
1. Deploy LETS processor with CloudFront log trigger only
2. Dev implements the CloudFront log parser and transformer
3. Validate end-to-end: Firehose -> S3 -> LETS Lambda -> cache service -> admin UI
4. Add S3 access log triggers once the CloudFront pipeline is proven

### R2: Single Lambda, Multiple Parsers

One Lambda function (`sgraph-send-lets-processor`) handles all log sources. It inspects the S3 event to determine which bucket and prefix triggered it, then dispatches to the appropriate parser:

```
Event received
    |
    v
Inspect event.Records[0].s3.bucket.name + object.key
    |
    +-- cloudfront-realtime/ prefix --> CloudFront log parser
    +-- transfers-bucket/ prefix   --> S3 access log parser (transfers)
    +-- cache-bucket/ prefix       --> S3 access log parser (cache)
```

This keeps infrastructure simple (one Lambda, one DLQ, one set of IAM permissions) while supporting multiple log formats.

### R3: Use the Existing Deploy Pattern

The LETS processor Lambda should follow the same deployment pattern as the User and Admin Lambdas:

- Package with `Deploy__Service` from osbot-aws
- Deploy via GitHub Actions (`ci-pipeline.yml`)
- Use the same Python 3.12 arm64 runtime
- Dependencies: `osbot-utils`, `osbot-aws`, `memory-fs`, `mgraph-ai-service-cache-client`

This means adding a third deployment job to the CI pipeline: `aws-deploy-lambda__lets-processor`.

### R4: CI/CD Pipeline Changes

Add to `ci-pipeline.yml`:

1. **New deployment job:** `aws-deploy-lambda__lets-processor` (parallel with user and admin deploys)
2. **New GitHub Actions secrets:**

| Secret Name | Purpose |
|-------------|---------|
| `SGRAPH_SEND__LETS_PROCESSOR__FUNCTION_NAME` | LETS processor Lambda function name |
| `SGRAPH_SEND__CF_LOGS_BUCKET` | CloudFront logs S3 bucket name |
| `SGRAPH_SEND__S3_ACCESS_LOGS_BUCKET` | S3 access logs bucket name |

3. **Env vars passed to LETS Lambda at deploy time:** `CACHE__SERVICE__BUCKET_NAME`, `URL__TARGET_SERVER__CACHE_SERVICE`

### R5: Defer SQS Buffering (Option B) Until Needed

The direct S3-to-Lambda trigger (Option A) is simpler and sufficient at current traffic. Adding SQS between S3 and Lambda (Option B) provides batching, better retry control, and message deduplication -- but adds infrastructure complexity.

**When to switch to Option B:**
- If DLQ messages accumulate regularly (indicating retry failures)
- If traffic grows to where Lambda concurrency is consistently saturated
- If we need to batch multiple S3 events into a single Lambda invocation for efficiency

For the Explorer team, simplicity wins. Option A is the right choice now.

### R6: Observability Pipeline Env Vars -- Complete This Week

The observability pipeline deployment checklist (`v0.3.8`) is fully documented and requires only AWS Console actions to activate. This should be completed before the LETS processor work begins, because:

- The metrics endpoints provide the "collected data" half of the SA pipeline
- The LETS processor provides the "event data" half
- Both need to be working for the SA dashboard to have meaningful data

**Action:** Set the 5 required env vars on the admin Lambda and attach the CloudWatch IAM policy. Verify with the smoke test commands in the v0.3.8 checklist.

---

## 6. Implementation Sequence

| Step | Action | Owner | Dependency | ETA |
|------|--------|-------|------------|-----|
| 1 | Apply observability pipeline env vars + IAM (v0.3.8 checklist) | DevOps | None | Immediate |
| 2 | Create LETS processor Lambda function in AWS | DevOps | None | 1 hour |
| 3 | Create SQS DLQ (`sgraph-send-lets-dlq`) | DevOps | None | 15 min |
| 4 | Create IAM execution role with permissions (Section 3.6) | DevOps | Step 2 | 30 min |
| 5 | Add Lambda resource-based policy for S3 invocation | DevOps | Step 2 | 15 min |
| 6 | Configure S3 event notification on CF logs bucket (CloudFront only -- R1) | DevOps | Steps 2, 4, 5 | 30 min |
| 7 | Dev implements LETS processor code (CloudFront log parser) | Dev | Steps 2-6 | Dev task |
| 8 | Add LETS processor to CI/CD pipeline | DevOps | Step 7 merged | 1 hour |
| 9 | Add S3 lifecycle policies to log buckets | DevOps | None (independent) | 30 min |
| 10 | Add CloudWatch alarms for LETS processor | DevOps | Step 2 | 30 min |
| 11 | Configure S3 event notification for S3 access logs (Phase 2) | DevOps | Step 7 validated | After Phase 1 proven |

**Total DevOps effort:** ~4 hours of infrastructure work (Steps 1-6, 8-10), spread across the LETS implementation cycle.

---

## 7. Cross-References

| Topic | Document |
|-------|----------|
| AWS observability setup (all services live) | `team/humans/dinis_cruz/briefs/02/15/aws-setup/v0.3.6__devops__aws-observability-setup-guide.md` |
| Architect's observability pipeline proposal | `team/roles/architect/reviews/26-02-15/v0.3.6__implementation-proposal__observability-pipeline.md` |
| Observability deployment checklist (metrics env vars) | `team/roles/devops/reviews/26-02-15/v0.3.8__deployment-checklist__observability-pipeline.md` |
| Daily brief (SA Deep Dive section) | `team/humans/dinis_cruz/briefs/02/15/v0.3.10__daily-brief__explorer-team-15-feb-2026.md` |
| Master index (task #15 assignment) | `team/roles/librarian/reviews/26-02-15/v0.3.10__master-index__daily-brief-15-feb.md` |
| Previous DevOps brief response (14 Feb) | `team/roles/devops/reviews/26-02-14/v0.3.0__response-to-daily-brief__14-feb.md` |
| Lambda env vars fix | `team/roles/devops/reviews/26-02-14/v0.2.41__fix__lambda-env-vars-for-deploy.md` |
| Admin config (existing env var pattern) | `sgraph_ai_app_send/lambda__admin/admin__config.py` |

---

*DevOps -- Explorer Team Brief Response, 15 Feb 2026*
*S3-triggered Lambda plan for LETS pipeline, observability carryover, cost analysis, implementation sequence*
