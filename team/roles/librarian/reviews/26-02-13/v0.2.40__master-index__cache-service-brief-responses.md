# v0.2.40 â€” Master Index: Cache Service Architecture Brief & Responses (13 Feb 2026)

**Version:** v0.2.40
**Date:** 2026-02-13
**Role:** Librarian
**Context:** Dinis Cruz committed a cache service architecture brief (commit `3974317`) with three reference documents, superseding the "build from scratch" approach from the v0.2.33 Architect and Dev reviews. This index catalogues the brief, all supporting documentation, and the role responses being produced in parallel.
**Previous index:** [`v0.2.24__master-index__daily-brief-responses-13-feb.md`](v0.2.24__master-index__daily-brief-responses-13-feb.md)

---

## Read This First

The central message of this brief is a **strategic pivot**: the MGraph-AI Cache Service already exists as a production-ready PyPI package. SGraph Send does not need to build a custom cache service from scratch. The implementation effort drops from 25-35 hours to approximately 8-12 hours for a thin wrapper + LETS aggregation logic.

**The gating decision (D034):** Run the cache service in-process (`IN_MEMORY` mode) inside the SGraph Send Lambda, sharing the same S3 bucket with prefix separation (`user/` for transfers, `admin/` for cache data). Extraction to a separate Lambda is a future configuration change (flip `IN_MEMORY` to `REMOTE`).

**What this supersedes:** The v0.2.33 Architect review's `Cache__Service__Send` design and the v0.2.33 Dev review's 25-35 hour implementation plan. The data models and schemas from those reviews remain valid -- only the implementation vehicle changes.

**New dependency library:** `library/dependencies/cache-service/` -- a new directory housing the MGraph-AI Cache Service documentation for agent reference.

---

## 1. Source Brief

| Field | Value |
|---|---|
| **Author** | Dinis Cruz |
| **File** | [`v0.2.40__brief__cache-service-architecture.md`](../../../../humans/dinis_cruz/briefs/02/13/v0.2.40__brief__cache-service-architecture.md) |
| **Commit** | `3974317` |
| **Scope** | Cache service integration architecture for SGraph Send |
| **Key directive** | Use existing MGraph-AI Cache Service (`mgraph-ai-service-cache` + `mgraph-ai-service-cache-client` from PyPI) instead of building custom infrastructure |

### Brief Summary

The brief establishes:

1. **Integration model**: `IN_MEMORY` execution mode -- the cache service runs as a FastAPI app inside the same Lambda process via `TestClient`, zero network latency
2. **S3 architecture**: Same bucket, prefix-separated -- `user/` for transfer storage, `admin/` for cache data (analytics, tokens, costs, transfer events)
3. **Wrapper pattern**: `Send__Cache__Client` modelled on `Html_Cache__Client` -- thin `Type_Safe` wrapper with `@type_safe` decorators, domain-specific methods delegating to `Cache__Service__Client`
4. **Strategy mapping**: Analytics raw events use `TEMPORAL`, aggregations and token metadata use `KEY_BASED`, token usage events use child data, pulse uses `TEMPORAL_LATEST`
5. **Revised estimate**: Phase 1 (wrapper + raw events + pulse + tokens) in 4-6 hours, Phase 2 (LETS aggregations + admin endpoints) in 4-6 hours
6. **What remains custom**: Only the LETS aggregation pipeline (~200-300 lines) -- everything else delegates to the cache service

---

## 2. Reference Documentation (New)

Three reference documents were committed alongside the brief in commit `3974317`, establishing a new dependency documentation area at `library/dependencies/cache-service/`.

| # | Document | Location | Lines | What It Covers |
|---|----------|----------|-------|----------------|
| 1 | **Cache Service LLM Brief** | [`v0.5.68__cache-service__llm-brief.md`](../../../../library/dependencies/cache-service/v0.5.68__cache-service__llm-brief.md) | 968 | Service architecture, FastAPI structure, 5 storage strategies (direct, temporal, temporal_latest, temporal_versioned, key_based), all API endpoints, namespace isolation, storage backend options (S3, Memory, Local Disk, SQLite, ZIP), all enums and schemas |
| 2 | **Cache Client How-To-Use** | [`v0.6.0__v0.10.1__cache_service__client__how_to_use.md`](../../../../library/dependencies/cache-service/v0.6.0__v0.10.1__cache_service__client__how_to_use.md) | 2249 | Python client library (v0.10.1), three execution modes (REMOTE, IN_MEMORY, LOCAL_SERVER), store/retrieve/data operations, hierarchical child data, namespace management, real-world examples, testing patterns, best practices, troubleshooting |
| 3 | **Html_Cache__Client (reference implementation)** | [`Html_Cache__Client.py`](../../../../humans/dinis_cruz/code-example/cache-service/Html_Cache__Client.py) | 344 | The exact wrapper pattern for `Send__Cache__Client` -- thin `Type_Safe` class with `Cache__Service__Client` + `Cache__Hash__Generator`, entry CRUD, child data operations, cache hash lookups, namespace listing, health check. Uses `@type_safe` decorators on every method |

### Reading Order for Implementers

1. **Start with the brief** -- understand the integration architecture and strategy mapping
2. **Read the client how-to-use** -- learn the `IN_MEMORY` mode, store/retrieve patterns, and testing approach
3. **Study `Html_Cache__Client.py`** -- this is the template for `Send__Cache__Client`
4. **Reference the LLM brief** -- for API endpoint details and strategy specifics as needed during implementation

---

## 3. Additional Brief (Previously Uncatalogued)

| Field | Value |
|---|---|
| **Author** | Dinis Cruz |
| **File** | [`v0.2.33__briefs__the-briefing-workflow-voice-memo-to-agentic-execution.md`](../../../../humans/dinis_cruz/briefs/02/13/v0.2.33__briefs__the-briefing-workflow-voice-memo-to-agentic-execution.md) |
| **Scope** | Meta-documentation of the briefing workflow |
| **Summary** | Describes the end-to-end workflow from voice memo to Otter transcription to Claude refinement to polished brief to parallel agentic execution to consolidated debrief. Documents the brief/debrief symmetry, the folder structure conventions, and why the workflow works (speed, traceability, parallel multi-perspective analysis, institutional memory). |

This is a process document, not an action brief. It does not require role responses but is valuable as a reference for understanding how the team operates.

---

## 4. Role Responses (In Progress)

These role responses are being written in parallel to this index. Links will resolve once the files are committed.

| Role | Expected File | Key Focus Area |
|---|---|---|
| **Architect** | [`v0.2.40__review__cache-service-architecture-response.md`](../../architect/reviews/26-02-13/v0.2.40__review__cache-service-architecture-response.md) | Validate integration model, confirm strategy mapping, review S3 prefix architecture, assess extraction path to separate Lambda |
| **Dev** | [`v0.2.40__review__cache-service-architecture-response.md`](../../dev/reviews/26-02-13/v0.2.40__review__cache-service-architecture-response.md) | Implementation plan for `Send__Cache__Client`, dependency integration, revised task breakdown, testing approach using `Storage_FS__Memory` |
| **Conductor** | [`v0.2.40__review__cache-service-architecture-response.md`](../../conductor/reviews/26-02-13/v0.2.40__review__cache-service-architecture-response.md) | Updated execution sequencing, revised estimates, coordination matrix, impact on existing sprint plan |
| **AppSec** | [`v0.2.40__review__cache-service-architecture-response.md`](../../appsec/reviews/26-02-13/v0.2.40__review__cache-service-architecture-response.md) | Security review of in-process cache service, S3 prefix isolation assessment, new dependency supply chain analysis, data boundary verification |

---

## 5. Decision Updates

### Updated Decisions (D024-D027)

These decisions were originally logged in the v0.2.33 Historian review. The cache service brief updates their implementation approach while preserving the architectural intent.

| ID | Original (v0.2.33) | Updated (v0.2.40) | Impact |
|----|----|----|---|
| **D024** | Cache service as layer on Memory-FS; build custom `Cache__Service__Send` | Use existing MGraph-AI Cache Service via `Cache__Service__Client` in `IN_MEMORY` mode | Eliminates 20+ hours of custom infrastructure work |
| **D025** | Reuse `Cache__Hash__Generator`; import and wire manually | Already integrated in cache service -- configure `json_field_path` for field-level hashing | No manual wiring needed |
| **D026** | Namespace separation via custom directory creation | First-class cache service feature -- namespace parameter on every API call | Simpler, tested implementation |
| **D027** | On-demand LETS pipeline; custom implementation on `Storage_FS` | Custom implementation on top of cache service API -- storage handled, compute logic is ours | Same logic, better foundation |

Decisions **D028-D030** (human-friendly tokens, cost namespace, GA removal gate) remain unchanged.

### New Decision

| ID | Decision | Recommendation | Status |
|----|----------|----------------|--------|
| **D034** | Cache service runs in-process (`IN_MEMORY` mode) with `s3_prefix="admin/"`. Transfer service gets `s3_prefix="user/"`. Prefixes mirror Lambda architecture (`lambda__admin`, `lambda__user`). Extraction to separate Lambda is a configuration change (flip to `REMOTE`). | **APPROVE** | Pending role review |

---

## 6. Risk Register Updates

### Superseded / Resolved

| Risk | Previous Status | New Status | Notes |
|------|------|------|-------|
| **RF-15**: Cache service schema delay | Open (v0.2.33) | **SUPERSEDED** | No custom schema needed -- cache service already exists on PyPI |
| **RF-23**: Zero analytics (P0) | Open (v0.2.33) | **Faster to resolve** | Phase 1 delivers raw events + pulse in one session (~4-6 hours) |

### Updated

| Risk | Previous Status | New Status | Notes |
|------|------|------|-------|
| **RF-18**: LETS aggregation correctness | Open | **Still open** | This is the remaining custom logic (~200-300 lines) -- needs tests |
| **RF-21**: Scope expansion (5 cache consumers) | Medium | **Reduced** | Cache service handles storage complexity; wrapper is thin |

### New Risks

| Risk | Severity | Mitigation |
|------|----------|------------|
| Cache service client as new Lambda dependency | Medium | `IN_MEMORY` mode means no network dependency. Both packages on PyPI: `mgraph-ai-service-cache` and `mgraph-ai-service-cache-client`. Add to `APP__SEND__USER__LAMBDA_DEPENDENCIES` |
| Lambda package size increase from cache service packages | Low | Cache service client is lightweight. Monitor cold start times |
| Shared S3 bucket: cache operations could interfere with transfer operations | Low | Prefix isolation (`admin/` vs `user/`). Cache service never writes outside its prefix. Mirrors Lambda boundary |

---

## 7. New Dependency: MGraph-AI Cache Service

Two new PyPI packages are being introduced as dependencies:

| Package | PyPI | Purpose |
|---------|------|---------|
| `mgraph-ai-service-cache` | [mgraph-ai-service-cache](https://pypi.org/project/mgraph-ai-service-cache/) | Cache service application (FastAPI, runs in-process) |
| `mgraph-ai-service-cache-client` | [mgraph-ai-service-cache-client](https://pypi.org/project/mgraph-ai-service-cache-client/) | Python client library with `Cache__Service__Client` |

These share the same foundation as SGraph Send: `Type_Safe`, `Storage_FS` / `Memory_FS`, `osbot-utils`, `osbot-fast-api`. The cache service is already in production at `cache.dev.mgraph.ai`.

**Documentation location:** `library/dependencies/cache-service/` (new directory, two files).

---

## 8. Cross-Reference: What This Supersedes vs. What It Preserves

### Superseded (do not implement as originally described)

| Document | Element Superseded |
|---|---|
| Architect v0.2.33 review | Custom `Cache__Service__Send` class design, custom `Storage_FS` path generation, custom hash-to-ID resolution with sharded directories |
| Dev v0.2.33 review | 25-35 hour implementation plan, custom hash resolution, custom temporal write, custom namespace setup |

### Preserved (still valid, carry forward)

| Document | Element Preserved |
|---|---|
| Architect v0.2.33 review | 4 namespaces (analytics, tokens, costs, transfers), LETS pipeline design, human-friendly token names via hash, token lifecycle, dual-write for transfer events, same S3 bucket with prefix separation, all `Type_Safe` schemas |
| Dev v0.2.33 review | Data model definitions, testing strategy (in-memory stack), API endpoint designs |

---

## 9. Documents Produced This Session

| File | Role | Type | Status |
|---|---|---|---|
| `team/humans/dinis_cruz/briefs/02/13/v0.2.40__brief__cache-service-architecture.md` | Human (Dinis Cruz) | Architecture brief | Committed (`3974317`) |
| `library/dependencies/cache-service/v0.5.68__cache-service__llm-brief.md` | Reference | Dependency docs | Committed (`3974317`) |
| `library/dependencies/cache-service/v0.6.0__v0.10.1__cache_service__client__how_to_use.md` | Reference | Dependency docs | Committed (`3974317`) |
| `team/humans/dinis_cruz/code-example/cache-service/Html_Cache__Client.py` | Reference | Code example | Committed (`3974317`) |
| `team/humans/dinis_cruz/briefs/02/13/v0.2.33__briefs__the-briefing-workflow-voice-memo-to-agentic-execution.md` | Human (Dinis Cruz) | Process documentation | Previously committed |
| `team/roles/architect/reviews/26-02-13/v0.2.40__review__cache-service-architecture-response.md` | Architect | Brief response | In progress |
| `team/roles/dev/reviews/26-02-13/v0.2.40__review__cache-service-architecture-response.md` | Dev | Brief response | In progress |
| `team/roles/conductor/reviews/26-02-13/v0.2.40__review__cache-service-architecture-response.md` | Conductor | Brief response | In progress |
| `team/roles/appsec/reviews/26-02-13/v0.2.40__review__cache-service-architecture-response.md` | AppSec | Brief response | In progress |
| `team/roles/librarian/reviews/26-02-13/v0.2.40__master-index__cache-service-brief-responses.md` | Librarian | **This file (read first)** | Complete |

---

## 10. Chronological Index: All Master Indexes (13 Feb 2026)

| # | Index | Scope |
|---|---|---|
| 1 | [`v0.2.24__master-index__daily-brief-responses-13-feb.md`](v0.2.24__master-index__daily-brief-responses-13-feb.md) | Daily brief #1 responses (hash-fragment URLs, text input, observability, i18n, themes) -- 10 roles |
| 2 | **`v0.2.40__master-index__cache-service-brief-responses.md`** | **Cache service architecture brief + responses (this file)** -- 4 roles + reference docs |

---

*Start here. Read the brief for the architecture decision, then follow up with individual role responses as they complete. The key takeaway: the cache service already exists -- implementation is a wrapper, not a build.*
