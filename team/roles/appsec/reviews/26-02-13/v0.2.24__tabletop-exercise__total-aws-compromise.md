# v0.2.24 — Tabletop Exercise: Total AWS Compromise

**Role:** AppSec (CISO function)
**Date:** 2026-02-13
**Version:** v0.2.24
**Type:** Tabletop Exercise Design
**Source:** Incident Handling Series — Document 5 (Football Team)

---

## Overview

This tabletop exercise simulates a total AWS compromise: an attacker gains full access to the SGraph Send AWS account — Lambda functions, S3 buckets, CloudFront, IAM, CloudWatch, and all related services. The exercise tests the team's incident response processes, role activation, and the P3-as-P1 methodology from the incident handling series.

**Key question:** If our entire AWS infrastructure is compromised, what can the attacker learn? The answer — by design — should be: encrypted blobs they cannot decrypt, hashed IPs they cannot reverse, and metadata that does not identify file contents.

---

## Scenario

### The Setup

**Date:** Monday morning. The DevOps role discovers unusual activity in the AWS account during a routine pipeline check.

**Evidence discovered:**
1. An unfamiliar IAM user was created 48 hours ago with `AdministratorAccess`
2. The new IAM user has accessed: S3 (ListBuckets, GetObject on the transfers bucket), Lambda (GetFunction on both functions), CloudWatch (GetLogEvents), and Secrets Manager (GetSecretValue)
3. The Lambda function code for the user-facing service was modified 24 hours ago — a `console.log(request.body)` was added to the upload handler
4. CloudFront access logs show the modified Lambda has served ~200 requests since the change
5. The admin bearer token was retrieved from Secrets Manager

### What the attacker has (in this scenario)

- Full read access to all S3 objects (encrypted transfer blobs, metadata)
- Full read access to all CloudWatch logs (including any that captured request bodies)
- The admin bearer token
- The ability to modify Lambda code (which they exercised)
- CloudFront access logs with IP addresses
- IAM credentials with full account access

### What the attacker does NOT have

- The decryption keys (these never reach the server — they exist only in sender/recipient browsers)
- The original file names (never sent to the server)
- The plaintext file contents (encrypted client-side before upload — unless the `console.log` captured them)

### The critical question

The `console.log(request.body)` addition means that for the ~200 requests served by the modified Lambda, the upload request body may have been logged to CloudWatch. **Does this break the zero-knowledge guarantee?**

Answer: **It depends on the upload architecture.** If the upload flow uses pre-signed S3 URLs (the file goes directly to S3, not through Lambda), then `console.log(request.body)` captures only the metadata request, not the file content. If the upload goes through Lambda as a proxy, the encrypted bytes pass through Lambda — but they are already encrypted. The `console.log` would log encrypted bytes, not plaintext.

**However:** The question "how come this was possible?" is the real exercise. The server should never be modifiable in a way that could capture plaintext — even if current architecture prevents it.

---

## Exercise Structure

### Phase 1: Declaration (5 minutes)

**Conductor activates.**

- Declare the incident in Issues FS
- Assign severity: **P1** (total infrastructure compromise)
- Activate all roles

**Discussion questions:**
- Who do we activate first?
- What is the initial scope assessment?
- What is our hypothesis?

### Phase 2: Situate (10 minutes)

**All roles briefed on what is known.**

**Each role assesses from their perspective:**

| Role | Initial Assessment Question |
|------|---------------------------|
| **AppSec** | Has the zero-knowledge guarantee been broken? Trace the upload data flow — does encrypted content pass through Lambda? |
| **Architect** | Is the architecture correct? Should Lambda ever see file content, even encrypted? What boundary was crossed? |
| **Dev** | What code path was modified? What would `console.log(request.body)` capture? Write a test that proves what was logged. |
| **DevOps** | What is the blast radius? Which services were accessed? What logs exist? Preserve all evidence before remediation. |
| **DPO** | Was personal data exposed? Start the 72-hour ICO notification clock. What data categories are affected? |
| **GRC** | Was this risk on the register? What was the accepted risk level for IAM security? |
| **QA** | What tests should have detected the code modification? Why did CI not catch it? |
| **Cartographer** | Map the blast radius. Which components were accessed? Which trust boundaries were crossed? |
| **Historian** | Start the timeline. Record every decision made from this point. |
| **Librarian** | Find relevant runbooks. Is there an incident response runbook? Is it current? |
| **Journalist** | Prepare multi-audience communications. Do not publish until authorised by Conductor. |
| **Advocate** | Assess user impact. How many users transferred files through the modified Lambda? What was their experience? |
| **Sherpa** | Check user trails. Were any users affected during the 24-hour window? Did behaviour change? |
| **Ambassador** | Monitor external channels. Is there any public awareness? Prepare positioning strategy. |

### Phase 3: Investigate (15 minutes)

**Parallel investigation tracks:**

**Track A — Technical (Dev + AppSec + DevOps):**
1. Confirm the upload architecture: does file content pass through Lambda, or does the client upload directly to S3 via pre-signed URL?
2. Read the CloudWatch logs for the modified Lambda. What was actually logged by `console.log(request.body)`?
3. Determine: is it encrypted bytes or plaintext? (Answer should be: at worst, encrypted bytes, because encryption happens client-side before any upload)
4. Verify the admin bearer token was not used to create transfers or access admin endpoints

**Track B — Regulatory (DPO + GRC):**
1. Classify the data exposed: CloudFront logs contain IP addresses (personal data under UK GDPR)
2. Assess: does this meet the ICO notification threshold?
3. Draft the ICO notification (even if the decision is "probably not required" — practise the process)
4. Assess: do affected users need to be notified?

**Track C — Communications (Journalist + Ambassador + Conductor):**
1. Draft internal stakeholder update
2. Draft user notification (if needed)
3. Draft public statement (if needed)
4. Determine: who needs to know what, and when?

### Phase 4: Simulate the Fix (10 minutes)

**"Simulate before acting."**

Before any production action, model the response:

1. **Containment:** Revoke the attacker's IAM user. Rotate all credentials (admin token, AWS access keys). Redeploy the original Lambda code. Verify the restored code.
2. **Evidence preservation:** Export all CloudWatch logs before any remediation. Take S3 bucket snapshots. Export IAM audit trail.
3. **Verification:** How do we confirm the attacker is fully removed? What if they created a second backdoor?
4. **Communication:** What do we tell users? When? Through what channel?

**Discussion questions:**
- What is the order of operations? (Preserve evidence before remediation)
- What if we revoke IAM access but the attacker has a Lambda backdoor?
- What if the attacker copied S3 data to an external location?

### Phase 5: Assess and Close (10 minutes)

**Post-incident improvements.**

Each role identifies one systemic improvement:

| Role | "How come this was possible?" |
|------|-------------------------------|
| **AppSec** | How come an IAM user could be created without detection? → MFA enforcement, CloudTrail alerting |
| **DevOps** | How come Lambda code could be modified without triggering CI? → Code signing, deployment checksums |
| **QA** | How come we had no test for "Lambda code matches expected hash"? → Add deployment integrity tests |
| **DPO** | How come we didn't know exactly where IP addresses were logged? → Complete the data map |
| **GRC** | How come IAM security was not on the risk register? → Add IAM compromise as a tracked risk |
| **Architect** | How come Lambda could even theoretically see file content? → Verify the pre-signed URL architecture |
| **Librarian** | How come there was no incident runbook? → Create one |

---

## Success Criteria

The exercise succeeds if:

1. **Every role knows what to do** when activated — no confusion about responsibilities
2. **The zero-knowledge assessment is correct** — the team correctly determines whether plaintext was exposed
3. **The DPO can draft an ICO notification** — even in simulation, the 72-hour process must work
4. **Evidence is preserved before remediation** — the team does not destroy evidence by rushing to fix
5. **"Simulate before acting" is followed** — no production action without modelling it first
6. **Every role identifies one systemic improvement** — the incident produces lasting changes
7. **Communications are audience-appropriate** — technical team, management, users, and public get different messages

---

## Links to Incident Handling Series

This exercise operationalises all 5 documents:

| Document | How It Is Applied |
|----------|-------------------|
| **P3-as-P1 Philosophy** | The exercise forces P1 rigour. Even if the zero-knowledge guarantee held (technically P3), the systemic weakness is treated as P1. |
| **Running P3-as-P1 in Practice** | The 7-step playbook (declare, assemble, situate, investigate, assess, document, close) is the exercise structure. |
| **Best Time to Fix Things** | The "one systemic improvement per role" phase captures the 5x multiplier — more improvements emerge from one simulated incident than from months of normal ops. |
| **Simulation, Communication, Escalation** | Phase 4 is entirely "simulate before acting." Phase 3 Track C is the multi-audience communication exercise. |
| **The Football Team** | This IS the practice match. The preparation spectrum moves from "unprepared" to "practised" after running this exercise. |

---

## Scheduling

- **First run:** To be scheduled by the Conductor
- **Frequency:** Monthly (per the incident handling series recommendation)
- **Duration:** ~50 minutes
- **Facilitator:** Conductor
- **Participants:** All roles

---

## Cross-References

| Document | Location |
|----------|----------|
| Incident Philosophy (P3-as-P1) | `team/humans/dinis_cruz/briefs/02/13/v0.2.23__briefs__incident-philosophy-p3-as-p1.md` |
| Incident Practice | `team/humans/dinis_cruz/briefs/02/13/v0.2.23__briefs__incident-practice-running-p3-as-p1.md` |
| Incident Payoff (5x Multiplier) | `team/humans/dinis_cruz/briefs/02/13/v0.2.23__briefs__incident-payoff-best-time-to-fix-things.md` |
| Incident Simulation & Communication | `team/humans/dinis_cruz/briefs/02/13/v0.2.23__briefs__incident-simulation-communication-escalation.md` |
| Incident Football Team | `team/humans/dinis_cruz/briefs/02/13/v0.2.23__briefs__incident-football-team-practice-and-preparation.md` |
| Command brief | `team/humans/dinis_cruz/briefs/02/13/for-claude-code/v0.2.23__command__incident-series-full-team-activation.md` |
| Reading guide | `team/roles/librarian/reviews/26-02-13/v0.2.24__reading-guide__incident-handling-series.md` |
| Git commit identity incident | `team/roles/appsec/reviews/26-02-12/` |

---

*Tabletop Exercise: Total AWS Compromise*
*Produced as Phase 4 of the full team activation command.*
