# Sherpa Action Plan: Explorer Next Steps

**Version:** v0.3.2
**Date:** 14 February 2026
**Role:** Sherpa (Explorer Team)
**Trigger:** v0.3.2 daily brief -- P3 task: "Capture read receipts and multi-upload for roadmap. Sherpa: these are UX features that the trail data would inform."
**Previous:** [v0.3.0 -- Response to Daily Brief (Admin Journey)](v0.3.0__response-to-daily-brief__14-feb.md)
**Explorer role definition:** [v0.3.2 -- Explorer Role Definition](../../../../team/humans/dinis_cruz/briefs/02/14/v0.3.2__role-definition__explorer.md)
**Trail observation plan:** [v0.2.33 -- Trail Observation Plan](../26-02-13/v0.2.33__trail-observation-plan.md)
**First-time user journey:** [v0.2.33 -- First-Time User Journey Map](../26-02-13/v0.2.33__first-time-user-journey-map-and-friction-log.md)

---

## Executive Summary

The v0.3.2 brief assigns the Sherpa two P3 tasks -- read receipts and multi-file upload UX research -- and opens a significant observation opportunity: the design agency becoming a real user. This action plan covers six workstreams: read receipts design, multi-file upload UX, the design agency observation plan, large file transfer UX, admin workflow trail continuation, and carries forward from v0.3.0.

The key insight running through all six workstreams: **we already capture more trail data than we surface**. The `Transfer__Service` records `download` events with timestamps, IP hashes, and user agents in the transfer metadata's `events` array. The `Middleware__Analytics` records every HTTP request including `file_download` event types. The `Schema__Token__Usage_Event` tracks per-token actions. The gap is not data capture -- it is data surfacing.

---

## 1. Read Receipts Design (P3)

### What Trail Data Already Exists

The server already records download events. When a recipient downloads a file, `Transfer__Service.get_download_payload()` appends an event to the transfer's `events` array:

```python
# From sgraph_ai_app_send/lambda__user/service/Transfer__Service.py (lines 126-130)
meta['download_count'] += 1
meta['events'].append(dict(action       = 'download',
                           timestamp    = datetime.now(timezone.utc).isoformat(),
                           ip_hash      = self.hash_ip(downloader_ip),
                           user_agent   = user_agent or ''))
```

The transfer metadata also tracks:
- `download_count` -- total number of downloads
- `events` array -- every lifecycle event (upload, complete, download) with timestamps

The `Middleware__Analytics` separately records each download as a `file_download` event type in `Schema__Analytics__Raw_Event`, capturing:
- `event_id`, `timestamp`, `path`, `method`, `status_code`, `duration_ms`
- `ip_hash`, `user_agent_normalised`, `content_bytes`
- `transfer_id` (currently always empty -- see gap below)

### What's Missing for Read Receipts

| Gap | Description | Severity |
|-----|-------------|----------|
| **G-RR-01** | `Middleware__Analytics` does not populate `transfer_id` on download events. The `classify_event_type` function identifies `/transfers/download` as `file_download` but does not extract the transfer ID from the path. | HIGH |
| **G-RR-02** | The `GET /transfers/info/{transfer_id}` endpoint returns `download_count` but not the individual download events (timestamps, IP hashes). The sender has no way to see *when* downloads happened. | HIGH |
| **G-RR-03** | No sender-facing UI for checking transfer status. The sender's flow ends at stage S6 (Share). There is no "My Transfers" view or status polling. | HIGH |
| **G-RR-04** | No "page opened" event distinct from "file downloaded". A recipient might open the download page, see the transfer info, but not proceed to decrypt. Currently this is only captured in `Middleware__Analytics` as a `page_view` on the download.html path -- not linked to the specific transfer. | MEDIUM |
| **G-RR-05** | No notification channel. Even if the data is surfaced, there is no push mechanism (email, webhook, polling endpoint) to alert the sender. | LOW (v0.4.x) |

### Read Receipt Data Model

The Sherpa recommends a lightweight read receipt model built on data that already exists:

**Level 1 -- Transfer Status Check (build now)**
- New endpoint: `GET /transfers/status/{transfer_id}` (or extend `/transfers/info/`)
- Returns: `download_count`, `first_download_at`, `last_download_at`, `unique_downloaders` (count of distinct `ip_hash` values)
- Authentication: requires the sender to prove ownership (e.g., by providing the token used during upload, or a sender-specific hash generated at creation time)

**Level 2 -- Download Event Log (build soon)**
- New endpoint: `GET /transfers/events/{transfer_id}`
- Returns: array of download events with `timestamp`, `ip_hash` (truncated for display), `user_agent_normalised`
- This enables the sender to see: "3 downloads -- first at 14:32, last at 16:45, from 2 different devices"

**Level 3 -- Real-time Notification (future)**
- Polling endpoint or WebSocket for live updates
- Push notification via email or webhook when a download occurs
- This is the "read receipt" in the fullest sense -- like a delivery confirmation

### UX Design for Read Receipts

**Sender perspective -- what the trail tells us to build:**

After uploading and sharing a link, the sender's natural question is "Did they get it?" (documented as Stage S7 in the [first-time user journey map](../26-02-13/v0.2.33__first-time-user-journey-map-and-friction-log.md)). The Sherpa recommends:

1. **"Check Status" link on the upload success screen.** After sharing the link, the sender can click "Check if downloaded" to poll the transfer status. Display: "Not yet downloaded" / "Downloaded 1 time (14:32 UTC)" / "Downloaded 3 times (latest: 16:45 UTC)".

2. **Transfer history in localStorage.** The *download* component already stores history in localStorage (see `send-download.js`, `saveToHistory`). The upload component should do the same -- record each sent transfer's ID, timestamp, and token. This enables a "My Sent Files" view that can poll for status.

3. **Status indicator design.** Use the same visual language as the token status badges recommended in [v0.3.0 admin journey](v0.3.0__response-to-daily-brief__14-feb.md):
   - Grey: "Pending -- not yet downloaded"
   - Green: "Delivered -- downloaded at least once"
   - Blue: "Opened multiple times"

### Trail Events to Add for Read Receipts

| Event | Trigger | Data | Why |
|-------|---------|------|-----|
| `transfer.status.checked` | Sender polls transfer status | `transfer_id`, `ip_hash`, `timestamp` | Track how often senders check on their transfers. High frequency = high anxiety = need for push notifications. |
| `transfer.download.page_opened` | Recipient loads download page (before decrypt) | `transfer_id`, `ip_hash`, `key_present`, `timestamp` | Distinguish "opened the link" from "actually downloaded". Important for read receipts -- a link was opened but the file was not decrypted. |
| `transfer.download.completed` | Recipient successfully decrypts | `transfer_id`, `ip_hash`, `timestamp` | Already captured as `download` action in events array, but should also be recorded in the analytics pipeline with `transfer_id` populated. |

### Action Items

| # | Action | Owner | Priority |
|---|--------|-------|----------|
| RR-1 | Fix `Middleware__Analytics` to extract and populate `transfer_id` from download paths | Dev | P2 |
| RR-2 | Extend `GET /transfers/info/{transfer_id}` to include `first_download_at`, `last_download_at`, `unique_downloaders` | Dev | P3 |
| RR-3 | Add sender-side localStorage history for sent transfers (mirror recipient history pattern) | Dev | P3 |
| RR-4 | Design "Check Status" UI component for sender post-upload screen | Designer | P3 |
| RR-5 | Add `transfer.download.page_opened` event capture (link download page load to specific transfer_id) | Dev | P3 |
| RR-6 | Architect: define sender authentication model for status checks (how does the sender prove they own the transfer?) | Architect | P3 |

---

## 2. Multi-File Upload UX Research (P3)

### The Design Agency Use Case

The v0.3.2 brief identifies the design agency as the primary driver for multi-file upload. Their workflow will involve:

1. **Delivering design assets** -- multiple files (PSD, AI, PNG, SVG, font files) as a cohesive set
2. **Iterating on feedback** -- sending revised versions of specific files
3. **Receiving briefs and references** -- the SGraph team sending the agency reference materials, brand guidelines, example screenshots

This is fundamentally different from the current single-file transfer model. The current flow assumes: one sender, one file, one recipient, one link. The agency workflow requires: one sender, multiple files, one or more recipients, managed as a collection.

### Trail Data That Would Inform Multi-File UX

Before building multi-file upload, the Sherpa needs to observe:

| Observation | What It Reveals | How to Capture |
|-------------|----------------|----------------|
| **Repeat sender patterns** | How often does the same `ip_hash` create multiple transfers in a session? | Correlate `transfer.complete` events by `ip_hash` within a time window |
| **Batch upload timing** | When a user sends multiple files, how much time passes between each transfer? | Time-between-steps: `transfer.complete[n]` to `transfer.encrypt.start[n+1]` |
| **Send-another usage** | How often is the "Send another" button used? | New trail event: `upload.send_another.clicked` |
| **Token usage per session** | How many transfers does a single token use in one session? | Correlate token usage events with session-level `ip_hash` grouping |
| **Download patterns for related files** | When a recipient downloads one file, do they download others from the same sender? | Correlate `download` events by `ip_hash` across transfers sharing the same sender `ip_hash` |

### Multi-File UX Models to Explore

**Model A: Sequential Single Uploads (current, enhanced)**
- Keep the current one-file-at-a-time flow
- Add a "Files Sent" panel that accumulates links as the sender creates transfers
- After each upload, the "Send another" button keeps the user in-flow
- At the end, provide a "Copy all links" action
- Lowest friction to implement; aligns with current architecture

**Model B: Batch Upload (new component)**
- Allow drag-and-drop of multiple files simultaneously
- Each file is encrypted and uploaded independently (parallel or sequential)
- Generate a "collection" -- a single link that lists all files for the recipient
- Requires a new data model: `Collection` that groups multiple `Transfer` IDs
- Higher development cost; better UX for the design agency use case

**Model C: Folder Upload (new component)**
- Allow drag-and-drop of a folder
- Preserve directory structure in the encrypted metadata
- Recipient sees a file tree and can download individual files or the whole collection
- Highest development cost; best experience for structured deliverables

### Sherpa Recommendation

**Start with Model A (enhanced sequential)** as an Explorer experiment. It requires no backend changes -- only a client-side "sent files" accumulator. Observe how the design agency uses it. If the trail data shows:
- Senders consistently upload 3+ files per session --> explore Model B
- Senders attempt to drop folders --> explore Model C
- Senders copy-paste multiple links into a single message --> confirms need for collection links

The key Explorer question is: **"Do users want to manage files as a collection, or are they fine with individual links?"** Trail data will answer this.

### Action Items

| # | Action | Owner | Priority |
|---|--------|-------|----------|
| MF-1 | Add trail event for "Send another" button usage | Dev | P3 |
| MF-2 | Implement client-side "Files Sent This Session" accumulator panel | Dev | P3 |
| MF-3 | Design "Copy All Links" UI for multi-file senders | Designer | P3 |
| MF-4 | Architect: define Collection data model (grouping multiple transfer IDs) for future Model B | Architect | P3 |
| MF-5 | Sherpa: observe design agency batch sending patterns for 2 weeks before deciding Model B vs C | Sherpa | P3 |

---

## 3. Design Agency as User: Observation Plan

### Why This Matters

The v0.3.2 brief says: "the design agency becomes a real user and a source of feedback." This is the first external professional user -- not a friend, not a community member, but a paid service provider who will use SGraph Send as part of their daily workflow. Their behaviour will reveal friction points that internal testing cannot.

### Observation Windows

| Phase | Duration | Focus |
|-------|----------|-------|
| **Onboarding** | First 48 hours | Token setup, first file send, first file receive. Do they understand the flow without human guidance? |
| **Active Use** | Weeks 1-2 | How many files per session? What file types? What sizes? Do they use combined or split-key links? |
| **Steady State** | Weeks 3+ | Is usage growing, flat, or declining? Are they switching to other tools for certain tasks? What's their retry rate? |

### Trail Signals to Watch

| Signal | What It Reveals | Threshold for Action |
|--------|----------------|---------------------|
| **Time from token receipt to first upload** | Onboarding friction. A design agency should be able to start within 5 minutes. | > 15 minutes = investigate |
| **Files per session** | Batch sending behaviour. Informs multi-file UX decision. | > 3 files/session consistently = build Model B |
| **File sizes** | Whether they hit large file limits. Design files (PSD, AI) can be 100MB-2GB. | > 100MB = priority on large file upload |
| **Upload failure rate** | Network reliability for larger files. | > 5% failure rate = priority on retry logic |
| **Time between upload and first download** | Measures the agency-to-team feedback loop. Are files being received promptly? | > 24 hours = the sharing channel may be broken |
| **Download-to-upload ratio** | Are they both sending *and* receiving, or just one direction? | Ratio skewed > 3:1 either direction = explore the dominant direction UX |
| **"Send another" click rate** | Sequential multi-file sending pattern. | > 50% of sessions = confirms batch upload need |
| **Error rate by file type** | Certain file types may cause issues (e.g., very large PSDs, files with special characters in names). | Any file type with > 10% error rate = investigate |
| **Token exhaustion speed** | How quickly they burn through their usage limit. | Exhaustion < 1 week = token limit set too low |

### Friction Points to Anticipate

Based on the [first-time user journey map](../26-02-13/v0.2.33__first-time-user-journey-map-and-friction-log.md) and the design agency context:

| ID | Anticipated Friction | Stage | Likelihood | Mitigation |
|----|---------------------|-------|------------|------------|
| DA-001 | **File size limits.** Design files (PSD, AI, INDD) routinely exceed 100MB. The current upload uses `request.body()` which loads the entire file into memory. Lambda has a 6MB payload limit for synchronous invocation, and Lambda URL has a 20MB limit for streaming. | S5 | HIGH | Must be solved before agency engagement. See Section 4. |
| DA-002 | **Multiple files per delivery.** A design review typically involves 10-20 files. Sending each individually with a separate link is untenable. | S6 | HIGH | Enhanced sequential model (Model A from Section 2) as a minimum. |
| DA-003 | **No delivery confirmation.** The agency needs to know their files were received. "Did you get the mockups I sent yesterday?" is a real conversation that will happen. | S7 | HIGH | Read receipts (Section 1) become essential. |
| DA-004 | **Version confusion.** When the agency sends a revised logo, both the old and new versions exist as separate transfers. There is no versioning concept. | S6 | MEDIUM | Naming convention in link sharing; future: collection versioning. |
| DA-005 | **Recipient needs to download multiple files.** The recipient (our team) opens 10-20 separate download links. Each requires a separate decrypt action. | R3 | MEDIUM | Future: collection download page with batch decrypt. |
| DA-006 | **Token sharing within the agency.** Multiple designers at the agency may need to send files. One token shared across the team, or individual tokens? | S2 | MEDIUM | Admin should create a team token with a generous limit. Trail data: watch for multiple distinct `ip_hash` values using the same token. |

### Data Collection Checklist (before agency onboarding)

| # | Item | Status | Owner |
|---|------|--------|-------|
| DA-C1 | Ensure `Middleware__Analytics` captures `transfer_id` on all transfer-related requests | Not done | Dev |
| DA-C2 | Create dedicated token for design agency (generous limit, descriptive name like `agency-design-partner`) | Not done | Admin |
| DA-C3 | Baseline current upload failure rate by file size bracket | Not done | Sherpa |
| DA-C4 | Establish "normal" usage patterns from internal team usage before agency starts | Not done | Sherpa |
| DA-C5 | Prepare a brief observation report template for weekly agency usage review | Not done | Sherpa |

---

## 4. Large File Transfer UX

### The User Journey for 1GB+ Files

The v0.3.2 brief identifies large files (1GB+) as the sweet spot. The user feedback was unprompted: "this becomes really useful for big file transfers." Here is what the journey looks like today for a 1GB file:

**Current state -- will it even work?**

| Stage | What Happens | Problem |
|-------|-------------|---------|
| S4: File Selection | User selects a 1GB file via drag-drop or file picker. `FileReader.readAsArrayBuffer()` loads the entire file into browser memory. | Browser memory pressure. A 1GB file requires ~1GB of heap memory. On a mobile device or a machine with limited RAM, this will cause the tab to crash. |
| S5a: Encryption | `SendCrypto.encryptFile()` (AES-256-GCM via Web Crypto API) encrypts the entire buffer. | The encrypted output is a second ~1GB buffer. Now the browser holds ~2GB in memory. Additionally, AES-GCM has a maximum plaintext size of ~64GB, so we are within limits, but the single-buffer approach is the bottleneck. |
| S5b: Upload | `ApiClient.uploadPayload()` sends the encrypted buffer via `fetch()` as a single POST body. | Lambda URL has a 20MB streaming payload limit. **A 1GB upload will fail.** Even without the Lambda limit, a single POST with no retry means any network hiccup loses the entire upload. |
| Progress | The progress bar shows "Encrypting..." (33%) then "Uploading..." (66%). | For a 1GB file, the user sees 33% for potentially minutes with no movement. Then 66% for many more minutes. No real-time byte-level progress. No time estimate. The user cannot tell if the upload is progressing or stalled. |

**Verdict:** Large file transfers (>20MB) are currently broken due to Lambda URL payload limits. Files between 20MB and ~100MB may work with Lambda URL streaming but have no progress indication. Files >100MB will cause significant browser memory pressure.

### What Users Need for Large File UX

| Need | Description | Trail Signal |
|------|-------------|-------------|
| **Real progress** | A progress bar that moves smoothly from 0% to 100%, reflecting actual bytes processed. | `upload.progress` events at 10% intervals |
| **Time estimate** | "About 3 minutes remaining" based on current upload speed. | Computed client-side from bytes/second |
| **Retry capability** | If the upload fails at 80%, resume from where it stopped, not from the beginning. | `upload.retry` event with `bytes_completed`, `bytes_total` |
| **Integrity verification** | Confirmation that the file arrived intact. SHA-256 checksum comparison. | `upload.checksum.verified` event |
| **Background upload** | The user can close the tab and come back. The upload continues in a service worker. | `upload.backgrounded` event |
| **Cancel capability** | The user can cancel a long-running upload without losing the encryption key. | `upload.cancelled` event with `bytes_completed` |

### Trail Events for Large File Observation

| Event | Trigger | Data | Purpose |
|-------|---------|------|---------|
| `upload.file.size_bracket` | File selected | `bracket` (small:<1MB, medium:1-100MB, large:100MB-1GB, xlarge:>1GB) | Segment all metrics by file size bracket |
| `upload.encrypt.duration` | Encryption complete | `file_size_bytes`, `duration_ms` | Measure encryption performance across devices |
| `upload.progress.checkpoint` | Every 10% of upload | `transfer_id`, `percent`, `bytes_sent`, `elapsed_ms` | Real progress tracking, identify stall points |
| `upload.error.size_limit` | Upload rejected due to size | `file_size_bytes`, `error_detail` | Track how often users hit size limits |
| `upload.retry.attempt` | User retries after failure | `transfer_id`, `retry_count`, `bytes_previously_sent` | Measure retry behaviour |
| `upload.abandon.large_file` | User abandons a large file upload (navigates away during upload) | `file_size_bytes`, `percent_completed`, `elapsed_ms` | Identify the pain threshold -- at what file size do users give up? |

### Friction Severity by File Size Bracket

| Bracket | Key Frictions | Current UX Rating |
|---------|--------------|-------------------|
| Small (<1MB) | Minimal. Flow is near-instant. | Good |
| Medium (1-100MB) | F010 (fake progress bar). Upload takes seconds to minutes but progress looks stuck. | Acceptable |
| Large (100MB-1GB) | Memory pressure in browser. Lambda URL limit may block upload. No progress. No retry. | Broken |
| XLarge (>1GB) | Browser will likely crash. Upload cannot complete due to infrastructure limits. | Non-functional |

### Action Items

| # | Action | Owner | Priority |
|---|--------|-------|----------|
| LF-1 | Architect: design chunked upload approach (multipart to S3 via presigned URLs) | Architect | P2 |
| LF-2 | Dev: implement streaming encryption (Web Crypto API + Streams API) to avoid 2x memory overhead | Dev | P2 |
| LF-3 | Dev: implement `XMLHttpRequest` or `ReadableStream` upload for real progress tracking | Dev | P2 |
| LF-4 | Sherpa: define file size brackets for trail segmentation | Sherpa | P3 |
| LF-5 | Sherpa: instrument "file selected" event to capture size bracket for baseline data | Sherpa | P3 |
| LF-6 | Architect: design checksum verification flow (client-side SHA-256 before encrypt, server-side SHA-256 of encrypted payload) | Architect | P3 |

---

## 5. Admin Workflow Trails (Continuing from v0.3.0)

### What Was Delivered in v0.3.0

The [v0.3.0 admin journey response](v0.3.0__response-to-daily-brief__14-feb.md) delivered:
- Full admin user journey map (stages A1-A7)
- 10 friction points identified (AF-001 through AF-010)
- Recommended telemetry events for admin actions
- Three-phase trail observation plan (baseline, pattern recognition, side effect detection)

### What Needs to Happen Next (Explorer Track)

| # | Action | Status | Notes |
|---|--------|--------|-------|
| AW-1 | Verify AF-001 (API key entry) has been solved in the admin UI | Check with Dev | This was P1 -- blocks all admin functionality |
| AW-2 | Verify AF-002 (share URL generation) is implemented | Check with Dev | P2 -- important for token distribution workflow |
| AW-3 | Observe Dinis's first real admin session and compare to predicted journey stages | Pending | Requires admin UI to be deployed |
| AW-4 | Capture baseline admin usage metrics (from v0.3.0 Phase 1 plan) | Pending | Requires admin analytics to be recording events |
| AW-5 | Add the 9 admin telemetry events defined in v0.3.0 Section 4 to the implementation backlog | Pending | Dev task |

### New Admin Trail Requirements from v0.3.2

The v0.3.2 brief introduces two new admin workflows that need trail observation:

**Token management for the design agency:**
- Admin creates a token for the agency (stage A4)
- Admin monitors agency token usage (stage A5)
- Admin may need to extend the token limit (friction point AF-006)
- New trail signal: `admin.token.usage_reviewed` -- when the admin checks a specific token's usage details

**Explorer/Villager team management:**
- The admin may need to manage tokens for different deployment environments (Explorer vs Villager)
- Trail signal: how often does the admin switch between monitoring different tokens?
- Pattern to watch: does the admin check agency token more frequently than community tokens? (indicates anxiety about external partner usage)

---

## 6. Carries Forward from v0.3.0

### Friction Points Still Open

| ID | Friction | v0.3.0 Status | Current Status | Notes |
|----|----------|--------------|----------------|-------|
| F001 | No explanation of what the service does | Pending | **Still open** | v0.3.2 brief reinforces this: "why not WeTransfer?" is the immediate user reaction. The landing page content is now P2 for Designer + Journalist. |
| F002 | Too many sharing steps | Partially resolved (hash-fragment URLs) | **Partially resolved** | Combined URL is primary. Split-key is toggle. No email/messaging sharing yet. |
| F003 | Download link not drag-and-droppable | Pending | **Still open** | Quick fix: wrap URL in `<a>` tag. |
| F005 | Generic download filename | Pending | **Resolved** | The SGMETA envelope (added in v0.1.3+) now embeds the original filename in the encrypted payload. The download component extracts it. Code in `send-download.js` line 348: `if (metadata && metadata.filename) { this.fileName = metadata.filename; }` and line 358: `this.saveFile(content, this.fileName || 'download');`. The filename is now preserved through the zero-knowledge flow. |
| F006 | URL contains version path segments | Pending | **Still open** | Technical-looking URLs reduce trust. Needs URL rewrite/redirect. More important now that the design agency will share links externally. |
| F007 | No explanation of zero-knowledge | Pending | **Still open** | Part of landing page work. |
| F008 | CTA on download page before download | Pending | **Still open** | Low effort fix. |
| F009 | No error recovery guidance | Pending | **Still open** | Important for the design agency -- they should not need to contact us for every error. |
| F010 | Progress bar not real-time | Pending | **Critical for large files** | Becomes blocking once large file support is prioritised. See Section 4. |

### Observations Still Relevant

1. **The "Why not WeTransfer?" question is now confirmed by a second user.** The v0.3.2 brief reports another user had the same reaction. This validates F001 -- the product works but does not explain itself. The Sherpa recommends the landing page address this directly: "Unlike WeTransfer, your files are encrypted in your browser. We never see your data." This is the one-line answer.

2. **The stage S7 gap (no delivery confirmation) is now a P3 feature (read receipts).** What was a missing journey stage is now on the roadmap. The Sherpa's original observation that "the sender has no visibility into whether the recipient opened the link" has been validated by the design agency use case.

3. **The trail observation infrastructure is still not implemented.** The v0.2.33 trail observation plan defined 10 readiness checklist items. None are complete. The `Middleware__Analytics` exists and records events, but:
   - `transfer_id` is not populated (G-RR-01)
   - Trail events are not linked to specific transfers
   - No aggregation pipeline exists
   - No trail observation dashboard exists

4. **localStorage history exists on the download side but not the upload side.** The download component saves history (`sgraph-send-history`). The upload component does not. This asymmetry means we can track what recipients received but not what senders sent. Adding upload-side history is low effort and high value (enables both read receipts and multi-file tracking).

### Updated Friction Priority (incorporating v0.3.2 context)

| Rank | ID | Friction | Why Priority Changed |
|------|-----|---------|---------------------|
| 1 | F001 | No explanation / "Why not WeTransfer?" | **Elevated.** Second user confirmed. Design agency will face this too. |
| 2 | F010 | Progress bar not real-time | **Elevated.** Large file sweet spot identified. Progress is critical for 100MB+ files. |
| 3 | F006 | Technical-looking URLs | **Elevated.** Design agency will share these links externally. Professional appearance matters. |
| 4 | F009 | No error recovery guidance | **Elevated.** External partner (agency) should not need to contact us for error recovery. |
| 5 | F002 | No email/messaging sharing | Unchanged. Still partially resolved. |
| 6 | F003 | Download link not draggable | Unchanged. Quick fix. |
| 7 | F007 | No zero-knowledge explanation | Unchanged. Part of landing page. |
| 8 | F008 | CTA timing on download page | Unchanged. Low effort fix. |

---

## 7. Summary: Explorer Team Priority Map

### Immediate (before design agency onboarding)

| # | Action | Owner | Dependency |
|---|--------|-------|------------|
| 1 | Fix `Middleware__Analytics` to populate `transfer_id` | Dev | None |
| 2 | Understand actual upload size limits in current Lambda URL deployment | DevOps | None |
| 3 | Create agency token with generous limit | Admin | Admin UI deployed |
| 4 | Add upload-side localStorage history | Dev | None |
| 5 | Add "Send another" trail event | Dev | None |

### Short-term (first 2 weeks of agency use)

| # | Action | Owner | Dependency |
|---|--------|-------|------------|
| 6 | Implement Level 1 read receipts (extended transfer info endpoint) | Dev | #1 |
| 7 | Implement "Files Sent This Session" accumulator panel | Dev | #4 |
| 8 | Design "Check Status" UI for sender | Designer | #6 |
| 9 | Produce first agency usage observation report | Sherpa | #1, agency using the service |
| 10 | Architect chunked upload design for large files | Architect | None |

### Medium-term (weeks 3-6)

| # | Action | Owner | Dependency |
|---|--------|-------|------------|
| 11 | Build chunked upload with real progress | Dev | #10 |
| 12 | Implement streaming encryption | Dev | #10 |
| 13 | Decide multi-file Model B vs C based on agency trail data | Sherpa | #9 |
| 14 | Implement Level 2 read receipts (event log endpoint) | Dev | #6 |
| 15 | Produce second agency observation report with friction analysis | Sherpa | #9 |

---

*Sherpa -- Explorer Team Action Plan*
*Read receipts designed, multi-file UX researched, design agency observation plan defined, large file UX mapped, admin trails continued, v0.3.0 carries forward assessed*
