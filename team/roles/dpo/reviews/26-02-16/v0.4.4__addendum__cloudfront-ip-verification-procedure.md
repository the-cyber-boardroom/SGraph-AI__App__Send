# v0.4.4 -- DPO Addendum: CloudFront IP Verification Procedure

**Version:** v0.4.4
**Date:** 2026-02-16
**Role:** DPO (Data Protection Officer)
**Context:** DPO addendum for the CloudFront log pipeline briefing pack
**Brief:** `team/humans/dinis_cruz/briefs/02/16/v0.4.4__briefs__briefing-packs-for-agents.md`
**Related:** `team/humans/dinis_cruz/briefs/02/15/aws-setup/v0.3.6__devops__aws-observability-setup-guide.md`

---

## Purpose

This document defines the DPO's requirements for the CloudFront log pipeline briefing pack. It specifies:

1. The empirical IP verification procedure -- what to check, what counts as adequate evidence
2. Lawful basis requirements for retained log data
3. Data retention guidance for the Glacier archive step

**Note:** The pipeline is not live yet. This document defines the PROCEDURE and REQUIREMENTS, not findings.

---

## 1. Empirical IP Verification Procedure

### 1.1 Why This Is Required

The AWS observability setup (v0.3.6) configures CloudFront Real-Time Logs to exclude IP-related fields (`c-ip`, `c-port`, `x-forwarded-for`, `c-ip-version`). Configuration is necessary but not sufficient for compliance. The DPO requires empirical evidence that the configuration is effective.

Reasons:

1. **Configuration drift.** AWS console changes, Terraform updates, or Firehose reconfiguration could re-add excluded fields without anyone noticing.
2. **AWS service changes.** AWS may alter field names, add new fields, or change default behaviour in service updates.
3. **Firehose transformation.** If a Firehose transformation Lambda is added later, it could introduce data not in the original log stream.
4. **Accountability obligation.** GDPR Article 5(2) requires the controller to be able to demonstrate compliance. "We configured it" is a statement of intent. "We verified it, and here is the evidence" is a demonstration of compliance.

### 1.2 What to Check

The verification procedure has three stages.

#### Stage A: Configuration Verification

Verify the Firehose delivery stream configuration matches the approved field list.

**Procedure:**

1. Retrieve the Firehose delivery stream configuration via AWS API:
   ```
   GET /api/sa/collect/firehose
   ```
   (Or via `aws firehose describe-delivery-stream`)

2. Extract the list of selected CloudFront Real-Time Log fields.

3. Confirm the following fields are **NOT** in the selected list:
   - `c-ip` (client IP address)
   - `c-port` (client port)
   - `x-forwarded-for` (forwarded IP addresses)
   - `c-ip-version` (client IP version -- IPv4/IPv6)

4. Confirm the following fields are **NOT** in the selected list (excluded by design per v0.3.6):
   - `c-country` (client country -- excluded for data minimisation)

5. Document the complete field list that IS selected and store as a compliance artefact.

**What counts as adequate evidence:** A timestamped snapshot of the Firehose field selection, stored in the cache service with temporal versioning. The snapshot must include the delivery stream name, the associated CloudFront distribution, and the complete field list.

#### Stage B: Log Data Inspection

Examine actual log data files in the S3 bucket to confirm no IP addresses are present.

**Procedure:**

1. List recent files in the CloudFront log S3 bucket.
2. Download and decompress at least 3 log files from different time periods (to account for any configuration change mid-stream).
3. Parse each file and extract the column headers / field names.
4. Verify no field contains IP address data by:
   a. Checking field names -- no field named `c-ip`, `x-forwarded-for`, or similar.
   b. Checking field values -- apply a regex pattern match for IPv4 (`\d{1,3}\.\d{1,3}\.\d{1,3}\.\d{1,3}`) and IPv6 (`::`-delimited hex) addresses across ALL field values, not just expected IP fields.
   c. Checking for any field that contains dots-and-numbers patterns that could be IP addresses stored under a different field name.

5. Document the findings: which files were examined, which fields were present, whether any IP-like values were detected.

**What counts as adequate evidence:** A document listing: (a) the files examined (S3 keys, timestamps), (b) the fields found in each file, (c) the result of the IP address pattern scan across all field values, and (d) a clear statement of whether IP addresses were or were not found. This document is a compliance artefact.

**Important:** Stage B must be performed after real traffic has generated logs. It cannot be performed against an empty bucket or synthetic test data.

#### Stage C: Ongoing Monitoring

Establish a recurring verification to detect configuration drift.

**Procedure:**

1. **Automated field check:** A scheduled Lambda (or manual API call) that runs Stage A weekly and alerts if the field list changes.
2. **Periodic log inspection:** Stage B repeated monthly (or after any infrastructure change) to confirm no new PII has appeared in the log data.
3. **Change detection:** Any modification to the Firehose delivery stream, the CloudFront Real-Time Log configuration, or the associated Lambda transformation must trigger a Stage A + Stage B verification before the change is considered complete.

**What counts as adequate evidence:** A log of verification runs with timestamps and results, stored in the cache service. Any failed verification (IP addresses detected, or field list changed) must be escalated to the DPO immediately.

### 1.3 What Does NOT Count as Adequate Evidence

- "We configured Firehose to exclude IP fields" -- this is configuration, not evidence.
- "The DevOps setup guide says IPs are excluded" -- this is documentation, not evidence.
- "AWS says Real-Time Logs respect field selection" -- this is vendor assurance, not our evidence.
- "We tested once and it was fine" -- single-point verification does not address drift.

### 1.4 Verification Output Template

The verification should produce a document with this structure:

```
# CloudFront IP Verification Report
Date: YYYY-MM-DD
Performed by: [role]

## Stage A: Configuration
Firehose stream: [name]
CloudFront distribution: [id]
Fields selected: [complete list]
IP-related fields present: YES/NO
Fields checked against exclusion list: c-ip, c-port, x-forwarded-for, c-ip-version, c-country
Result: PASS/FAIL

## Stage B: Log Data Inspection
Files examined:
  - s3://bucket/path/file1.gz (timestamp: ...)
  - s3://bucket/path/file2.gz (timestamp: ...)
  - s3://bucket/path/file3.gz (timestamp: ...)
Fields found in data: [complete list]
IPv4 pattern matches found: [count, details if any]
IPv6 pattern matches found: [count, details if any]
Result: PASS/FAIL

## Conclusion
IP addresses present in CloudFront log data: YES/NO
Verification status: COMPLIANT / NON-COMPLIANT / REQUIRES INVESTIGATION
```

---

## 2. Lawful Basis Requirements for Retained Log Data

### 2.1 What Personal Data Exists in the Logs (Post-IP-Exclusion)

Even with IP addresses excluded, CloudFront logs may contain pseudonymising data:

| Field | Personal Data Status | Rationale |
|-------|---------------------|-----------|
| `cs-user-agent` | Pseudonymising | In rare cases, a highly customised user-agent can contribute to browser fingerprinting. Alone, it is not personal data. In combination with other data points, it could be. |
| `cs-referer` | Potentially personal | If the referring URL contains personal data (e.g., a search query, a username in a URL path). Mitigated by `Referrer-Policy: no-referrer` on outbound requests, but external referrers to our site are not controlled. |
| `cs-uri-stem` + `cs-uri-query` | Not personal data | Transfer IDs are cryptographically random and not linked to individuals. Token names are shared secrets, not personal identifiers. |
| `sc-status`, `time-taken`, `sc-bytes` | Not personal data | Operational metrics with no personal data content. |

### 2.2 Lawful Basis Analysis

Even for pseudonymising data, a lawful basis must be documented.

**Recommended lawful basis:** Legitimate Interests (GDPR Article 6(1)(f))

**Legitimate Interests Assessment (LIA) outline:**

1. **Purpose:** Processing CloudFront log data (excluding IP addresses) for the purpose of: (a) ensuring network and information security (detecting attacks, anomalous traffic patterns), (b) operational monitoring (service uptime, error rates, performance), and (c) service improvement (understanding usage patterns to improve the product).

2. **Necessity:** Processing is necessary for these purposes. The specific fields retained (request path, status code, user-agent, response time, bytes transferred) are the minimum needed for effective operational monitoring. IP addresses have already been excluded as unnecessary.

3. **Balancing test:** The interests of SGraph (security, operational visibility, service quality) are balanced against data subjects' interests:
   - **Data subjects' reasonable expectations:** Users expect that a web service monitors its own traffic for security and operational purposes.
   - **Nature of the data:** Operational metadata with no direct identifiers. The most sensitive field (user-agent) is low-risk in isolation.
   - **Impact on data subjects:** Minimal. The processing does not result in decisions about individuals. No profiling occurs. No individual is identifiable from the retained data.
   - **Safeguards:** IP addresses excluded at collection point. Log data retained for a defined period only (see Section 3). Access restricted to authorised team members.

4. **Conclusion:** The balancing test favours SGraph's legitimate interests. Processing proceeds under Article 6(1)(f).

**Decision required from human:** Confirm the LIA assessment above, or instruct the DPO to produce a more formal, standalone LIA document.

### 2.3 Lawful Basis for User-Agent Strings

User-agent strings warrant specific attention because they are the closest thing to personal data in the retained logs.

**Assessment:** User-agent strings are pseudonymising at best, and typically not personal data when processed in isolation. They become potentially identifying only when combined with other data (IP address, device fingerprint, access patterns over time). Since we do not have IP addresses and do not perform fingerprinting, the combination risk is low.

**Safeguard:** User-agent strings should be aggregated during the consolidation step (Step 2 of the LETS pipeline). The consolidated data should show counts per user-agent, not per-request user-agent values. This further reduces any residual identification risk.

---

## 3. Data Retention Guidance for the Archive Step

### 3.1 Retention Tiers

The LETS pipeline has multiple data stages, each requiring its own retention period:

| Data Stage | Location | Content | Recommended Retention | Rationale |
|-----------|----------|---------|----------------------|-----------|
| **Raw logs (S3)** | Live logging bucket | Firehose output, tab-delimited, unprocessed | **7 days** | Only retained until processed. The live bucket is a queue, not an archive. |
| **Typed JSON (cache)** | Cache service, `raw/` prefix | Parsed JSON per log line | **30 days** | Working data for consolidation and troubleshooting. Superseded by consolidated data. |
| **Consolidated data (cache)** | Cache service, `consolidated/` prefix | Grouped, compressed, summarised | **90 days** | Operational analytics. Superseded by temporal aggregations. |
| **Temporal aggregations (cache)** | Cache service, `hourly/`, `daily/`, `monthly/` | Time-bucketed statistics | **Hourly: 90 days, Daily: 1 year, Monthly: indefinite** | Hourly data is detailed and loses value over time. Daily/monthly aggregations are statistical summaries with no personal data risk. |
| **Archived raw logs (Glacier)** | Archive S3 bucket, Glacier storage class | Original Firehose output, compressed | **1 year** | Retained for potential reprocessing. After 1 year, no operational need and retention becomes disproportionate. |

### 3.2 Rationale for Each Period

**Raw logs (7 days):** The sole purpose of raw logs is to be processed. Once the LETS pipeline has transformed them into typed JSON, the raw logs serve no operational purpose. A 7-day buffer accounts for pipeline delays or failures that require reprocessing.

**Typed JSON (30 days):** The typed JSON is the first transformation output. It is useful for debugging pipeline issues and verifying transformations. 30 days provides adequate debugging window.

**Consolidated data (90 days):** Consolidated data is the primary operational dataset. 90 days allows quarter-over-quarter comparison and trend identification.

**Temporal aggregations:** Aggregated data is statistical. Hourly granularity is useful for recent operational analysis (last 90 days). Daily and monthly aggregations are valuable for long-term trend analysis and have no personal data risk (individual requests are not distinguishable in aggregated counts). Monthly aggregations can be retained indefinitely as they are purely statistical.

**Archived raw logs (1 year):** Glacier archival serves two purposes: (a) reprocessing if a new metric is needed, and (b) forensic investigation if a security incident requires analysis of historical traffic patterns. 1 year balances these needs against the storage limitation principle. After 1 year, the operational value of raw log data is minimal.

### 3.3 Deletion Triggers and Procedures

For each tier, the deletion trigger and procedure must be defined.

| Data Stage | Deletion Trigger | Deletion Procedure | Verification |
|-----------|-----------------|-------------------|--------------|
| **Raw logs** | File age > 7 days AND file has been processed (processing confirmed) | S3 lifecycle policy: delete objects > 7 days in the live logging bucket | Weekly verification: count of objects > 7 days should be zero |
| **Typed JSON** | Age > 30 days | Cache service TTL: delete `raw/` entries > 30 days old | Monthly verification: no `raw/` entries older than 30 days |
| **Consolidated** | Age > 90 days | Cache service TTL: delete `consolidated/` entries > 90 days old | Monthly verification |
| **Hourly aggregations** | Age > 90 days | Cache service TTL: delete `hourly/` entries > 90 days | Monthly verification |
| **Daily aggregations** | Age > 1 year | Cache service TTL: delete `daily/` entries > 1 year | Quarterly verification |
| **Monthly aggregations** | No automatic deletion | Manual review annually: confirm no personal data risk in retained aggregations | Annual review |
| **Archived raw logs** | Age > 1 year | S3 lifecycle policy on archive bucket: delete Glacier objects > 1 year | Quarterly verification |

### 3.4 Important: Deletion Must Be Actual Deletion

For S3 objects:
- Deletion means the object is no longer accessible. For versioned buckets, all versions must be deleted.
- Glacier objects may take up to 24 hours to delete (async).
- MFA Delete should NOT be enabled on the logging bucket (it would interfere with automated lifecycle deletion).

For cache service entries:
- Deletion means the entry is removed from the cache service storage backend (Memory-FS, disk, or S3).
- If the cache service is backed by S3, the same S3 deletion considerations apply.

### 3.5 Exception: Legal Hold

If a legal hold is placed (e.g., litigation, regulatory investigation), the affected data must be preserved regardless of retention schedules. The DPO must be notified of any legal hold and will assess whether additional data protection measures are needed for the preserved data.

### 3.6 Decisions Required from Human

The retention periods above are the DPO's recommendation. The human should confirm or adjust:

1. **Raw log retention (7 days):** Is this sufficient for pipeline reprocessing needs?
2. **Glacier archive retention (1 year):** Is 1 year sufficient for forensic investigation needs, or does the security team need longer?
3. **Monthly aggregation retention (indefinite):** Is there any reason to delete monthly aggregations?
4. **S3 lifecycle policies:** Should these be implemented via S3 lifecycle rules (automated) or via the SA pipeline (manual/scheduled Lambda)?

---

## 4. Consolidated DPO Requirements for the CloudFront Pipeline

The following requirements must be met before the CloudFront log pipeline is considered DPO-approved:

| # | Requirement | Stage | Blocking? |
|---|-------------|-------|-----------|
| R1 | IP verification Stage A (configuration check) completed | Before pipeline goes live | YES |
| R2 | IP verification Stage B (log data inspection) completed | After first logs arrive | YES (for DPO sign-off) |
| R3 | IP verification Stage C (ongoing monitoring) designed | Before pipeline goes live | No (can be implemented after, but must be designed before) |
| R4 | Lawful basis documented (LIA for legitimate interests) | Before pipeline goes live | YES |
| R5 | Retention periods confirmed by human | Before archive step is implemented | YES (for archive step) |
| R6 | Deletion procedures implemented | Before pipeline goes live | No (can follow, but must be within 30 days of go-live) |
| R7 | User-agent aggregation in consolidation step verified | When consolidation is built | No |
| R8 | Referrer field review (check for personal data in referrer values) | When first logs arrive | No |

---

*This addendum is ready for inclusion in the CloudFront log pipeline briefing pack at `briefing-packs/active/cloudfront-log-pipeline/addenda/dpo.md`. The DPO will review and sign off on the pipeline when requirements R1, R2, and R4 are met.*
