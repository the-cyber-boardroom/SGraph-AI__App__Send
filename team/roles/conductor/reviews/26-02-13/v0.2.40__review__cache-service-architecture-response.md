# v0.2.40 -- Conductor Review: Cache Service Architecture Response

**Version:** v0.2.40
**Date:** 13 February 2026
**Role:** Conductor
**Type:** Brief Response + Sprint Plan Update
**Context:** Response to Dinis Cruz's cache service architecture brief (v0.2.40), which fundamentally changes the cache service implementation approach
**Inputs:**
- [Cache Service Architecture Brief](../../../../humans/dinis_cruz/briefs/02/13/v0.2.40__brief__cache-service-architecture.md)
- [Conductor v0.1.3 Session Wrap-Up](v0.2.33__review__v0.1.3-wrap-up.md)
- [Conductor Brief #2 Response (7-phase plan)](v0.2.33__response-to-daily-brief-2__13-feb.md)
- [Dev Cache Service Implementation Plan](../../../dev/reviews/26-02-13/v0.2.33__review__cache-service-implementation-plan.md)
- [Architect Server/Admin Features Review](../../../architect/reviews/26-02-13/v0.2.33__review__server-admin-features-next-sprint.md)

---

## 1. What Changed and Why It Matters

The previous plan was wrong. The Architect designed a cache service from scratch (Schemas A-D, custom hash resolution, custom temporal storage, custom namespace management). The Dev estimated 25-35 hours of implementation across 3 weeks. Both documents were thorough, well-specified, and would have produced a working system.

They were also unnecessary. The MGraph-AI Cache Service already exists as a production-ready PyPI package with exactly the capabilities we specified. The brief corrects this by replacing a "build from scratch" approach with a thin wrapper pattern.

### The Numbers

| Metric | Previous Plan | New Plan | Change |
|--------|--------------|----------|--------|
| Total estimated effort | 25-35 hours | ~8-12 hours | **-60% to -66%** |
| Phase 1 (minimum viable analytics) | 5-8 days | ~4-6 hours (one session) | **Compressed to single session** |
| Phase 2 (aggregations + admin) | 5-7 days | ~4-6 hours | **Compressed to single session** |
| Custom infrastructure code | ~800-1200 lines | ~200-300 lines (LETS pipeline only) | **-75%** |
| New dependencies | 0 | 2 PyPI packages | **New external dependency** |
| Dev's blocking questions (Q1/Q2) | Unresolved | **Resolved** | Reference code provided |

### Why This Is a Positive Deviation

The Conductor's wrap-up review (v0.2.33) flagged RF-25: "plan-to-execution ratio too high." That diagnosis was correct but incomplete. The ratio was high not just because we produced too many planning documents -- it was high because the planning assumed we had to build something that already existed.

This is the kind of error that a human stakeholder corrects. Dinis saw the Architect and Dev designing a system that duplicates work already available in the MGraph-AI ecosystem, and redirected the effort. The team's schemas, data models, and LETS pipeline design are still valid -- they describe *what* gets stored. The brief changes *how* it gets stored: through an existing service, not custom infrastructure.

The lesson for the Conductor: when the team estimates 25-35 hours for a component that "sits on top of" an existing framework (Memory-FS, osbot-utils, Storage_FS), the first question should be "does this already exist as a service?" We did not ask that question. The brief answered it anyway.

---

## 2. Impact on Sprint Timeline

### Before the Brief

```
Session N+1:  Cache service scaffolding + hash generator (Day 1-2)
Session N+2:  Temporal storage + raw event recording (Day 2-3)
Session N+3:  Pulse endpoint + 30-min aggregation (Day 3-4)
Session N+4:  Token CRUD + admin endpoints (Day 4-6)
Session N+5:  Admin UI + aggregation cascade (Day 7-10)
```

Five sessions minimum before analytics answers the two essential questions. RF-23 (zero analytics) stays open for a week or more.

### After the Brief

```
Session N+1:  Send__Cache__Client wrapper + raw event recording + pulse + token CRUD
              (~4-6 hours -- one session)

Session N+2:  LETS aggregation cascade + admin endpoints + transfer dual-write
              (~4-6 hours -- one session)

Session N+3:  Admin UI + cost tracking (future sprint)
```

**Phase 1 can complete in a single session.** This is the headline change. The two essential questions ("Is anyone using the site?" and "How many people used the site today?") can be answered by the end of the next working session, not in a week.

RF-23 (zero analytics, P0) goes from "multi-session resolution path" to "resolvable in the next session."

### What This Frees Up

With the cache service effort compressed from 25-35 hours to 8-12 hours, the sprint gains ~15-25 hours of capacity. This creates space for work that was previously queued behind the cache service:

| Previously Queued | New Status |
|-------------------|------------|
| Landing page content (Phase 6C) | Can run in parallel with Phase 2 |
| Email sharing (Phase 6D) | Unblocked after Phase 1 |
| Token gate i18n bug fix (F015) | Can ship alongside Phase 1 |
| Theme system (Phase 6E) | Reachable within this sprint |

---

## 3. Updated Priority Stack

The previous priority stack (from the v0.1.3 wrap-up) was:

```
P0  [1] Operational visibility (logging + pulse endpoint)
P0  [2] Cache service core implementation
P1  [3] Landing page content integration
P2  [4] Email sharing
P2  [5] Theme system
```

The brief collapses [1] and [2] into a single work item because the pulse endpoint is now a direct feature of Phase 1 (not a separate emergency measure). Updated stack:

```
P0  [1] Phase 1: Send__Cache__Client + raw events + pulse + token CRUD (one session)
P0  [2] Phase 2: LETS aggregations + admin endpoints + transfer dual-write (one session)
P1  [3] Landing page content integration (parallel with Phase 2)
P1  [4] Token gate i18n fix (F015) (ship alongside Phase 1)
P2  [5] Email sharing
P2  [6] Theme system
P3  [7] Phase 3: Admin UI + cost tracking (future sprint)
```

### What Moved

| Item | Previous Priority | New Priority | Why |
|------|------------------|-------------|-----|
| Operational visibility | P0 (separate) | **Merged into P0 [1]** | Pulse is built into Phase 1, not a standalone emergency |
| Cache service core | P0 (multi-session) | **P0 [1] (single session)** | Thin wrapper, not from-scratch build |
| LETS aggregations | Embedded in cache service (Day 3-4) | **P0 [2] (second session)** | Moved up because Phase 1 is faster |
| Landing page content | P1 | **P1 [3] (unchanged, but reachable sooner)** | Sprint capacity freed up |
| Token gate i18n (F015) | Not prioritised | **P1 [4]** | Small fix, high-visibility bug, ship with Phase 1 |
| Admin UI | Embedded in Phase 6B | **P3 [7] (deferred to future sprint)** | Not needed for analytics/tokens to work |

---

## 4. Risk Register Updates

### Superseded Risks

| ID | Risk | Previous Status | New Status | Notes |
|----|------|----------------|-----------|-------|
| RF-15 | Cache service schema delay blocks all Brief #2 work | CLOSED (schema delivered) | **SUPERSEDED** | The schema described what to build from scratch. The brief eliminates the from-scratch build entirely. The schema's data models remain valid as documentation of what gets stored, but the implementation path is fundamentally different. |

### Faster Resolution

| ID | Risk | Previous Status | New Status | Notes |
|----|------|----------------|-----------|-------|
| RF-23 | Zero analytics on publicly-announced product | ACTIVE (P0) | **ACTIVE (P0) -- faster resolution path** | Previous path: 5+ sessions to resolve. New path: resolvable in Session N+1 (Phase 1). Raw event recording + pulse can ship in one session. This was the Conductor's #1 concern. The brief directly addresses it. |
| RF-17 | GA removed before server-side analytics ready | MATERIALISED (accepted) | **MATERIALISED -- mitigation accelerated** | The visibility gap that GA removal created can be closed in one session instead of a week. The risk was accepted because the DPO/AppSec recommendation was unanimous. The brief makes the acceptance cost much lower. |

### Unchanged Risks

| ID | Risk | Status | Notes |
|----|------|--------|-------|
| RF-18 | LETS aggregation correctness (cascading errors) | **OPEN -- pending Phase 2** | The LETS pipeline is the only substantial custom code (~200-300 lines). This is where bugs will live. QA's test specifications (16 cache service tests) should be adapted to test through the cache service client API, not against custom Storage_FS paths. |
| RF-19 | Cache service becomes performance bottleneck | **ACCEPTED -- monitor** | IN_MEMORY mode eliminates network latency. Monitor Lambda cold start times after adding the two PyPI dependencies. |
| RF-21 | Scope expansion (5 cache consumers) | **REDUCED** | The cache service handles the complexity of storage, path generation, hash resolution, and namespace isolation. The wrapper is thin. Scope expansion risk is lower because the surface area of custom code is smaller. |
| RF-22 | No CloudFront logs flowing yet | **ACTIVE** | Unchanged. CloudFront log ingestion is still a separate workstream. The cache service brief does not address this. Application-level analytics (from Lambda) are the priority; CloudFront data is additive. |
| RF-24 | LinkedIn traffic spike hits unpolished UI | **ACTIVE -- P1** | Unchanged. Landing page content (Priority [3]) addresses this. |
| RF-25 | Plan-to-execution ratio too high | **ACTIVE -- directly addressed by this brief** | The brief is itself the correction. Less custom code = less planning overhead = faster shipping. |

### New Risks

| ID | Risk | Likelihood | Impact | Mitigation | Status |
|----|------|-----------|--------|-----------|--------|
| RF-26 | **External dependency on mgraph-ai-service-cache + client** | Medium | Medium | IN_MEMORY mode means zero network dependency. Both packages are on PyPI, maintained within the MGraph-AI ecosystem (same team, same patterns). Pin versions in requirements. The extraction path (IN_MEMORY to REMOTE) is a configuration change, not a rewrite. | **ACCEPTED** |
| RF-27 | **Lambda package size increase from two new PyPI dependencies** | Low | Low | Cache service client is lightweight. Monitor cold start times after adding. If cold starts increase beyond acceptable threshold, evaluate tree-shaking or lazy imports. | **MONITOR** |
| RF-28 | **Shared S3 bucket: cache operations could interfere with transfer operations** | Low | Medium | Prefix isolation (`admin/` vs `user/`). The cache service never writes outside its configured prefix. The prefix structure mirrors the Lambda architecture boundary. This is the same isolation pattern used in production by MGraph-AI. Test with concurrent read/write operations. | **MONITOR** |

---

## 5. Decision D034 -- APPROVE with Notes

The brief proposes D034:

> Cache service runs in-process (IN_MEMORY mode) with s3_prefix="admin/". Transfer service gets s3_prefix="user/". Prefixes mirror Lambda architecture. Extraction to separate Lambda is a configuration change.

### Recommendation: APPROVE

**Rationale:**

1. **IN_MEMORY mode eliminates the deployment complexity** of a separate cache service Lambda during the MVP phase. No new Lambda functions, no new IAM policies, no new API Gateway routes.

2. **The extraction path is clean.** Flipping IN_MEMORY to REMOTE and providing a service URL is a configuration change. The `Send__Cache__Client` wrapper, all application logic, and all tests stay identical. This is the same pattern used by `Html_Cache__Client` in the MGraph-AI HTML service.

3. **The S3 prefix mirroring** (`admin/` and `user/`) is architecturally sound. It reflects the existing Lambda split and provides a natural boundary for future extraction. When the cache service becomes its own Lambda, the S3 prefixes already match.

4. **Same S3 bucket, no new IAM** is the right trade-off for MVP. A separate bucket adds IAM complexity with no practical benefit at current scale.

### Notes for the Record

- **Version pinning is mandatory.** Both `mgraph-ai-service-cache` and `mgraph-ai-service-cache-client` must be pinned in `APP__SEND__USER__LAMBDA_DEPENDENCIES`. Unpinned dependencies in Lambda are an operational risk.
- **Cold start monitoring.** After adding the two packages, measure Lambda cold start times. If they increase by more than 500ms, investigate lazy imports.
- **The `admin/` prefix must be treated as the admin Lambda's domain.** No user Lambda code should read from or write to `admin/`. No admin code should read from or write to `user/`. This boundary must be enforced by convention now and by IAM policy when the Lambdas use separate roles.

---

## 6. Dev's Blocking Questions -- Resolution Status

The Dev's implementation plan (v0.2.33) listed 6 questions. The brief resolves the critical ones:

| # | Question | Status | Resolution |
|---|----------|--------|------------|
| Q1 | Share the reference `Cache__Service` code | **RESOLVED** | The reference is `Html_Cache__Client.py` in `team/humans/dinis_cruz/code-example/cache-service/`. This is the exact pattern for `Send__Cache__Client`. |
| Q2 | Walk through `osbot-utils` cache module | **RESOLVED** | Two reference documents provided: `library/dependencies/cache-service/v0.5.68__cache-service__llm-brief.md` (service architecture) and `v0.6.0__v0.10.1__cache_service__client__how_to_use.md` (client library). |
| Q3 | Does Storage_FS have temporal write support? | **RESOLVED** | The cache service handles all temporal writes via its `TEMPORAL` strategy. Dev does not need to implement dual-write. |
| Q4 | Middleware vs hooks for analytics? | **Unchanged** | Still needs a decision. The brief does not address this. Default: middleware (capture all HTTP requests). |
| Q5 | Admin authentication pattern? | **Unchanged** | Still needs a decision. Default: same `SGRAPH_SEND__ACCESS_TOKEN` for now. |
| Q6 | S3 layout confirmation? | **RESOLVED** | Brief specifies: `user/` prefix for transfers, `admin/` prefix for cache data, same S3 bucket. Mirrors Lambda architecture. |

**4 of 6 blocking questions are now resolved.** Q4 and Q5 are not blocking -- the Dev can proceed with sensible defaults (middleware for analytics, shared access token for admin).

The Dev can start implementation immediately.

---

## 7. Directive for the Next Session

### Session Goal

Ship Phase 1: `Send__Cache__Client` wrapper + raw analytics event recording + pulse endpoint + token CRUD. Deliver minimum viable analytics that answers the two essential questions.

### Session Plan

| # | Task | Est. | Deliverable | Dependencies |
|---|------|------|-------------|-------------|
| 1 | Read the three reference documents (LLM brief, client how-to-use, Html_Cache__Client.py) | 15 min | Understanding of the API surface | None |
| 2 | Add `mgraph-ai-service-cache` and `mgraph-ai-service-cache-client` to Lambda dependencies | 15 min | PyPI packages available in Lambda | None |
| 3 | Implement `Send__Cache__Client` scaffolding: configure `Cache__Service__Client` in IN_MEMORY mode, wire S3 backend with `admin/` prefix, set up 4 namespaces | 1-2h | Thin wrapper class with health check | Task 2 |
| 4 | Analytics raw event recording: FastAPI middleware writes one event per request via TEMPORAL strategy | 1-2h | Every request produces a JSON file in `admin/analytics/` | Task 3 |
| 5 | Pulse computation: read recent temporal files, count, return | 30-60 min | `/health/pulse` endpoint answers "is anyone using the site?" | Task 4 |
| 6 | Token CRUD: create/lookup/use/revoke via KEY_BASED + child data | 1-2h | Token system operational | Task 3 |
| 7 | Tests: all tasks above with `Storage_FS__Memory`, no mocks | Integrated | Full test coverage for Phase 1 | Tasks 3-6 |

**Total estimated: 4-6 hours.**

### What NOT to Do in the Next Session

- Do not build LETS aggregations yet. That is Phase 2. Raw events + pulse are enough for Session N+1.
- Do not build admin UI. The admin REST endpoints come in Phase 2. The UI comes in Phase 3.
- Do not refactor the Transfer Service. The 70+ TODOs are quality improvements, not blockers.
- Do not produce more than 2 role response documents. The architecture is decided. Ship code.

### Success Criteria for Session N+1

1. Raw analytics events are being written to `admin/analytics/` for every HTTP request
2. `/health/pulse` returns request counts for the last 5 minutes
3. Token create/lookup/use/revoke works through `Send__Cache__Client`
4. All tests pass with `Storage_FS__Memory` -- no mocks, no patches
5. RF-23 (zero analytics) status moves from ACTIVE to MITIGATED

---

## 8. Resource Allocation

### Can the Dev Proceed Immediately?

**Yes.** The three blockers that existed before this brief are now resolved:

| Blocker | Before | After |
|---------|--------|-------|
| No reference implementation | Dev asked for `Cache__Service` code (Q1) | `Html_Cache__Client.py` provided as reference |
| No understanding of osbot-utils cache module | Dev asked for walkthrough (Q2) | Two reference docs in `library/dependencies/cache-service/` |
| Custom temporal write implementation needed | Dev would build from scratch | Cache service handles temporal writes via TEMPORAL strategy |

The Dev has everything needed: reference code, client library documentation, service architecture documentation, and a clear pattern to follow (`Html_Cache__Client` -> `Send__Cache__Client`).

### Pairing Recommendation

The brief mentions Dinis agreed to pair on the cache service. With the effort reduced from 25-35 hours to 8-12 hours, the pairing session should focus on:

1. **First 30 minutes:** Walk through `Html_Cache__Client.py` together. Map each method to its `Send__Cache__Client` equivalent.
2. **Next 1-2 hours:** Implement `Send__Cache__Client` scaffolding with Dinis available for questions about cache service API nuances.
3. **Remaining time:** Dev works independently on analytics recording, pulse, and token CRUD. Dinis reviews at end of session.

This is a more efficient use of the pairing time than the previous plan, which would have required multi-day pairing across custom infrastructure code.

---

## 9. Gate Status Update

| Gate | Description | Previous Status | Updated Status |
|------|-------------|----------------|---------------|
| G1 | Cache service schema | CLEARED | **SUPERSEDED** | The schema described what to build from scratch. The brief replaces the build-from-scratch approach. The schema's data models remain valid as documentation. |
| G2 | Cache service implementation | OPEN (top priority) | **OPEN -- reframed** | The gate is no longer "implement custom cache service." It is now "implement Send__Cache__Client wrapper + wire raw event recording." Scope reduced by ~75%. |
| G3 | Server-side analytics minimum bar | BYPASSED (GA removed early) | **REACHABLE in Session N+1** | Phase 1 delivers raw events + pulse. The two essential questions become answerable. |
| G4 | i18n foundation | CLEARED | CLEARED (unchanged) |
| G5 | IFD compliance | STANDING | STANDING (unchanged) |
| G6 | IP masking configuration | OPEN | OPEN (unchanged -- not addressed by this brief) |
| G7 | Hash-fragment URLs | CLEARED | CLEARED (unchanged) |

---

## 10. Decisions Updated by This Brief

| # | Decision | Original | Updated | Status |
|---|----------|----------|---------|--------|
| D024 | Cache service as layer on Memory-FS | Build custom `Cache__Service__Send` | **Use existing MGraph-AI Cache Service via `Cache__Service__Client` in IN_MEMORY mode** | Updated |
| D025 | Reuse `Cache__Hash__Generator` | Import and wire manually | **Already integrated in cache service client** | Updated |
| D026 | Namespace separation | Custom directory creation | **First-class cache service feature** | Updated |
| D027 | On-demand LETS pipeline | Custom implementation on Storage_FS | **Custom implementation on top of cache service API** (storage handled, compute logic is ours) | Updated |
| D028-D030 | Human-friendly tokens, cost namespace, GA removal gate | As specified | **Unchanged** | Unchanged |
| **D034** | Cache service runs in-process (IN_MEMORY), s3_prefix mirroring | New | **APPROVED** (see Section 5) | New |

---

## 11. Summary

The cache service architecture brief is the single most impactful course correction since the project started. It converts a 25-35 hour custom build into an 8-12 hour integration. It resolves the Dev's two blocking questions. It compresses the timeline for resolving RF-23 (zero analytics) from a week to a single session. It introduces two new PyPI dependencies but eliminates ~600-900 lines of custom infrastructure code.

The original plan was overengineered. The Architect's schemas and the Dev's implementation plan were technically correct -- they would have produced a working cache service. But they solved a problem that was already solved. The brief identifies this, provides the existing solution, and redirects the team's effort toward the only custom code that matters: the LETS aggregation pipeline and the domain-specific wrapper.

**What changes:** Implementation approach, timeline, effort estimate, dependency list.
**What stays:** Data models, namespace design, LETS pipeline logic, S3 prefix structure, testing strategy.
**What to watch:** Lambda cold start times, PyPI version pinning, S3 prefix boundary enforcement.
**What to ship next:** Phase 1, one session, then assess.

---

*Next Conductor document: after Phase 1 ships (Session N+1), or if a blocker emerges during implementation.*
