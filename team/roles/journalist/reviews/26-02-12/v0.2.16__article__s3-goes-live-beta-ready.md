# v0.2.16 -- S3 Goes Live: SGraph Send Is Beta-Ready

**Version:** v0.2.16
**Date:** 12 February 2026
**Role:** Journalist
**Type:** Feature article -- S3 storage milestone and beta readiness

---

## The headline

SGraph Send's encrypted file transfers now persist in Amazon S3. The one limitation that kept the MVP from being shareable with real users -- files vanishing when Lambda cold-started -- has been eliminated. The application is ready for its first batch of beta testers: the "friendlies."

In the same session, the team ran its first security incident response, investigating a commit that appeared to come from GitHub Actions on a feature branch. The outcome: a benign git rebase, but the exercise exposed three systemic gaps in commit identity governance and kicked off a formal remediation process.

---

## 1. The problem: uploads that disappeared

When SGraph Send hit its first MVP on 11-12 February, the full transfer cycle worked. A file could be encrypted in the browser, uploaded to an AWS Lambda function, downloaded from a separate browser window, and decrypted -- with the server never seeing the original file, the file name, or the decryption key. Dinis Cruz proved this live and posted the screenshots on LinkedIn.

But there was a catch. The storage backend was in-memory. Lambda functions are ephemeral: AWS spins them up and tears them down as demand fluctuates. Every time a Lambda instance recycled, every uploaded file disappeared with it. Worse, if the download request hit a different Lambda instance than the one that received the upload, the file simply was not there.

As Dinis noted in his review: "this only works if we hit the same lambda function (which is the one that stored the file in memory), so in most cases we can upload but can't download."

For a demo, that was fine. For sharing with real users, it was a dealbreaker.

**Source:** `team/humans/dinis_cruz/briefs/02/12/v0.2.10__review-of-user-site-mvp.md`, lines 99-106

---

## 2. The blueprint: a pattern from production

Dinis did not just identify the problem. He provided the solution -- as working code.

He uploaded a reference implementation from his caching service (`team/humans/dinis_cruz/code-example/example-of-memory-fs-with-s3/`), showing the exact pattern needed: a `Storage_FS__S3` class that implements the Memory-FS interface backed by S3, and a `Cache__Config` factory class that auto-detects whether AWS credentials are available and creates the appropriate backend.

The pattern is elegant in its simplicity. `Cache__Config` checks for AWS credentials. If they exist, it returns an S3-backed storage instance. If not, it returns an in-memory instance. The application code -- the `Cache__Service` in his example -- accepts a `Storage_FS` interface and never knows or cares which backend it is talking to.

This is the Memory-FS abstraction layer doing exactly what it was designed to do: making the storage backend a deployment configuration decision, not an application code decision.

**Source:** `team/humans/dinis_cruz/code-example/example-of-memory-fs-with-s3/Cache__Config.py`

---

## 3. Building the storage layer

The Dev agent took the reference implementation and adapted it for SGraph Send. Three new files were created:

### Enum__Storage__Mode

A straightforward enum at `sgraph_ai_app_send/lambda__user/storage/Enum__Storage__Mode.py`:

```python
class Enum__Storage__Mode(str, Enum):
    MEMORY = "memory"    # In-memory (default, for dev/test)
    S3     = "s3"        # AWS S3 (production)
```

Two modes. Memory for development and testing. S3 for production. Clean.

### Storage_FS__S3

The S3 backend at `sgraph_ai_app_send/lambda__user/storage/Storage_FS__S3.py` implements the `Storage_FS` interface -- the same interface that `Storage_FS__Memory` implements. Seven methods: `file__bytes`, `file__delete`, `file__exists`, `file__json`, `file__save`, `file__str`, and `files__paths`. Plus `setup()` to initialise the S3 client and auto-create the bucket if it does not exist.

All S3 operations go through `osbot-aws` (the project's mandatory AWS abstraction layer), never through `boto3` directly. This is not an arbitrary constraint -- it ensures consistent credential handling, region resolution, and error patterns across the entire project.

### Send__Config

The factory at `sgraph_ai_app_send/lambda__user/storage/Send__Config.py` ties it together:

```python
class Send__Config(Type_Safe):
    storage_mode : Enum__Storage__Mode = None
    s3_bucket    : str                 = None

    def __init__(self, **kwargs):
        super().__init__(**kwargs)
        if self.storage_mode is None:
            self.storage_mode = self.determine_storage_mode()
        self.configure_for_storage_mode()
```

When `Send__Config` is instantiated, it auto-detects the storage mode. If AWS credentials are present (checked via `aws_config.aws_configured()` from osbot-aws), it selects S3. Otherwise, it falls back to memory. An environment variable (`SEND__STORAGE_MODE`) can override this detection.

The `create_storage_backend()` method returns a ready-to-use `Storage_FS` instance -- either `Storage_FS__Memory()` or `Storage_FS__S3(s3_bucket=...).setup()`. The caller does not need to know which.

**Source:** `sgraph_ai_app_send/lambda__user/storage/Send__Config.py`

---

## 4. The code review that improved everything

Dinis reviewed the first implementation and left seven review comments on `Send__Config` (commit `d11b57d`). Each one targeted a specific architectural concern:

**S3 bucket naming.** The initial implementation used a generic bucket name. Dinis required the format `{account_id}--{infix}--{region}` -- specifically `745506449035--sgraph-send-transfers--eu-west-2`. This pattern embeds the AWS account ID and region directly in the bucket name, making it globally unique without relying on random strings, and immediately telling you where the bucket lives when you see its name.

**No state in `__init__`.** A philosophical point about Type_Safe classes: the constructor should not perform side effects. The current implementation auto-detects and configures on instantiation, which works but is not idiomatic. Dinis flagged this as a conversation topic for the Service Registry pattern -- a more sophisticated approach to dependency wiring that osbot-utils supports.

**Use `AWS_Config` for credential detection.** The initial implementation checked for specific environment variables (`AWS_ACCESS_KEY_ID`, `AWS_SECRET_ACCESS_KEY`, etc.) to determine if AWS was available. Dinis directed it to use `aws_config.aws_configured()` from osbot-aws instead -- a single call that handles all the edge cases (IAM roles, Lambda execution environment, AWS profiles, explicit credentials).

The Dev agent applied all fixes. The result is cleaner, more idiomatic, and more maintainable than the first iteration.

---

## 5. Transfer__Service: zero application code changes

This is where the Memory-FS abstraction proved its value.

The `Transfer__Service` at `sgraph_ai_app_send/lambda__user/service/Transfer__Service.py` was refactored to use `Storage_FS` operations instead of raw dictionary storage. The service stores two files per transfer:

- `transfers/{id}/meta.json` -- metadata (transfer ID, status, file size, timestamps, IP hash, events log)
- `transfers/{id}/payload` -- the encrypted binary blob

The critical architectural point: `Transfer__Service` accepts any `Storage_FS` instance via dependency injection:

```python
class Transfer__Service(Type_Safe):
    storage_fs : Storage_FS = None

    def __init__(self, **kwargs):
        super().__init__(**kwargs)
        if self.storage_fs is None:
            self.storage_fs = Storage_FS__Memory()
```

If you give it a memory backend, it stores in memory. If you give it an S3 backend, it stores in S3. The service code -- all 137 lines of it -- does not contain a single `if s3` or `if memory` branch. It calls `self.storage_fs.file__save()`, `self.storage_fs.file__json()`, `self.storage_fs.file__exists()`, and `self.storage_fs.file__bytes()`. That is all.

This is what "pluggable storage" looks like when the abstraction layer is done right. The entire S3 integration, from prototype to production, required zero changes to the transfer service logic.

**Source:** `sgraph_ai_app_send/lambda__user/service/Transfer__Service.py`

---

## 6. It worked

CI deployed both Lambda functions automatically. The user Lambda started with AWS credentials available in its execution environment. `Send__Config` detected the credentials, selected S3 mode, resolved the bucket name to `745506449035--sgraph-send-transfers--eu-west-2`, and the `Storage_FS__S3.setup()` method created the bucket automatically (because it did not yet exist).

The first transfer went through. Two objects appeared in S3:

```
transfers/d2ac6237561c/meta.json
transfers/d2ac6237561c/payload
```

A 12-character hex transfer ID, a metadata JSON file, and the encrypted payload. The same structure that existed in memory now persisted in S3. Lambda cold-starts no longer matter. Different Lambda instances serve the same data. The file stays until it is explicitly deleted.

Following the successful deployment, Dinis re-enabled API key authentication on the user Lambda (`enable_api_key = True`). The Lambda Function URL cannot be left open to anyone who discovers it -- not until proper access controls are in place.

---

## 7. Tests: 56 and counting

The test suite grew from 44 tests (at the MVP milestone) to 56 tests, all passing in 2.86 seconds. All tests continue to use real implementations with an in-memory backend -- no mocks, no patches. The full application stack starts in memory in approximately 100 milliseconds.

The new storage classes (`Storage_FS__S3`, `Send__Config`, `Enum__Storage__Mode`) are covered by unit tests that exercise the auto-detection logic, the S3 bucket naming convention, and the factory pattern. In test mode, `Send__Config` falls back to memory (because no AWS credentials are present in the test environment), which means the existing transfer tests implicitly validate that the storage abstraction works end-to-end.

This is the design philosophy paying off: the same test suite validates both storage modes, because the test does not know or care which backend is active.

---

## 8. The incident that made us stronger

During the same session that delivered the S3 integration, the team experienced its first security incident -- or more precisely, its first incident response exercise.

### What happened

Dinis Cruz spotted something odd in the git log. Commit `956c37f` on the feature branch `claude/read-secure-send-brief-Mz6yN` was authored by "GitHub Actions \<actions@github.com\>" with the message "Update release badge and version file." That is the exact signature of a CI pipeline commit. But CI pipelines only run on pushes to `dev` and `main`. They do not run on feature branches.

The question was immediate: how did a GitHub Actions commit get onto a feature branch?

### The investigation

DevOps opened INC-001 with a dual severity rating: P3-actual (low real-world impact) and P1-exercise (treated as a critical incident for training purposes).

The investigation traced the commit's lineage. The original commit (`f3b8a82`) was created by the CI pipeline on the `dev` branch -- a legitimate version bump from v0.2.16 to v0.2.17. When the Dev agent later rebased the feature branch onto `dev`, git replayed the CI commit onto the feature branch, producing a new commit (`956c37f`) that preserved the original Author (GitHub Actions) while updating the Committer to Claude.

This is standard git behaviour. A rebase preserves the Author field of every replayed commit. The Committer field changes to whoever performed the rebase. The result: a commit that looks like it came from CI in the Author field, but was actually created by the Claude agent in the Committer field.

The same pattern was found in 5 total commits impersonating GitHub Actions and 3 additional commits impersonating Dinis Cruz -- all produced by the same rebase workflow on two occasions (11 February and 12 February).

### The systemic gaps

AppSec's analysis (`team/roles/appsec/reviews/26-02-12/v0.2.16__incident-analysis__INC-001__commit-impersonation.md`) concluded this was not malicious. But it exposed three gaps that matter for any project using AI agents:

**1. No commit signing.** The git configuration has `commit.gpgsign=true`, but the signing key is an empty file. The signing program silently fails. Every commit appears unsigned despite the configuration suggesting otherwise. This is a false sense of security -- worse than having no signing at all.

**2. No detection mechanisms.** There is no pre-commit hook that checks Author identity, no CI pipeline step that validates Author/Committer consistency, and no audit script that cross-references "GitHub Actions" commits with actual CI pipeline runs.

**3. No agent guidance on commit identity.** CLAUDE.md specifies branch naming conventions (`claude/{description}-{session-id}`) but has no policy on commit authorship. There is nothing telling an agent "do not use `--author` to override your identity" or "use `--reset-author` when rebasing."

### The response

The incident activated roles that had not yet been exercised: GRC (Governance, Risk, and Compliance) and DPO (Data Protection Officer), both newly defined in Dinis's role expansion brief. The DevOps team created the incident infrastructure (`team/roles/devops/.issues/issues/Incident-1/`) with a structured timeline of 6 entries tracking the incident from detection through resolution.

AppSec delivered 8 recommendations, ranging from immediate (fix the broken signing infrastructure, add an agent identity policy to CLAUDE.md) to medium-term (require signed commits on protected branches, build a commit provenance audit script).

The incident took approximately 90 minutes from detection to documented resolution. For a team that had never run an incident response, that is a strong first exercise.

**Source:** `team/roles/devops/.issues/issues/Incident-1/issue.json`, `team/roles/appsec/reviews/26-02-12/v0.2.16__incident-analysis__INC-001__commit-impersonation.md`

---

## 9. From prototype to product

Consider the trajectory:

| Milestone | Date | What changed |
|-----------|------|-------------|
| First MVP | 11-12 Feb | Full transfer cycle works, but in-memory only |
| S3 goes live | 12 Feb | Transfers persist across Lambda instances, ready for real users |

That is one session. Same day. The application went from "works if you are lucky enough to hit the same Lambda instance" to "works reliably, every time, from any instance."

The reason it happened so fast is the architecture. Memory-FS was designed from the start as a pluggable abstraction. When Dinis provided the S3 reference implementation, the Dev agent did not need to refactor the transfer service, rewrite the routes, or update the frontend. The service already spoke the `Storage_FS` interface. The new S3 backend just needed to implement the same interface. Plug it in, and the entire application works with persistent storage.

This is the payoff of investing in the right abstractions early. The Architect and Cartographer identified the critical path at the start: Schemas, Storage Wrapper, Transfer Service, User Lambda, Frontend, E2E. The storage wrapper was on the critical path because everything upstream depends on it. Getting that abstraction right meant the S3 integration was a configuration change, not an architecture change.

---

## 10. What comes next: the friendlies

Dinis's daily brief for 12 February (`v0.2.16__daily-brief__sgraph-send-next-mvp-friendlies.md`) lays out the path to the first beta release. The goal: "an end-to-end working solution I can share with the first batch of users -- the 'friendlies.' People I can reach out to, post about on LinkedIn, invite-only flow."

Four workstreams are in motion:

**UI and design improvements.** The current interface is functional but vanilla. A Designer role (adapted from another project) will create branding guidelines, a logo brief, and polished UI mockups. The functionality works; it needs to look the part.

**Foundation architecture.** SPA routing (all paths serve `index.html`, except static assets), URL-based navigation, multi-language support (Portuguese first -- Dinis is Portuguese), and accessibility theming (high contrast, black and white modes). All foundations that become harder to add later.

**Sharing UX.** Three sharing modes: individual copy/paste (most secure -- channel separation), single URL with hash fragment (the fragment never reaches the server -- AppSec is reviewing this claim), and email template (pre-filled body with everything the recipient needs). The user sees all three options with clear trade-off explanations.

**Journalist website.** Jekyll on GitHub Pages, sourced from the journalist's folder in the repo. Daily updates, project articles, technical documentation. The journalism and documentation site for the project.

The S3 integration was the infrastructure blocker. With persistent storage in place, the path to the friendlies release is clear: polish the UX, add proper sharing modes, and ship.

---

## 11. The architecture that made it possible

Three design decisions made the S3 transition seamless:

**Memory-FS abstraction.** Every storage operation in the application goes through a `Storage_FS` interface. The interface has eight methods: save, read bytes, read string, read JSON, exists, delete, list paths, and clear. Any class that implements these eight methods is a valid storage backend. The application never knows which one it is talking to.

**Dependency injection via Type_Safe.** `Transfer__Service` accepts a `storage_fs` parameter. In tests, it gets `Storage_FS__Memory()`. In production, it gets `Storage_FS__S3(s3_bucket=...).setup()`. The service code is identical in both cases.

**Factory pattern with auto-detection.** `Send__Config` encapsulates the logic of "which backend should I use?" It checks credentials, resolves bucket names, and produces a ready-to-use storage instance. The Lambda handler calls `Send__Config().create_storage_backend()` and passes the result to `Transfer__Service`. One line of wiring. Done.

This is not accidental. The project's rules (`CLAUDE.md`) mandate that "all storage goes through Memory-FS (`Storage_FS`), never direct filesystem or S3 calls." The rule was written before the S3 backend existed. When the time came to implement S3, the rule had already shaped the codebase into the right form.

---

## 12. What the server stores

For transparency -- and because SGraph Send considers transparency a core product feature -- here is exactly what sits in S3 for each transfer:

**`transfers/{id}/meta.json`** contains:
- `transfer_id` -- 12-character cryptographically random hex string
- `status` -- pending, completed
- `file_size_bytes` -- size of the encrypted payload
- `content_type_hint` -- MIME type hint (not the original file type, but what the browser reports)
- `created_at` -- UTC timestamp
- `sender_ip_hash` -- SHA-256 hash of the sender's IP address
- `download_count` -- number of times the file has been downloaded
- `events` -- timestamped log of actions (upload, complete, download with downloader IP hash and user-agent)

**`transfers/{id}/payload`** contains:
- The encrypted binary blob: `[12 bytes IV][ciphertext + GCM auth tag]`

Not stored, anywhere, ever:
- The original file name
- The decryption key
- The raw IP address
- The plaintext file content

The S3 bucket stores ciphertext. Without the key -- which lives on the sender's device and wherever they chose to share it -- the data is computationally useless. A full S3 breach is, by design, a non-event for file confidentiality.

---

## Sources

| Claim | Source |
|-------|--------|
| In-memory limitation of the MVP | `team/humans/dinis_cruz/briefs/02/12/v0.2.10__review-of-user-site-mvp.md`, lines 99-106 |
| Reference S3 implementation from Dinis | `team/humans/dinis_cruz/code-example/example-of-memory-fs-with-s3/Cache__Config.py` |
| Storage mode enum | `sgraph_ai_app_send/lambda__user/storage/Enum__Storage__Mode.py` |
| S3 storage backend | `sgraph_ai_app_send/lambda__user/storage/Storage_FS__S3.py` |
| Send config factory | `sgraph_ai_app_send/lambda__user/storage/Send__Config.py` |
| Transfer service (refactored) | `sgraph_ai_app_send/lambda__user/service/Transfer__Service.py` |
| Friendlies daily brief | `team/humans/dinis_cruz/briefs/02/12/v0.2.16__daily-brief__sgraph-send-next-mvp-friendlies.md` |
| Incident INC-001 issue record | `team/roles/devops/.issues/issues/Incident-1/issue.json` |
| Incident timeline (6 entries) | `team/roles/devops/.issues/issues/Incident-1/timeline/` |
| AppSec incident analysis | `team/roles/appsec/reviews/26-02-12/v0.2.16__incident-analysis__INC-001__commit-impersonation.md` |
| MVP milestone article | `team/roles/journalist/reviews/26-02-12/v0.2.15__article__mvp-milestone.md` |

---

## AppSec verification notes

The following claims in this article should be verified by AppSec before external publication:

1. "A full S3 breach is, by design, a non-event for file confidentiality" -- True assuming no code path leaks the key to server-side storage or logs. Same claim as MVP article; same verification needed.
2. "The fragment never reaches the server" (URL hash sharing mode) -- Dinis has flagged this for AppSec threat modelling. Not yet verified. Included here as a forward-looking statement, not a confirmed fact.
3. "56 tests passing" -- Verified at time of writing. Test count may have changed by publication.

---

*SGraph Send Journalist -- S3 Goes Live Article*
*Version: v0.2.16*
*Date: 12 February 2026*
