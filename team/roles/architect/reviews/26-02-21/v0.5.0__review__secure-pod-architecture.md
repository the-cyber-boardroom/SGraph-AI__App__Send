# Architect Review: Secure Pod -- Multi-User Revocable Encryption

**Version:** v0.5.0
**Date:** 2026-02-21
**Role:** Architect
**Team:** Explorer
**Status:** RESEARCH -- future capability, triggered by customer demand
**Responding to:** [`v0.4.27__architecture__secure-pod-multi-user-encryption.md`](../../../humans/dinis_cruz/briefs/02/21/v0.4.27__architecture__secure-pod-multi-user-encryption.md)

---

## Verdict: Architecturally Sound, with Caveats

The Secure Pod architecture is **well-designed and correctly identifies a genuine hard problem**. The encryption trilemma is real. The brief's analysis of why naive approaches fail is precise. The pod concept -- an intermediary that decrypts, re-encrypts per-user, then forgets -- is the standard pattern used by proxy re-encryption schemes in the academic literature, grounded here in practical infrastructure choices rather than exotic cryptographic primitives.

**The architecture is sound.** It does what it claims: it shifts the trust boundary from the storage server (which sees only ciphertext) to the pod (which sees plaintext momentarily), and it bounds the blast radius of compromise through access-time authorisation, per-request unique keys, and append-only audit trails.

**Three caveats the Architect wants on record:**

1. **The pod IS a single point of trust.** The brief acknowledges this but frames pod compromise as "containable." The Architect agrees with the containment analysis -- but containment depends on detection speed, and detection depends on the observability pipeline (v0.3.6) being operational. Without monitoring, pod compromise is silent and total.

2. **The "forget" guarantee depends entirely on the runtime environment.** In a standard Lambda or container, "clearing memory" means zeroing a variable -- but the runtime may page to disk, the OS may swap, and the hypervisor snapshot may persist. Only a true hardware enclave (Nitro, SGX, HSM) provides a cryptographic guarantee of memory isolation. The brief's Phase 2 (separate AWS account) does not provide this guarantee -- it provides network isolation, not memory isolation. This distinction matters for the security claims.

3. **The online requirement is a significant operational constraint.** Pod unavailability means zero access to all pod-mediated content. For the investor data room use case, this means a pod outage during a due diligence process is a business incident. High availability design for the pod is not optional -- it is a prerequisite for production deployment.

---

## Security Model Validation

### The Encryption Trilemma

The brief correctly identifies the three properties in tension: zero-knowledge server, multi-user access, and revocable access. The analysis of why each naive approach fails is accurate:

| Approach | Failure Mode | Brief's Analysis | Architect's Assessment |
|---|---|---|---|
| Per-user PKI wrapping | Revoked user retains wrapped key | Correct | Correct -- this is the fundamental "you can't unknow a key" problem |
| Group key with rotation | Old content remains decryptable | Correct | Correct -- rotation only protects forward content |
| Server-side re-encryption | Breaks zero-knowledge | Correct | Correct -- the server becomes a trusted intermediary |

The pod architecture resolves this by introducing a **dedicated, hardened intermediary** that is NOT the storage server. The storage server remains zero-knowledge. The pod is a separate trust domain with a constrained attack surface (no persistent storage, no network access beyond its operational boundary, append-only audit).

**Key insight the Architect endorses:** The user never holds a key that works on stored content. This is the critical property. Stored blobs are encrypted with the pod's key. Users receive re-encrypted copies with unique, per-request keys. The per-request key does not decrypt the stored blob. This separation is the architectural foundation.

### Maximum Damage Model Assessment

| Scenario | Brief's Assessment | Architect's Assessment |
|---|---|---|
| Server compromised | Zero exposure | **Agree.** Blobs are ciphertext encrypted with pod's key. Server has no key material. |
| User's private key stolen | Only documents user actually opened | **Agree.** Each re-encrypted copy uses a unique per-request key wrapped with the user's public key. The attacker can only decrypt what was delivered to that user. |
| Server + user's private key | Same as user alone | **Agree.** Server blobs are still useless without the pod's key. The attacker gains nothing beyond the user's own re-encrypted copies. |
| Pod compromised (alone) | Bounded by detection + download rate | **Agree with qualification.** The containment analysis is correct: the attacker needs both the pod's private key AND the encrypted blobs. Obtaining blobs requires a second attack (server compromise) or use of normal API channels (detectable). BUT -- the detection speed assumption requires mature monitoring. If the observability pipeline is not operational, detection could take hours or days, not minutes. |
| Server + pod compromised | Total exposure | **Agree.** Both ciphertext and key material are in the attacker's hands. This is the scenario the architecture must make as difficult as possible through infrastructure separation. |

### Pod Compromise Response Protocol

The seven-step protocol (detect, isolate, new pod, re-encrypt, decommission, audit, notify) is well-structured. The Architect notes:

- **Step 4 (re-encrypt)** is the most operationally complex step. Re-encrypting all stored blobs requires the old pod to be operational (to decrypt) and the new pod's public key to be available (to re-encrypt). The old pod must have network access to the storage layer but no other network access. This is a controlled, isolated re-encryption job -- essentially a batch migration with the old pod as the decrypt oracle and the new pod's public key as the encrypt target. For large data rooms with many blobs, this could take significant time. The architecture should define a maximum re-encryption window and an escalation path if it is exceeded.

- **Step 6 (audit)** is straightforward because the hash-chained access log provides a definitive record. The Architect endorses this design -- the audit trail is the single source of truth for blast radius calculation.

### Micro-Payment Budget as Security Control

This is a clever dual-use mechanism. The Architect endorses the concept: a budget that serves both cost control and security control is architecturally elegant because it avoids the "security feature that no one turns on" problem -- the budget exists for cost reasons, and the security benefit is a side effect.

**One concern:** the budget granularity matters. A daily budget that allows 10,000 decryptions per day may be operationally appropriate but still allows an attacker to exfiltrate 10,000 documents before the budget is exhausted. An hourly budget of 500 decryptions is more constraining but may be too restrictive for legitimate batch access patterns (e.g., an investor reviewing 200 documents in a morning session). The budget thresholds need to be tuned per data room based on expected access patterns.

---

## Implementation Option Evaluation

### Option 1: AWS Nitro Enclave

| Criterion | Assessment |
|---|---|
| **Security guarantee** | Strongest. Cryptographic attestation. No SSH. No persistent storage. Enclave memory isolated from parent EC2 instance. |
| **Memory isolation** | Hardware-enforced. The "forget" guarantee is real -- enclave memory is encrypted and inaccessible to the host. |
| **Complexity** | Medium-high. Requires building an enclave application (using the Nitro Enclaves SDK), packaging it as an EIF (Enclave Image File), and running it on a Nitro-capable EC2 instance. |
| **Infrastructure** | Requires EC2 (not Lambda). This is a departure from the current Lambda-first architecture. |
| **Attestation** | The enclave produces a cryptographic attestation document that proves: (a) the exact code running, (b) the enclave was not modified since boot, (c) the signing key is bound to the enclave. Clients can verify this attestation before trusting the pod. |
| **Cost** | EC2 instance cost (Nitro-capable instances start at ~$0.05/hr for m5.xlarge). Not significant for an enterprise data room product. |
| **Verdict** | **Best for production enterprise deployment.** The attestation model is the strongest argument -- clients can cryptographically verify the pod is running the expected code. |

### Option 2: Separate AWS Account

| Criterion | Assessment |
|---|---|
| **Security guarantee** | Network-level isolation. The pod account's IAM cannot be accessed from the content server account. |
| **Memory isolation** | None beyond standard Lambda/container isolation. The pod's memory is not hardware-protected. AWS has access. |
| **Complexity** | Low. Standard AWS multi-account setup. Lambda function in the pod account receives encrypted blobs via API call, decrypts with the pod's private key (stored in Secrets Manager or KMS), re-encrypts for the user, returns the result. |
| **Infrastructure** | Fits the existing Lambda model. No EC2 required. |
| **Attestation** | AWS organizational controls (SCPs, CloudTrail). Not cryptographic attestation -- you trust AWS's account isolation, not a hardware proof. |
| **Cost** | Minimal. Lambda invocations in a second account cost the same as the first. |
| **Verdict** | **Best for prototype and investor demo.** Fast to build, uses familiar infrastructure, provides meaningful isolation (separate account = separate blast domain). The security claims are weaker than Nitro but sufficient for early customers. |

### Option 3: Hardware Security Module (CloudHSM)

| Criterion | Assessment |
|---|---|
| **Security guarantee** | FIPS 140-2/3 validated. Tamper-resistant hardware. Keys never leave the HSM. |
| **Memory isolation** | Keys are hardware-isolated. However, the decryption and re-encryption operations happen in the HSM's limited processing environment, not in general-purpose memory. |
| **Complexity** | High. CloudHSM requires a VPC, an HSM cluster, and custom integration via PKCS#11 or JCE. The pod's decrypt-re-encrypt logic must be adapted to use HSM APIs for all key operations. |
| **Infrastructure** | CloudHSM is an always-on resource (~$1.60/hr per HSM, ~$1,150/month). Requires two HSMs for HA. |
| **Attestation** | FIPS certification. Strongest regulatory compliance story. |
| **Cost** | Significant. $2,300+/month for an HA HSM cluster. Only justified for high-value enterprise data rooms. |
| **Verdict** | **Best for regulated industries (financial services, healthcare, government).** The FIPS certification is valuable for compliance. Not appropriate for the prototype or early customers due to cost. |

### Option 4: Client-Side Pod (Sender's Browser)

| Criterion | Assessment |
|---|---|
| **Security guarantee** | No server trust required. The sender's browser IS the pod. The sender holds the pod's private key. |
| **Memory isolation** | Browser-level only. IndexedDB with non-extractable keys (Web Crypto API). |
| **Complexity** | Low to implement. The sender's browser already has the plaintext (they created the content). They can re-encrypt for any authorised user on demand. |
| **Infrastructure** | Zero. No server-side pod at all. |
| **Attestation** | None. Trust the sender. |
| **Cost** | Zero infrastructure cost. |
| **Availability** | **Critical limitation.** The sender must be online for every access request. If the sender's browser is closed, no one can access the content. |
| **Verdict** | **See dedicated analysis below -- this is the critical question.** |

### Option 5: Third-Party Enclave (Azure / GCP)

| Criterion | Assessment |
|---|---|
| **Security guarantee** | Comparable to Nitro Enclaves. Azure Confidential Computing uses Intel SGX/AMD SEV. GCP Confidential VMs use AMD SEV-SNP. |
| **Complexity** | High. Cross-cloud deployment adds operational complexity. |
| **Infrastructure** | Requires Azure or GCP account, different SDK, different attestation model. |
| **Attestation** | Provider-specific. Not interoperable with AWS attestation. |
| **Verdict** | **Future consideration for multi-cloud enterprise deployments.** Not relevant for the current stage. Useful when a customer requires non-AWS infrastructure. |

### Recommendation: Phased Implementation

```
Phase 1 (Prototype):     Option 2 -- Separate AWS Account
                          (Lambda in isolated account, Secrets Manager for pod key)
                          Fast to build, proves the concept, investor-demo ready.

Phase 2 (Early customers): Option 2 hardened
                          (add CloudTrail, SCPs, budget alerts, rate limiting)
                          Sufficient for non-regulated enterprise customers.

Phase 3 (Enterprise):    Option 1 -- Nitro Enclave
                          (hardware-isolated pod with cryptographic attestation)
                          Required for customers who need attestation or regulated-industry compliance.

Phase 4 (Regulated):     Option 3 -- HSM (alongside or inside Nitro Enclave)
                          FIPS 140 compliance for financial/healthcare/government.

Phase N (Multi-cloud):    Option 5 -- as customer requirements dictate.
```

---

## The Critical Question: Is Client-Side Pod Viable for the Investor Use Case?

The brief flags this question and the Architect must address it directly.

### The Investor Use Case

The investor wants to share documents (NDAs, term sheets, financial models) with founders and co-investors in a data room with revocable access. The investor (or their fund administrator) is the data room owner. They upload documents. Multiple recipients access them. Recipients can be added and removed.

### Client-Side Pod: How It Would Work

```
1. Investor creates a data room in SG/Send
2. Investor's browser generates a pod key pair (RSA-OAEP 4096 or ECDH)
3. Investor uploads documents encrypted with the pod's public key
4. Server stores encrypted blobs (standard)
5. When a recipient requests a document:
   a. Server verifies recipient is authorised (trust graph check)
   b. Server sends the encrypted blob to... the investor's browser
   c. Investor's browser decrypts, re-encrypts for the recipient, sends back
   d. Server delivers the re-encrypted copy to the recipient
```

### Why It Does Not Work for This Use Case

| Problem | Detail |
|---|---|
| **Availability** | The investor must have their browser open and connected for every access request. A founder opening a term sheet at 11pm will fail if the investor's laptop is closed. This is a dealbreaker for a data room product. |
| **Latency** | Every access request requires a round-trip to the investor's browser: server --> investor browser --> server --> recipient. On a mobile connection, this adds seconds of latency. |
| **Multi-device** | The pod key is in one browser's IndexedDB. The investor cannot manage the data room from their phone and their laptop simultaneously without key synchronisation -- which reintroduces the key management problem. |
| **Scale** | If 20 recipients are reviewing documents simultaneously, the investor's browser is handling 20 concurrent decrypt-re-encrypt operations. This is feasible for small documents but degrades for large financial models. |
| **UX burden** | The investor becomes the infrastructure. "Keep your browser open so people can read the NDA" is not a product-grade experience. |

### When Client-Side Pod IS Viable

The client-side pod model works for a **different, simpler use case**: peer-to-peer secure sharing where the sender and recipient are both online and the sender is willing to act as the access controller. Examples:

- **Live presentations**: "I'm presenting now, click this link to see the slides" -- the sender is present and online by definition.
- **Real-time collaboration**: Both parties are in a session together (like the encrypted chat channel concept from the 18 Feb brief).
- **Ephemeral sharing**: "I'll share this for the next 30 minutes while we're on this call."

**Verdict:** Client-side pod is NOT viable for the investor data room use case. It IS viable as an additional mode for ephemeral, real-time sharing. The server-side pod (Options 1-3) is required for the data room product.

### A Hybrid Worth Considering

There is a middle ground: **client-side pod with server-side fallback**.

```
1. When the sender's browser is online: client-side pod handles re-encryption
   (zero server trust, sender controls everything)

2. When the sender's browser is offline: server-side pod handles re-encryption
   (sender pre-authorises the server-side pod to act on their behalf)
```

This preserves the zero-trust property when the sender is available and gracefully degrades to the server-side pod model when they are not. The sender explicitly opts in to the fallback by encrypting their pod key for the server-side pod's public key. This is a future enhancement, not a Phase 1 concern.

---

## Mapping to Existing SG/Send Infrastructure

### How the Pod Fits

```
Current Architecture:
  ┌──────────────────────────────────┐
  │  Lambda__User (Public)            │
  │  - Routes__Transfers              │
  │  - Routes__Health                 │
  │  - Static UI                      │
  │  - Storage_FS (Memory/S3)         │
  └──────────────────────────────────┘
  ┌──────────────────────────────────┐
  │  Lambda__Admin                    │
  │  - Routes__Admin                  │
  │  - Routes__Analytics              │
  │  - Storage_FS (Memory/S3)         │
  └──────────────────────────────────┘

Pod Architecture (adds):
  ┌──────────────────────────────────┐
  │  Lambda__Pod (NEW -- separate     │
  │           AWS account)            │
  │  - Routes__Pod_Decrypt            │
  │  - Pod_Key (in Secrets Manager    │
  │            or KMS)                │
  │  - Audit_Log (append-only)        │
  │  - NO Storage_FS (no blobs here)  │
  └──────────────────────────────────┘
```

### Infrastructure Requirements (Phase 1 Prototype)

| Component | Implementation | Notes |
|---|---|---|
| **Pod Lambda function** | New Lambda in a separate AWS account | Uses `Serverless__Fast_API` (same base class as existing Lambdas). Follows osbot-fast-api-serverless patterns. |
| **Pod key storage** | AWS Secrets Manager or KMS in the pod account | Accessed via `osbot-aws` (never boto3 directly). Key material never leaves the pod account. |
| **Pod API** | Two endpoints: `POST /pod/re-encrypt`, `GET /pod/health` | The re-encrypt endpoint receives: encrypted blob, user's public key, authorisation proof. Returns: re-encrypted content, wrapped unique key. |
| **Audit log** | Storage_FS in the pod account (append-only) | Hash-chained entries as described in the brief. Stored in the pod's own S3 bucket. |
| **Authorisation** | Trust graph query to the main server | Pod calls the main server's trust graph API to verify the user is still authorised. This is the authorisation check at Step 2 of the access flow. |
| **Network** | Cross-account API calls via Lambda Function URLs or API Gateway | The user Lambda calls the pod Lambda for re-encryption. The pod Lambda calls back to verify authorisation. Both calls are HTTPS. |

### What Uses osbot-aws

All AWS interactions in the pod account use `osbot-aws`:

- Secrets Manager access (pod key retrieval): `osbot-aws` Secrets Manager wrapper
- S3 access (audit log storage): `osbot-aws` S3 wrapper via `Storage_FS`
- Lambda invocation: `osbot-aws` Lambda wrapper
- IAM/STS: `osbot-aws` IAM wrapper

### What Does NOT Change

- The existing `Lambda__User` and `Lambda__Admin` are unchanged
- The existing `Storage_FS` abstraction is unchanged
- The existing transfer workflow is unchanged
- Mode 1 (Direct PKI) is unaffected
- The UI for Mode 1 is unaffected

---

## Latency and Availability Assessment

### Latency Analysis

For a pod-mediated access request, the data flow is:

```
User browser
    | (1) Request document
    v
Lambda__User
    | (2) Verify trust graph (local check)
    | (3) Fetch encrypted blob from S3
    | (4) Call Pod Lambda with blob + user's public key
    v
Lambda__Pod (separate account)
    | (5) Decrypt blob (pod's private key)
    | (6) Generate unique AES key
    | (7) Re-encrypt with unique key
    | (8) Wrap unique key with user's public key
    | (9) Log to audit trail
    | (10) Return re-encrypted content + wrapped key
    v
Lambda__User
    | (11) Forward to user
    v
User browser
    | (12) Unwrap key, decrypt content
```

| Step | Estimated Latency | Notes |
|---|---|---|
| (1) User request | ~50ms | HTTPS round-trip to Lambda Function URL |
| (2) Trust graph check | ~5-20ms | In-memory or S3 read |
| (3) S3 blob fetch | ~20-100ms | Depends on blob size. 1MB file ~ 50ms. |
| (4) Cross-account Lambda call | ~100-300ms | Lambda cold start in pod account is the risk. Warm invocation ~ 50ms. |
| (5-8) Crypto operations | ~10-50ms | RSA-OAEP decrypt + AES-GCM encrypt + RSA-OAEP wrap. CPU-bound. Fast for sub-10MB files. |
| (9) Audit log write | ~20-50ms | S3 PUT in pod account |
| (10) Return | ~20-50ms | Response transit |
| (11) Forward | ~5ms | In-process |
| (12) Client decrypt | ~5-20ms | Web Crypto API |

**Total estimated latency: 235-640ms** for a warm pod. **Add 500-1000ms for pod Lambda cold start** on the first request after idle.

**Comparison:** Mode 1 (Direct PKI) requires only steps 1, 3, 11, 12 -- approximately 75-170ms. Pod-mediated access adds 160-470ms of overhead.

**Assessment:** For document access (NDA, term sheet, financial model), sub-1-second latency is acceptable. Users expect a brief loading period when opening a document. For real-time chat or interactive workflows, the overhead is noticeable but still within acceptable bounds.

**Mitigation strategies:**
- Keep the pod Lambda warm via scheduled invocations (CloudWatch Events trigger every 5 minutes)
- Use provisioned concurrency for the pod Lambda (eliminates cold starts entirely, costs ~$3.80/month per provisioned instance)
- For large files, consider streaming the re-encrypted content back rather than buffering the entire blob in memory

### Availability Analysis

| Component | Availability | Impact if Down |
|---|---|---|
| Lambda__User | 99.95% (Lambda SLA) | No access to any content (both modes) |
| S3 (content storage) | 99.99% (S3 SLA) | No access to any content (both modes) |
| Lambda__Pod | 99.95% (Lambda SLA) | **Mode 2 content inaccessible; Mode 1 unaffected** |
| Secrets Manager (pod key) | 99.99% | Pod cannot decrypt; Mode 2 inaccessible |

**Combined availability for Mode 2:** approximately 99.90% (the product of Lambda__User and Lambda__Pod availability). This translates to approximately 8.7 hours of potential downtime per year.

**Mitigation strategies:**
- Multi-region pod deployment (active-active or active-passive)
- Circuit breaker: if pod is unavailable, Mode 2 requests fail gracefully with a clear error ("Data room temporarily unavailable, please try again")
- Pod health monitoring with PagerDuty/similar alerting
- Consider Nitro Enclave on a dedicated EC2 instance (higher base availability than Lambda for always-on workloads)

---

## What Current PKI Work Creates the Foundation

The current PKI work (v0.4.17 implementation plan, v0.4.25 PKI messaging system, v0.4.27 key discovery brief) provides the essential building blocks for the pod architecture:

| Current Work | How It Foundations the Pod |
|---|---|
| **Browser key generation** (RSA-OAEP 4096, ECDSA P-256) | Users already have key pairs. The pod needs users to have public keys so it can re-encrypt for them. This is done. |
| **Non-extractable private keys** (Web Crypto API, IndexedDB) | Users' private keys are hardware-protected in the browser. The pod wraps unique keys with users' public keys, and only the user's non-extractable private key can unwrap. This is the security guarantee. |
| **Key discovery and public registry** (v0.4.27 dev brief) | The pod needs to know users' public keys to re-encrypt for them. The registry provides this lookup. Without key discovery, the pod would need a separate key distribution mechanism. |
| **Trust graph and hash chaining** (v0.4.27 chain-of-trust brief) | The pod queries the trust graph for authorisation decisions. Revocation is a graph operation. The hash-chained commit log provides the tamper-evident audit trail. This is the authorisation backbone. |
| **Hybrid encryption pattern** (AES-256-GCM + RSA-OAEP) | The pod uses the same hybrid pattern: generate a random AES key per request, encrypt content with AES-GCM, wrap the AES key with the user's RSA public key. The pattern is already implemented and tested in the PKI messaging system. |
| **Key bundle format** (JSON with v, encrypt, sign fields) | The pod can use the same key bundle format for user key exchange. No new format required. |
| **Signed requests** (ECDSA P-256 signatures, v0.4.17 admin mTLS plan) | The pod can authenticate the requesting user via ECDSA challenge-response, the same pattern designed for admin authentication. |

**Summary:** The current PKI work provides approximately 70% of the client-side infrastructure the pod needs. The remaining 30% is the server-side pod implementation itself (the decrypt-re-encrypt Lambda, the pod key management, the cross-account API).

---

## The Minimum Viable Pod

The simplest thing that proves the concept:

### Scope

- Single data room with 2-3 users
- Documents under 5MB
- Separate AWS account for the pod
- No Nitro Enclave (standard Lambda)
- No micro-payment budget (manual monitoring)
- No multi-region HA
- Polling for trust graph changes (no real-time revocation push)

### Components

```
1. Lambda__Pod (new, in pod AWS account)
   - POST /pod/re-encrypt
     Input:  { encrypted_blob: base64, user_public_key: PEM, auth_token: string }
     Output: { re_encrypted_content: base64, wrapped_key: base64 }
   - GET /pod/health
     Output: { status: "ok", key_fingerprint: "sha256:..." }

2. Pod key pair
   - Generated once, stored in Secrets Manager
   - Public key published to the main server
   - Private key never leaves the pod account

3. Audit log
   - S3 bucket in pod account
   - One JSON object per access request
   - Hash-chained (each entry includes SHA-256 of previous entry)

4. Trust graph integration
   - Pod calls main server's trust graph API to verify authorisation
   - Simple HTTP call: GET /api/trust/{data_room_id}/check?user={user_id}
   - Returns: { authorised: true/false }

5. UI changes
   - Mode selector on upload: "Direct PKI" / "Data Room (Pod-Mediated)"
   - When "Data Room" is selected, content is encrypted with the pod's public key
   - Download page detects pod-mediated content and routes through the pod
```

### Implementation Estimate

| Component | Effort | Depends On |
|---|---|---|
| Pod Lambda skeleton (FastAPI, osbot-fast-api-serverless) | 0.5 day | Nothing |
| Pod key generation and storage (Secrets Manager via osbot-aws) | 0.5 day | Pod Lambda skeleton |
| Re-encryption endpoint (decrypt + re-encrypt logic) | 1 day | Pod key storage |
| Audit log (hash-chained S3 writes) | 0.5 day | Pod Lambda skeleton |
| Trust graph check integration | 0.5 day | Trust graph API (from chain-of-trust work) |
| Cross-account networking (Lambda Function URL, IAM) | 0.5 day | Pod Lambda skeleton |
| UI mode selector | 0.5 day | Re-encryption endpoint |
| Upload flow modification (encrypt with pod key) | 0.5 day | Pod public key published |
| Download flow modification (route through pod) | 1 day | Re-encryption endpoint |
| Integration tests (full cycle: upload, re-encrypt, download, revoke) | 1 day | All above |
| **Total** | **~6-7 days** | Trust graph is the external dependency |

**Critical path dependency:** The trust graph API must exist for the pod to make authorisation decisions. If the trust graph is not yet implemented, the MVP pod can use a simpler authorisation model (a static list of authorised user IDs stored in the pod account) as a temporary stand-in.

---

## Phased Recommendations

### Phase 0: Foundation (NOW -- already in progress)

**Status: Active**

Complete the current PKI work that enables the pod:

1. Key discovery UI and public registry (v0.4.27 dev brief) -- **immediate priority**
2. Trust graph data model in Issues-FS (v0.4.27 chain-of-trust brief) -- **design work**
3. Hash-chained commit log for trust graph mutations -- **design work**

**These are the prerequisites. Without key discovery and trust graphs, the pod has no user keys to re-encrypt for and no authorisation model to query.**

### Phase 1: Minimum Viable Pod (WHEN triggered by customer demand)

Build the MVP described above. Target: working demo with a single data room, 2-3 users, sub-5MB documents. Separate AWS account, standard Lambda, manual monitoring.

**Goal:** Prove the decrypt-re-encrypt-forget cycle works end-to-end. Demonstrate revocation (remove a user, show they can no longer access). Show the audit trail.

**Duration:** 6-7 days of development effort once Phase 0 prerequisites are met and the decision to proceed is made.

### Phase 2: Harden for Early Customers

- Add CloudTrail audit in the pod account
- Add SCPs to prevent pod account access from the main account
- Add provisioned concurrency for the pod Lambda (eliminate cold starts)
- Add budget alerts (CloudWatch billing alarms per pod)
- Add rate limiting on re-encryption requests
- Add monitoring dashboards (requests per minute, latency percentiles, error rates)
- Add graceful degradation (if pod is down, show clear error, Mode 1 continues working)

### Phase 3: Enterprise (Nitro Enclave)

- Migrate pod logic to Nitro Enclave on EC2
- Implement cryptographic attestation flow
- Add attestation verification to the client (user's browser verifies the pod is running expected code before trusting it with their public key)
- Consider HSM for pod key storage (keys never leave hardware)

### Phase 4: Production Data Room Product

- Multi-region pod deployment
- Client-side pod option for real-time/ephemeral sharing
- Micro-payment budget integration (Accountant role)
- Full data room UI with member management, document organisation, access history
- SLA and support model

---

## Architectural Decisions

| Decision | Description | Recommendation |
|---|---|---|
| **AD-25** | Pod implementation for prototype | Separate AWS account (Lambda) -- fastest path to working demo |
| **AD-26** | Client-side pod for investor use case | Not viable -- requires sender to be always-online. Server-side pod is required. Consider client-side as additional mode for ephemeral sharing. |
| **AD-27** | Pod key storage | Secrets Manager for prototype, KMS for hardened, HSM for regulated |
| **AD-28** | Pod-to-server authorisation protocol | Pod calls trust graph API via HTTPS. Standard bearer token auth for the cross-service call. |
| **AD-29** | Audit log format | Hash-chained JSON objects in S3, same pattern as trust graph commits |
| **AD-30** | MVP scope | Single data room, 2-3 users, sub-5MB documents, manual monitoring |

---

## Open Questions for Human Decision

1. **Pod key rotation ceremony.** The brief describes a response protocol for pod compromise that includes re-encrypting all blobs with a new pod key. Who authorises this operation? Is it automated (the system detects compromise and auto-rotates) or manual (a human reviews the evidence and initiates rotation)? The Architect recommends manual authorisation for Phase 1 (the blast radius of an incorrect auto-rotation is high).

2. **Trust graph dependency.** Should the MVP pod use the full trust graph (from the chain-of-trust architecture) or a simplified authorisation model (static list of user IDs)? The full trust graph is more correct but depends on work that may not be complete. A static list is faster to build but loses the revocation elegance.

3. **Multi-pod isolation.** Should each data room have its own pod (with its own key pair), or should one pod serve multiple data rooms? Per-data-room pods provide stronger isolation (compromise of one pod affects only one data room) but multiply infrastructure costs. A shared pod reduces costs but increases the blast radius.

4. **Timeline trigger.** What specific customer demand signal triggers Phase 1 implementation? The Architect needs clarity on whether the investor meeting follow-up constitutes sufficient demand, or whether we wait for a signed commitment or paid pilot.

---

## Cross-References

| Document | Location | Relationship |
|---|---|---|
| Secure Pod brief (this review responds to) | [`v0.4.27__architecture__secure-pod-multi-user-encryption.md`](../../../humans/dinis_cruz/briefs/02/21/v0.4.27__architecture__secure-pod-multi-user-encryption.md) | Source brief |
| Chain of trust and key graphs | [`v0.4.27__architecture__chain-of-trust-and-key-graphs.md`](../../../humans/dinis_cruz/briefs/02/21/v0.4.27__architecture__chain-of-trust-and-key-graphs.md) | Trust graph that the pod queries for authorisation |
| Key discovery and public registry | [`v0.4.27__dev-brief__key-discovery-and-public-registry.md`](../../../humans/dinis_cruz/briefs/02/21/v0.4.27__dev-brief__key-discovery-and-public-registry.md) | How users publish keys the pod re-encrypts for |
| PKI messaging architecture | [`v0.4.25__briefing__pki-messaging-architecture.md`](../26-02-20/v0.4.25__briefing__pki-messaging-architecture.md) | Hybrid encryption pattern the pod reuses |
| Browser PKI key management plan | [`v0.4.17__implementation-plan__browser-pki-key-management.md`](../26-02-20/v0.4.17__implementation-plan__browser-pki-key-management.md) | Key generation and storage foundation |
| Admin PKI and mTLS plan | [`v0.4.17__implementation-plan__admin-pki-mtls.md`](../26-02-20/v0.4.17__implementation-plan__admin-pki-mtls.md) | Signed request auth pattern reusable for pod API |
| Investor PKI business incident | [`v0.4.17__business-incident__investor-pki-claude-integration.md`](../../../humans/dinis_cruz/briefs/02/20/v0.4.17__business-incident__investor-pki-claude-integration.md) | Business driver -- the investor's data room use case |
| Service Worker trust anchor | [`v0.4.20__appsec-research__service-worker-trust-anchor.md`](../../../humans/dinis_cruz/briefs/02/20/v0.4.20__appsec-research__service-worker-trust-anchor.md) | Client-side integrity layer that complements pod confidentiality |
| Architect response to 17-19 Feb briefs | [`v0.4.12__response-to-briefs__17-18-19-feb.md`](../26-02-19/v0.4.12__response-to-briefs__17-18-19-feb.md) | Previous architectural decisions (AD-17 through AD-24) |

---

*Architect review complete. The Secure Pod architecture is sound. The encryption trilemma is correctly identified and the pod concept resolves it within well-understood trade-offs. The critical insight -- user never holds a key that works on stored content -- is the architectural foundation. Client-side pod is not viable for the investor data room use case; server-side pod (separate AWS account for prototype, Nitro Enclave for production) is the correct path. Current PKI work provides ~70% of the foundation. The minimum viable pod is approximately 6-7 days of effort once prerequisites (key discovery, trust graph) are met. This is explicitly future work -- implementation is gated on customer demand.*

---

This document is released under the Creative Commons Attribution 4.0 International licence (CC BY 4.0). You are free to share and adapt this material for any purpose, including commercially, as long as you give appropriate credit.
