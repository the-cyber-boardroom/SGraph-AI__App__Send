# Architect Response: Briefs from 17, 18, and 19 February 2026

**Version:** v0.4.12
**Date:** 19 February 2026
**Role:** Architect (Explorer Team)
**Responding to:**

### 17 February
- [`v0.4.7__daily-brief__all-teams-17-feb-2026.md`](../../../humans/dinis_cruz/briefs/02/17/v0.4.7__daily-brief__all-teams-17-feb-2026.md) -- Security review and user feedback

### 18 February
- [`v0.4.10__daily-brief__afternoon-session-18-feb-2026.md`](../../../humans/dinis_cruz/briefs/02/18/v0.4.10__daily-brief__afternoon-session-18-feb-2026.md) -- Afternoon session daily brief
- [`v0.4.10__brief__encrypted-chat-realtime-transfer.md`](../../../humans/dinis_cruz/briefs/02/18/v0.4.10__brief__encrypted-chat-realtime-transfer.md) -- Encrypted chat and real-time transfer
- [`v0.4.10__brief__partner-ecosystem-and-deployment.md`](../../../humans/dinis_cruz/briefs/02/18/v0.4.10__brief__partner-ecosystem-and-deployment.md) -- Partner ecosystem and deployment
- [`v0.4.10__brief__accountant-role-cost-control-financial-flows.md`](../../../humans/dinis_cruz/briefs/02/18/v0.4.10__brief__accountant-role-cost-control-financial-flows.md) -- Accountant role
- [`v0.4.10__brief__ux-observations-whats-next-problem.md`](../../../humans/dinis_cruz/briefs/02/18/v0.4.10__brief__ux-observations-whats-next-problem.md) -- UX observations
- [`v0.4.10__interview__user-a-security-reviewer.md`](../../../humans/dinis_cruz/briefs/02/18/v0.4.10__interview__user-a-security-reviewer.md) -- User A interview

### 19 February
- [`v0.4.11__daily-brief__19-feb-2026.md`](../../../humans/dinis_cruz/briefs/02/19/v0.4.11__daily-brief__19-feb-2026.md) -- 19 Feb daily brief
- [`v0.4.11__brief__post-quantum-cryptography-research.md`](../../../humans/dinis_cruz/briefs/02/19/v0.4.11__brief__post-quantum-cryptography-research.md) -- Post-quantum cryptography
- [`v0.4.11__brief__surf-security-browser-integration.md`](../../../humans/dinis_cruz/briefs/02/19/v0.4.11__brief__surf-security-browser-integration.md) -- Surf Security browser integration
- [`v0.4.11__brief__chaos-agent-runbooks-fire-drills.md`](../../../humans/dinis_cruz/briefs/02/19/v0.4.11__brief__chaos-agent-runbooks-fire-drills.md) -- Chaos agent and runbooks
- [`v0.4.11__brief__guest-visitor-programme.md`](../../../humans/dinis_cruz/briefs/02/19/v0.4.11__brief__guest-visitor-programme.md) -- Guest/Visitor programme
- [`v0.4.11__brief__industry-dinner-debrief-and-validation.md`](../../../humans/dinis_cruz/briefs/02/19/v0.4.11__brief__industry-dinner-debrief-and-validation.md) -- Industry dinner debrief
- [`v0.4.11__brief__ui-bugs-and-improvements.md`](../../../humans/dinis_cruz/briefs/02/19/v0.4.11__brief__ui-bugs-and-improvements.md) -- UI bugs and improvements

**Previous Architect reviews:**
- [`v0.4.4__architect__interactive-encrypted-workflows-assessment.md`](../26-02-16/v0.4.4__architect__interactive-encrypted-workflows-assessment.md) -- Interactive workflow assessment (16 Feb)
- [`v0.3.12__action-plan__brief-response-15-feb.md`](../26-02-15/v0.3.12__action-plan__brief-response-15-feb.md) -- Action plan (15 Feb)
- [`v0.3.0__response-to-daily-brief__14-feb.md`](../26-02-14/v0.3.0__response-to-daily-brief__14-feb.md) -- Admin UI architecture (14 Feb)

---

## Executive Summary

Three days of briefs (17--19 Feb) introduce eight architecturally significant themes. The most consequential is the **encrypted chat / real-time transfer** proposal, which represents a strategic pivot from atomic file transfer to persistent secure channels. This changes the system's identity -- from a transactional utility to a relationship tool -- and requires new architectural primitives (channel abstraction, signalling, chronological message ordering). The second most consequential is the **Surf Security browser integration** concept, which redefines the trust boundary by making the browser itself a first-class security control plane.

The remaining themes -- post-quantum cryptography, partner deployment packaging, financial data flows (Accountant), chaos/resilience testing, the visitor programme, and UI improvements -- are all architecturally tractable within the existing system and mostly additive rather than transformative.

This document assesses each theme, identifies what must change in the architecture, and produces a prioritised list of architectural work items at the end.

---

## 1. Encrypted Chat and Real-Time Transfer

**Source:** [`v0.4.10__brief__encrypted-chat-realtime-transfer.md`](../../../humans/dinis_cruz/briefs/02/18/v0.4.10__brief__encrypted-chat-realtime-transfer.md)

### 1.1 Architectural Assessment

This is the most significant architectural proposal since the interactive encrypted workflows (v0.4.4). The core insight is correct: if two parties share a key, they already have a secure channel. A "chat" is a sequence of small encrypted payloads associated with that key.

**What the architecture gains:** A channel abstraction that groups multiple transfers under a shared key identifier. The server currently treats each transfer as an isolated unit. The chat concept requires the server to understand that transfers can belong to a *group* -- without knowing anything about the group's content.

**What the architecture does NOT need to change:** The encryption model. The zero-knowledge guarantee. The transfer engine. The storage abstraction. These all remain exactly as they are.

### 1.2 New Architectural Primitive: Channel

The current data model is:

```
Transfer (isolated)
  - transfer_id: str
  - status: str
  - created_at: str
  - content_type_hint: str
  - payload: encrypted bytes
```

The chat concept requires a new abstraction:

```
Channel
  - channel_id: str (derived from shared key hash, or a separate random ID)
  - created_at: str
  - message_ids: list[str]  (ordered list of transfer_ids)

Transfer (unchanged)
  - transfer_id: str
  - channel_id: str (optional -- NULL for standalone transfers)
  - status: str
  - created_at: str
  - content_type_hint: str
  - payload: encrypted bytes
```

**Key design decisions:**

1. **Channel ID derivation.** The channel_id MUST NOT be the encryption key itself (that would expose the key to the server). Options:
   - **Option A (recommended):** SHA-256 hash of the key. The server stores `hash(key)` as the channel_id. Clients who know the key can compute the hash and query for their channel. The server learns that transfers are grouped but cannot derive the key from the hash.
   - **Option B:** Random channel_id generated at channel creation, shared alongside the key. Adds one more piece of information to share out-of-band but provides stronger separation between the channel identity and the key.

   **Recommendation:** Option A for simplicity. The hash acts as a deterministic channel identifier that both parties can independently compute. The server learns only the grouping, not the key.

2. **Channel storage in Memory-FS.** The channel metadata lives alongside transfer metadata:
   ```
   channels/{channel_id}/metadata.json    # channel_id, created_at
   channels/{channel_id}/messages.json    # ordered list of transfer_ids
   transfers/{transfer_id}/metadata.json  # unchanged, plus optional channel_id
   transfers/{transfer_id}/payload        # unchanged
   ```

   This follows the existing Storage_FS pattern. The `messages.json` file is appended to as new messages arrive. This file is the only new server-side data structure.

3. **API endpoints.** Three new routes:
   ```
   POST   /channels/{channel_id}/messages     # add a message (transfer) to a channel
   GET    /channels/{channel_id}/messages     # list all message transfer_ids in chronological order
   GET    /channels/{channel_id}/messages?since={timestamp}  # poll for new messages
   ```

   These are thin wrappers around the existing transfer engine. `POST /channels/{channel_id}/messages` creates a transfer and appends its ID to the channel's message list. `GET /channels/{channel_id}/messages` returns the ordered list of transfer_ids. The client then fetches each transfer individually (existing `GET /transfers/download/{id}` endpoint).

4. **Authentication model.** Knowledge of the channel_id (which requires knowledge of the key) is the authentication. If you can compute `SHA-256(key)`, you can access the channel. This is consistent with the existing model where knowledge of the transfer_id grants access.

### 1.3 Real-Time Signalling

The brief correctly identifies four signalling options (polling, SSE, WebSocket, WebRTC). The Architect's assessment:

| Approach | Lambda Compatible | Infrastructure Change | Recommended Phase |
|----------|-------------------|----------------------|-------------------|
| **Polling** | Yes -- standard HTTP requests | None | **Phase 1 (now)** |
| **SSE** | Partial -- Lambda has 15-min timeout, API Gateway has 29-sec timeout. API Gateway WebSocket API can simulate SSE. | Moderate | Phase 2 |
| **WebSocket** | Yes -- via API Gateway WebSocket API, backed by Lambda | Moderate -- new API Gateway config, connection management Lambda, DynamoDB for connection state | Phase 3 |
| **WebRTC** | Not applicable -- P2P bypasses server | Major -- STUN/TURN server needed, NAT traversal | Phase 4 (future) |

**Recommendation: Start with polling.** The polling endpoint (`GET /channels/{channel_id}/messages?since={timestamp}`) fits perfectly into the existing Lambda architecture. The client polls every 2--5 seconds. This is adequate for the chat use case (humans do not type faster than once every few seconds). Move to WebSocket via API Gateway only if polling latency becomes a user-visible problem.

**Critical architectural note on Lambda and persistent connections:** SSE and WebSocket require persistent connections. Lambda functions are stateless and short-lived. AWS API Gateway provides a WebSocket API that maintains persistent connections at the gateway layer and invokes Lambda functions for connect/disconnect/message events. This adds DynamoDB as a connection store (mapping connectionId to channel_id). This is a moderate infrastructure change. It is NOT needed for the MVP chat feature.

### 1.4 Chunked Streaming (Download Before Upload Completes)

This is architecturally feasible but significantly more complex than the basic chat feature. It requires:

1. **Chunked upload:** File is split into chunks on the sender side. Each chunk is independently encrypted (with the same key but unique IVs) and uploaded as a separate transfer or sub-resource.
2. **Chunk manifest:** A manifest tracks which chunks have been uploaded and their order.
3. **Partial download:** The recipient downloads and decrypts available chunks while waiting for the rest.

This builds on the resilience research already assigned (A4 from the 15 Feb action plan, and the detailed research documents produced in `26-02-15/v0.3.13__research__*`). The chunked upload research (`v0.3.13__research__chunked-streaming-encryption-web-crypto-api.md`) already covers the encryption model for streaming chunks.

**Recommendation:** Defer chunked streaming to after the basic chat is working. The chat feature (small text messages and small files with polling) is independently valuable without streaming. Chunked streaming is a Phase 2 enhancement.

### 1.5 Impact on Interactive Encrypted Workflows

The encrypted chat concept and the interactive encrypted workflows (v0.4.4 assessment) are complementary. A workflow response sent within a chat channel is simply another message in the channel. The channel provides the thread context that the workflow currently lacks.

**Future integration path:**
```
Channel: [message] -> [workflow payload] -> [workflow response] -> [message] -> [file]
```

The DataContext, WorkflowRenderer, and ComponentRegistry designed in v0.4.4 are unaffected. The workflow is still a content-type specialisation of a transfer. The channel groups transfers into a sequence.

### 1.6 Corporate Key Escrow (Parked)

The daily brief explicitly parks the concept of enterprise key escrow / admin-accessible recovery. The Architect records this as a future consideration (AD-17):

**AD-17: Enterprise Key Escrow.** For regulated industries, the ability for authorised administrators to decrypt content. This fundamentally conflicts with zero-knowledge. If pursued, it would require a separate encryption mode where the content is encrypted with both the recipient's key AND an organisational master key. The server would need to store the additional ciphertext. This is a major architectural change and should not be undertaken without a clear business requirement. **Status: Parked by explicit human decision.**

---

## 2. Partner Ecosystem and Deployment

**Source:** [`v0.4.10__brief__partner-ecosystem-and-deployment.md`](../../../humans/dinis_cruz/briefs/02/18/v0.4.10__brief__partner-ecosystem-and-deployment.md)

### 2.1 Deployment Target Architecture

The current architecture runs as a Lambda function behind a Lambda Function URL. The brief requests packaging for Docker, PyPI, CloudFormation, Terraform, and GCP Cloud Run. The Architect's assessment of each:

| Target | Architectural Changes Needed | Complexity |
|--------|----------------------------|------------|
| **Docker** | The FastAPI app already runs via Uvicorn locally (`osbot-fast-api` handles this). Need a Dockerfile that installs dependencies and runs Uvicorn. Memory-FS switches to disk-backed mode. | Low -- the app already supports this mode |
| **PyPI** | Package the application as a pip-installable module. `pyproject.toml` already exists. Need an entry point script that starts the FastAPI server. | Low |
| **CloudFormation** | Template that provisions: Lambda function, Lambda Function URL, S3 bucket, IAM roles. Current deployment is manual -- this codifies it. | Medium -- the resource definitions exist implicitly in our manual setup |
| **Terraform** | Equivalent to CloudFormation but in HCL. Same resources. | Medium |
| **GCP Cloud Run** | Docker image deployed to Cloud Run. The FastAPI/Uvicorn path works directly. Storage backend changes from S3 to GCS (or remains in-memory for ephemeral deployments). | Medium -- need GCS storage backend |
| **EC2 AMI** | Pre-configured Amazon Linux image with the application installed. Runs Uvicorn as a systemd service. | Low -- standard AMI creation |

### 2.2 Architecture Changes for Multi-Target Deployment

The application already has the correct abstraction for multi-target deployment. The `Storage_FS` layer means application code never touches S3 directly. The `Serverless__Fast_API` base class from `osbot-fast-api-serverless` handles both Lambda (via Mangum) and direct HTTP (via Uvicorn).

**What needs to change:**

1. **Storage backend configuration.** Currently, the storage backend (memory, disk, S3) is configured implicitly. For multi-target deployment, we need explicit configuration:
   ```
   SGRAPH_STORAGE_BACKEND=memory    # Lambda (ephemeral), testing
   SGRAPH_STORAGE_BACKEND=s3        # Lambda (persistent), production
   SGRAPH_STORAGE_BACKEND=disk      # Docker, EC2, local dev
   SGRAPH_STORAGE_BACKEND=gcs       # GCP Cloud Run
   ```

   The `Storage_FS` abstraction supports this. We need a `Storage_FS__GCS` implementation for GCP (following the pattern of `Storage_FS__S3`). This is a new class, not a change to existing code.

2. **Configuration via environment variables.** All deployment-specific settings (storage backend, S3 bucket name, admin API key, etc.) must be configurable via environment variables. Most already are. Need to audit and document the full set.

3. **Health check endpoint.** The health endpoint (`/health`) already exists. Docker and Cloud Run health checks can use it directly. No change needed.

4. **Entry point script.** For PyPI and Docker, a `sgraph-send` CLI entry point that starts the FastAPI server:
   ```bash
   sgraph-send serve --port 8080 --storage disk --data-dir /var/sgraph-send
   ```

### 2.3 Build Configuration for Code-Level Feature Exclusion

The daily brief introduces a security principle: for sensitive features (mouse tracking, admin functions, debug endpoints), the code itself should be excluded from certain builds, not just toggled off at runtime.

**Architectural approach:** The IFD model already supports this. Different IFD versions can include or exclude components. For the backend:

1. **Lambda function separation.** Already done -- Public Lambda and Admin Lambda are separate functions. Admin code does not exist in the Public Lambda's deployment package.
2. **UI build variants.** The IFD versioning system can produce different bundles. A "public" build excludes admin components, debug tools, and mouse tracking code. A "beta" build includes feature flag UI and experimental components.
3. **Implementation:** The build step (which packages static assets for deployment) filters files based on a build manifest. The manifest defines which components are included in each variant.

This is a Villager-track concern (production build configuration). The Explorer does not need to implement it for prototyping, but the Architect records the requirement.

---

## 3. Accountant Role -- Financial Data Flows

**Source:** [`v0.4.10__brief__accountant-role-cost-control-financial-flows.md`](../../../humans/dinis_cruz/briefs/02/18/v0.4.10__brief__accountant-role-cost-control-financial-flows.md)

### 3.1 Architectural Implications

The Accountant role needs data. Specifically:

| Data Need | Source | Architectural Impact |
|-----------|--------|---------------------|
| Token cost (LLM usage) | Claude API / session metadata | None -- external to the application |
| AWS compute cost | AWS Cost Explorer API | New integration: `osbot-aws` wrapper for Cost Explorer |
| Transfer metrics | Existing SA (Server Analytics) pipeline | None -- SA pipeline already captures transfer events |
| Storage costs | S3 metrics via CloudWatch | Covered by v0.3.6 observability pipeline |
| Cost-per-user, cost-per-transfer | Derived from transfer metrics + AWS costs | New derivation logic, runs in admin Lambda |
| Cost spike detection | CloudWatch alarms or custom analysis | New: cost anomaly detection rules |

### 3.2 Cost Data Pipeline

The existing SA pipeline captures traffic events (requests, transfers, downloads). The v0.3.6 observability pipeline design adds CloudWatch metrics ingestion. The Accountant needs a cost overlay on top of this:

```
SA Pipeline (existing)
  --> Request events, transfer events, download events
  --> Aggregated into hourly/daily summaries

Observability Pipeline (v0.3.6, designed)
  --> Lambda invocation metrics, S3 operation metrics, CloudFront metrics

Cost Pipeline (new -- Accountant)
  --> AWS Cost Explorer data (daily cost breakdown by service)
  --> Joins cost data with usage metrics
  --> Produces: cost per transfer, cost per user session, cost per GB stored
  --> Alerts: cost spike detection (cost exceeds 2x rolling average)
```

**Architectural change needed:** A new `Routes__Costs` endpoint in the admin Lambda that fetches from AWS Cost Explorer (via `osbot-aws`), joins with SA metrics, and returns cost-per-unit metrics. This follows the same pattern as `Routes__Analytics` and `Routes__Metrics`.

**Note on `osbot-aws`:** The brief is clear -- never use `boto3` directly. All AWS calls go through `osbot-aws`. The Cost Explorer integration needs an `osbot-aws` wrapper (similar to the existing S3 and Lambda wrappers). If this wrapper does not yet exist, it needs to be created as an `osbot-aws` contribution.

### 3.3 Internal Credit System

The human's concept of internal micro-transactions (credits flowing to shared components that enable value) is a strategic framework, not an immediate architectural need. The Architect records this as a future system design task. When the credit system matures, it will likely be tracked in Issues FS (credits as issues/transactions against component identifiers).

---

## 4. UX Observations -- The "What's Next?" Problem

**Source:** [`v0.4.10__brief__ux-observations-whats-next-problem.md`](../../../humans/dinis_cruz/briefs/02/18/v0.4.10__brief__ux-observations-whats-next-problem.md)

### 4.1 Architectural Notes

Most UX observations are Designer/Developer concerns, not architectural. The two items with architectural implications:

#### 4.1.1 SPA Deep Linking and Routing Within IFD

The brief requests converting to a single-page application (SPA) with path-based routing. This was already identified as task A1 in the 15 Feb action plan (`v0.3.12__action-plan__brief-response-15-feb.md`), specifically AD-7 (SA Dashboard Routing Approach).

The same routing architecture applies to the user UI:
- A shell component reads `window.location.pathname` and loads the correct view
- Lambda/CloudFront serves the shell HTML for all matching paths (catch-all route)
- URL stays in sync with user state

**For the user-facing UI, the routing is simpler than the admin dashboard:**

```
send.sgraph.ai/                     --> Shell loads send view
send.sgraph.ai/pt-br/               --> Shell loads send view with locale=pt-BR
send.sgraph.ai/download/{id}        --> Shell loads download view
send.sgraph.ai/channel/{channel_id} --> Shell loads chat view (future)
```

The Lambda Function URL needs a catch-all route that serves `index.html` for all paths that do not match an API endpoint or static asset. FastAPI can handle this with a wildcard route at the lowest priority.

**Decision needed (AD-18):** Should user UI and admin UI share the same SPA shell, or remain separate applications? The Architect recommends **separate shells** -- they serve different audiences, different security contexts, and different deployment units. The admin Lambda should not serve user-facing pages and vice versa.

#### 4.1.2 Feature Flag System

The localStorage-based feature flag system described in the brief is architecturally simple. It is purely a client-side concern:

```javascript
// flags stored in localStorage, overridable via URL params
getFlag('ui.post-encrypt-v2')  --> true/false
```

**Architectural note:** Feature flags should NOT be used for security-critical toggles. If a feature must be excluded for security reasons (mouse tracking, admin functions), use build-time exclusion, not runtime flags. Flags are for UX experiments, not security boundaries.

#### 4.1.3 Web Share API and Send Facilitation

The brief asks about deep links (`whatsapp://`, `sms:`, Web Share API). Assessment:

| Mechanism | Desktop Support | Mobile Support | Recommendation |
|-----------|----------------|----------------|----------------|
| Web Share API | Chrome 89+, Edge 93+, no Firefox | Excellent | Use where available, with fallback |
| `sms:` URI | Limited | Good (iOS, Android) | Include for mobile users |
| `whatsapp://send?text=` | If WhatsApp Desktop installed | Excellent | Include for WhatsApp users |
| `mailto:` | Excellent | Excellent | Include as default |

**Implementation:** A `<send-share>` Web Component that detects `navigator.share` availability and renders either the native share sheet or a fallback with specific channel buttons.

---

## 5. Post-Quantum Cryptography

**Source:** [`v0.4.11__brief__post-quantum-cryptography-research.md`](../../../humans/dinis_cruz/briefs/02/19/v0.4.11__brief__post-quantum-cryptography-research.md)

### 5.1 Architectural Assessment

The brief's nuance is correct and important: **AES-256-GCM (our current symmetric encryption) is relatively quantum-resistant.** Grover's algorithm reduces effective key strength by half, making AES-256 equivalent to AES-128 against a quantum attacker. AES-128 is still considered secure by all major standards bodies.

**The real PQC risk is when we add asymmetric cryptography (PKI).** RSA and ECC are completely broken by Shor's algorithm. Our roadmap includes PKI for:
- User-defined encryption keys (User B's request)
- Certificate-based encryption (User B's request)
- Shared keys for chat channels (the encrypted chat proposal)
- Key exchange for interactive workflows

### 5.2 Migration Path

The Architect recommends a three-phase approach:

**Phase 1 (current): Symmetric only, AES-256-GCM.**
- Quantum risk: Low. AES-256 remains effectively AES-128 against Grover's algorithm.
- No action needed beyond documentation of the quantum posture.

**Phase 2 (when PKI is introduced): Hybrid classical + PQC.**
- Use CRYSTALS-Kyber (ML-KEM, NIST FIPS 203) for key encapsulation alongside X25519 (classical ECDH).
- The hybrid model: `shared_secret = KDF(X25519_shared || Kyber_shared)`. If either algorithm is broken, the other still protects.
- Use CRYSTALS-Dilithium (ML-DSA, NIST FIPS 204) for digital signatures if/when we need signed transfers.
- Browser availability: As of early 2026, Web Crypto API does NOT natively support PQC algorithms. Implementation requires JavaScript or WASM libraries.

**Phase 3 (when browser-native PQC is available): Pure PQC.**
- Transition from hybrid to PQC-only when Web Crypto API adds support.
- Timeline: likely 2027--2028 based on browser vendor roadmaps.

### 5.3 Browser-Based PQC Library Assessment

Research tasks for Developer (to verify):

| Library | Language | Key Encapsulation | Signatures | Size | Notes |
|---------|----------|-------------------|------------|------|-------|
| **liboqs-js** | WASM (compiled from C) | Kyber-512/768/1024 | Dilithium2/3/5 | ~500KB WASM | Most complete, but large bundle size |
| **pqc-js** | Pure JS | Kyber variants | Dilithium variants | ~200KB | Slower than WASM but no WASM dependency |
| **crystals-kyber** (npm) | JS/WASM | Kyber-768 | No signatures | ~100KB | Focused, smaller |

**Performance concern:** PQC key generation is significantly slower than classical. Kyber-768 key generation takes ~1ms in WASM (acceptable), but Dilithium signature generation takes ~5ms. These numbers need verification in browser context with real-world devices (mobile phones).

**Ciphertext size concern:** Kyber-768 ciphertext is ~1,088 bytes (vs. 32 bytes for X25519). This overhead is negligible for file transfers but matters for the SGMETA header format. The metadata structure needs to accommodate larger key encapsulation values.

### 5.4 Architect Recommendation

Do NOT implement PQC now. The current AES-256-GCM symmetric model is adequate. When PKI is introduced (which is on the roadmap), design it with hybrid PQC from the start rather than retrofitting. Produce a one-page PQC position statement for investor/customer conversations stating:

1. Our symmetric encryption (AES-256-GCM) has effective 128-bit quantum security.
2. When we add asymmetric key exchange, we will use NIST-standardised PQC algorithms in hybrid mode.
3. We are tracking browser vendor PQC support for native integration.

---

## 6. Surf Security Browser Integration

**Source:** [`v0.4.11__brief__surf-security-browser-integration.md`](../../../humans/dinis_cruz/briefs/02/19/v0.4.11__brief__surf-security-browser-integration.md)

### 6.1 Architectural Assessment

This is architecturally significant because it redefines the trust boundary. Currently, SGraph Send treats the browser as an untrusted environment -- we defend against it (CSP, no localStorage secrets, clear key from URL). A secure browser integration flips this: the browser becomes a trusted security control plane.

### 6.2 Integration Architecture: Detection Layer

The first architectural requirement is **detecting whether SGraph Send is running inside a secure browser.** This creates a branching point in the client-side code:

```
Page loads
  |
  +--> Detect browser environment
  |      |
  |      +--> Normal browser: standard security model
  |      |      - CSP headers
  |      |      - sessionStorage for temporary data
  |      |      - Key cleared from URL immediately
  |      |      - Heuristic security checklist (best effort)
  |      |
  |      +--> Secure browser detected: enhanced security model
  |             - Browser-managed key storage (if API available)
  |             - DLP enforcement (copy-paste prevention)
  |             - Screenshot prevention
  |             - Device posture check before decryption
  |             - "Secure environment" badge in UI
  |
  +--> Proceed with encryption/decryption
```

### 6.3 Detection Mechanisms

| Method | Reliability | Privacy | Notes |
|--------|-------------|---------|-------|
| **User-Agent string** | Low -- can be spoofed | Non-invasive | Surf likely has a distinctive UA |
| **Custom HTTP header** | Medium | Non-invasive | Secure browsers can inject headers; SG/Send server checks and reflects to client |
| **Browser API probe** | High | Non-invasive | If Surf exposes a JavaScript API (`window.surfSecurity` or similar), we can detect it reliably |
| **Extension message** | High | Requires extension | A Surf-specific extension could establish a trusted channel with SG/Send pages |

**Recommendation:** Design the detection as an interface:

```javascript
class BrowserSecurityProvider {
    async detect()                    // returns { isSecure: bool, provider: string, capabilities: [] }
    async getDevicePosture()          // returns { compliant: bool, details: {} }
    async requestSecureStorage(key)   // returns secure storage handle
    async isNetworkTrusted()          // returns { trusted: bool, details: {} }
}
```

Concrete implementations (`SurfSecurityProvider`, `IslandSecurityProvider`, `DefaultBrowserProvider`) implement this interface. The application code never checks for a specific browser -- it queries capabilities.

### 6.4 Feasibility Assessment of the Six Integration Ideas

| # | Idea | Feasibility | Dependency |
|---|------|-------------|------------|
| 1 | Browser-managed key storage | **Likely feasible** -- secure browsers typically expose secure storage APIs to managed web apps | Research: Surf's developer API docs |
| 2 | Public/private key management | **Possible** -- depends on whether Surf exposes certificate store access to web apps | Research: Web Authentication API + Surf extensions |
| 3 | Links that only work in secure browser | **Feasible** -- detection + conditional decryption. Architecturally simple: if `!securityProvider.isSecure()`, show "requires secure browser" | No external dependency |
| 4 | Compromised device/network detection | **Feasible if API exists** -- depends entirely on Surf's device posture API | Research: Surf's posture assessment endpoint |
| 5 | Self-expiring with enforcement | **Partially feasible** -- Surf can prevent downloads and screenshots but cannot prevent memory inspection or screen recording via external tools | DLP policy configuration |
| 6 | Secure copy-paste between SG/Send tabs | **Feasible** -- Surf's DLP can whitelist copy-paste between same-domain tabs while blocking external paste targets | DLP policy configuration |

**The Architect's key insight:** Ideas 1, 3, and 4 are application-level integrations (our code calls Surf's API). Ideas 2, 5, and 6 are policy-level integrations (Surf's admin configures DLP policies for the SG/Send domain). We need both, but they have different implementation paths.

### 6.5 Connection to Chrome Extension Detection

The 17 Feb security review brief proposed a browser security checklist with heuristic detection (extensions, HTTPS, proxy). The secure browser integration is the enterprise-grade version of the same concept:

| Feature | Normal Browser | Secure Browser |
|---------|---------------|----------------|
| Extension safety | Heuristic (limited by browser sandboxing) | Guaranteed (managed browser controls extensions) |
| Network integrity | Cannot verify (TLS termination is invisible) | Verified (browser reports network posture) |
| Screenshot prevention | Impossible | Enforced by DLP policy |
| Key storage security | sessionStorage (accessible to page scripts) | Secure enclave (isolated from page scripts) |

**Architectural recommendation:** Build the `BrowserSecurityProvider` interface now (it is lightweight -- a few abstract methods). Implement the `DefaultBrowserProvider` with heuristic checks (HTTPS verification, basic security indicators). Implement `SurfSecurityProvider` when Surf's API documentation is available. This way, the application code is ready for secure browser integration without being blocked by the research.

---

## 7. UI Bugs and Improvements

**Source:** [`v0.4.11__brief__ui-bugs-and-improvements.md`](../../../humans/dinis_cruz/briefs/02/19/v0.4.11__brief__ui-bugs-and-improvements.md)

### 7.1 Architectural Notes

Most UI items are Developer/Designer tasks. Architectural notes on items with broader implications:

#### 7.1.1 In-Browser File Rendering (Images and PDFs)

Rendering untrusted content after decryption has XSS implications. The Architect's recommendation, consistent with the v0.4.4 assessment:

**Use sandboxed iframes for all content rendering.**

```html
<iframe sandbox="allow-same-origin" srcdoc="..."></iframe>
```

- **Images:** Create a blob URL (`URL.createObjectURL(blob)`) from the decrypted bytes. Render inside an `<img>` tag within the sandboxed iframe. The blob URL is origin-scoped and cannot be accessed from outside the iframe.
- **PDFs:** Use PDF.js (Mozilla's JavaScript PDF renderer) inside a sandboxed iframe. PDF.js renders PDF content as canvas elements, neutralising any JavaScript embedded in the PDF.
- **Markdown:** As already recommended in the 17 Feb brief -- mini markdown parser with safe subset, rendered inside a sandboxed iframe.

**The pattern is consistent:** all post-decryption rendering goes through a sandboxed iframe. This is the architectural standard for content display.

#### 7.1.2 Delete Button Architecture

Both sender and recipient can delete. This requires:

1. **Delete API endpoint:** `DELETE /transfers/{id}` -- removes the encrypted payload and metadata from storage.
2. **Sender authentication:** The sender receives a `delete_secret` (random token) at upload time. The hash of this secret is stored in the transfer metadata. To delete, the sender presents the secret; the server verifies `SHA-256(presented_secret) == stored_hash`.
3. **Recipient authentication:** The recipient knows the transfer_id (from the URL). Knowledge of the transfer_id could be sufficient for recipient deletion, OR we could require the decryption key hash as proof of authorisation.
4. **Audit trail:** Deletion events should be logged (who deleted, when) even though the payload is removed.
5. **Soft vs hard delete:** The Architect recommends soft delete initially (mark as `DELETED`, remove payload, keep metadata for audit) with a scheduled cleanup process that hard-deletes after a retention period.

**Storage model change:**
```
Transfer metadata gains:
  - delete_secret_hash: str (SHA-256 of sender's delete secret)
  - status: DELETED (new status value)
  - deleted_at: str (timestamp)
  - deleted_by: str ("sender" or "recipient")
```

#### 7.1.3 Test File Drag-and-Drop from UI

Architecturally trivial. Client-side only. The HTML5 Drag and Drop API supports creating `File` objects from in-page elements via `DataTransfer`. The test files can be small static assets or generated client-side (e.g., a small PNG, a sample text file, a tiny PDF).

---

## 8. Chaos Agent, Runbooks, and Fire Drills

**Source:** [`v0.4.11__brief__chaos-agent-runbooks-fire-drills.md`](../../../humans/dinis_cruz/briefs/02/19/v0.4.11__brief__chaos-agent-runbooks-fire-drills.md)

### 8.1 Architectural Implications

The Chaos Agent concept has two architectural requirements:

1. **Environment isolation.** The Chaos Agent must never run against production without explicit authorisation. This requires clearly separated environments with independent infrastructure:
   ```
   Production:  Lambda (prod), S3 (prod), CloudFront (prod)
   QA/Staging:  Lambda (qa), S3 (qa), CloudFront (qa)
   Chaos:       Lambda (chaos), S3 (chaos), CloudFront (chaos)
   ```

   **Current state:** Environment separation (AD-1 from the v0.3.2 action plan) is still an open decision. The Chaos Agent requirement adds urgency to this decision. We need at least two distinct environments (production and non-production) before the Chaos Agent can be introduced.

2. **Observability as a prerequisite.** The Chaos Agent is useless without monitoring -- you need to detect what the chaos caused. The v0.3.6 observability pipeline design addresses this. It must be implemented before the Chaos Agent starts.

### 8.2 Runbook Architecture

Runbooks are documentation, not code. However, the P0 "pull the plug" runbook has an architectural component:

**"Turn it off" procedure -- what to disable and in what order:**

1. **CloudFront distribution:** Disable the distribution (takes ~15 minutes to propagate). This stops all public access.
2. **Lambda Function URLs:** Delete the function URL configurations. This immediately stops all direct API access.
3. **Lambda functions:** Set reserved concurrency to 0. This prevents any invocation.
4. **S3 buckets:** Apply a deny-all bucket policy. This prevents data access even if someone has IAM credentials.

**The reverse order for "turn it back on"** is: S3 policy restore, Lambda concurrency restore, Lambda URL recreate, CloudFront enable.

This procedure should be scripted (a single `sgraph-send-emergency-stop.sh` script) and tested in the first fire drill.

### 8.3 Load Testing Architecture

Load testing requires a load generation tool. For Lambda-based architectures:

| Tool | Notes |
|------|-------|
| **Artillery** | Node.js-based, supports Lambda targets, good for HTTP load testing |
| **Locust** | Python-based, fits our stack, scriptable scenarios |
| **k6** | Go-based, high performance, JavaScript scripting |

**Recommendation:** Use Locust (Python, consistent with our stack). Define test scenarios that mirror real usage: create transfer, upload payload, download payload, complete transfer. Measure: Lambda cold start latency, throughput (transfers per second), error rate under load, cost per 1000 transfers.

---

## 9. Guest / Visitor Programme

**Source:** [`v0.4.11__brief__guest-visitor-programme.md`](../../../humans/dinis_cruz/briefs/02/19/v0.4.11__brief__guest-visitor-programme.md)

### 9.1 Architectural Implications

The visitor programme is primarily a content and documentation initiative. Architectural items:

#### 9.1.1 OpenAI GPT with Live Repo Access

The brief asks whether an OpenAI GPT can be configured to pull from a GitHub repo at conversation start. Assessment:

| Approach | Freshness | Feasibility |
|----------|-----------|-------------|
| **GPT with Actions (function calling) that calls GitHub API** | Real-time | Medium -- requires a proxy API endpoint (GitHub's API has rate limits, CORS restrictions). We could deploy a small Lambda function that serves as the GPT's action endpoint, fetching files from the repo on demand. |
| **GPT with file uploads (manual)** | Stale | Low complexity but poor UX |
| **Custom GPT with web browsing enabled** | Near-real-time | GPT can browse the public GitHub repo directly. Depends on OpenAI's browsing capabilities for code files. |

**Recommended approach:** A thin API endpoint (Lambda or Cloudflare Worker) that serves as a GPT Action backend:

```
GET /api/visitor/files?path=team/roles/librarian/ROLE.md
  --> Returns the file content from the GitHub repo (with caching)

GET /api/visitor/tree
  --> Returns the repo directory structure
```

This endpoint is stateless, cacheable (5-minute TTL), and costs near-zero to operate. The GPT calls these actions to fetch the latest files on demand.

**Alternative:** If the goal is simpler, a nightly GitHub Action that generates a "project snapshot" file (concatenation of key files with table of contents) and uploads it to a public URL. The GPT is configured to fetch this snapshot at conversation start. Less fresh (daily), but zero API infrastructure.

### 9.2 Visitor Permissions

Visitors are read-only. They cannot commit, push, or modify the repo. They interact with the repo's knowledge through their own LLM sessions. No changes to the application architecture are needed.

---

## 10. Industry Dinner Debrief

**Source:** [`v0.4.11__brief__industry-dinner-debrief-and-validation.md`](../../../humans/dinis_cruz/briefs/02/19/v0.4.11__brief__industry-dinner-debrief-and-validation.md)

### 10.1 Architectural Observations

The dinner debrief is primarily validation data (product works, tagline resonates, cards create engagement). The one architectural item:

**Broken images on GitHub page.** This is a content issue, not an architectural one. However, it highlights that the GitHub README is a customer-facing surface that should be treated as part of the product's deployment -- updated with each release.

**The Dropbox encryption comparison** is a messaging asset, not an architectural concern. The Architect notes that our architecture is fundamentally different from Dropbox's: we perform client-side encryption before upload (the server never sees plaintext), while Dropbox performs server-side encryption at rest (Dropbox holds the keys). This distinction is the core value proposition.

---

## 11. Security Review (17 Feb) -- Architect-Relevant Items

**Source:** [`v0.4.7__daily-brief__all-teams-17-feb-2026.md`](../../../humans/dinis_cruz/briefs/02/17/v0.4.7__daily-brief__all-teams-17-feb-2026.md)

### 11.1 Token Race Condition (P3 #1)

This is the finding most relevant to the Architect. The current token validation uses a non-atomic read-modify-write pattern:

```
1. Read token usage count from storage
2. Check if count < max_uses
3. Increment count
4. Write updated count back to storage
```

In Lambda, concurrent invocations can read the same count, both pass the check, and both increment -- resulting in usage beyond the limit.

**Fix options:**

| Option | Mechanism | Storage Backend |
|--------|-----------|-----------------|
| **DynamoDB atomic counter** | `UpdateItem` with `ADD` operation | Requires DynamoDB table |
| **S3 conditional write** | `PutObject` with `If-None-Match` (ETag-based optimistic locking) | Works with S3 |
| **Memory-FS locking** | Add a compare-and-swap primitive to Storage_FS | In-memory only (not distributed) |

**Recommendation:** For the current Memory-FS abstraction, add a `compare_and_swap(path, expected_value, new_value) -> bool` method to `Storage_FS`. This is the minimum viable fix. For production (S3 backend), use S3 conditional writes or introduce DynamoDB for token state.

**Important:** This fix must preserve the Memory-FS abstraction. Application code should call `storage.compare_and_swap()`, not `dynamodb.update_item()`. The storage backend implementation handles the atomic operation.

### 11.2 Upload/Complete Race Condition (P5 #23)

Latent with in-memory storage but exploitable with S3. The same compare-and-swap primitive fixes this: the `complete` operation should only succeed if the transfer status is currently `PENDING`.

### 11.3 Pre-Signed URL Size Constraints (P4 #3, Research)

When we move to direct S3 uploads via pre-signed URLs, can we enforce max upload size?

**Answer: Yes.** S3 pre-signed POST policies support a `content-length-range` condition:

```json
{
  "conditions": [
    ["content-length-range", 0, 5242880]  // max 5MB
  ]
}
```

S3 rejects uploads that exceed the specified size. This is enforced server-side by S3, not by the client. It is the correct place to enforce upload limits when moving away from Lambda-proxied uploads.

**For pre-signed PUT URLs** (simpler but less flexible), the `Content-Length` header is part of the signing. The signed URL is only valid for the exact content length. This means the client must know the exact size before requesting the pre-signed URL, which is fine for our use case (the client knows the file size after encryption).

---

## 12. Carry-Forward Status

### Previous Architect Decisions Awaiting Response

| Decision | From | Status |
|----------|------|--------|
| AD-1: Environment separation | v0.3.2 | **Open** -- now more urgent due to Chaos Agent requirement |
| AD-2: Handover brief format | v0.3.2 | Open -- Villager track |
| AD-3: Branching strategy | v0.3.2 | Open |
| AD-4: Large file transfer | v0.3.2 | Subsumed by A4 research (15 Feb) |
| AD-5: Unencrypted mode | v0.3.2 | Open |
| AD-6: External partner communication | v0.3.2 | Open |
| AD-7: SA Dashboard routing | v0.3.12 | Open -- still blocking Dev |
| AD-8: Cache token data structure | v0.3.12 | Awaiting mapping |
| AD-9: IFD manifest format | v0.3.12 | Open |
| AD-10: Dashboard debug panels | v0.3.12 | Open -- coupled with AD-7 |
| AD-11 through AD-16: Interactive workflow decisions | v0.4.4 | Awaiting human approval |

### New Decisions from This Response

| Decision | Description | Options | Recommendation |
|----------|-------------|---------|----------------|
| **AD-17** | Enterprise key escrow | Multi-key encryption mode | **Parked** (explicit human decision) |
| **AD-18** | User UI vs Admin UI SPA shells | Shared shell vs separate | **Separate shells** |
| **AD-19** | Chat channel ID derivation | SHA-256(key) vs random ID | **SHA-256(key)** for simplicity |
| **AD-20** | Chat signalling mechanism | Polling vs SSE vs WebSocket | **Polling first**, WebSocket via API Gateway later |
| **AD-21** | PQC implementation timing | Now vs when PKI introduced vs never | **When PKI is introduced**, hybrid mode |
| **AD-22** | Secure browser detection interface | Specific browser check vs capability interface | **Capability interface** (BrowserSecurityProvider) |
| **AD-23** | Delete model | Hard delete vs soft delete | **Soft delete** with scheduled cleanup |
| **AD-24** | Atomic token operations | DynamoDB vs S3 conditional vs Memory-FS CAS | **Memory-FS compare_and_swap** abstraction |

---

## 13. Prioritised Architectural Work Items

Ordered by urgency and blocking impact:

| # | Work Item | Priority | Blocks | Est. Effort | Related Brief |
|---|-----------|----------|--------|-------------|---------------|
| **W1** | Complete A2: Cache service token usage mapping | **P1** | Dev: token fetch performance | 1 session | 15 Feb carry-forward |
| **W2** | Complete A1: Path-based routing design (user UI + admin UI) | **P1** | Dev: SA dashboard, SPA deep linking | 1--2 sessions | 15 Feb carry-forward + 18 Feb UX brief |
| **W3** | Design Channel abstraction for encrypted chat | **P2** | Dev: chat feature | 1 session | 18 Feb encrypted chat |
| **W4** | Design compare_and_swap primitive for Storage_FS | **P2** | Dev: token race condition fix (P3 #1) | 0.5 session | 17 Feb security review |
| **W5** | Design delete API and delete_secret pattern | **P2** | Dev: delete button feature | 0.5 session | 19 Feb UI improvements |
| **W6** | Design BrowserSecurityProvider interface | **P3** | Dev: secure browser detection | 0.5 session | 19 Feb Surf Security |
| **W7** | PQC position statement and migration path | **P3** | Ambassador: investor conversations | 0.5 session | 19 Feb PQC |
| **W8** | Research OpenAI GPT with live repo access | **P3** | Advocate: visitor programme | 0.5 session | 19 Feb visitor programme |
| **W9** | Document "turn it off" procedure (architecture component) | **P3** | GRC: P0 runbook | 0.5 session | 19 Feb chaos/runbooks |
| **W10** | Design Storage_FS__GCS for GCP Cloud Run deployment | **P4** | DevOps: GCP deployment | 1 session | 18 Feb partner ecosystem |
| **W11** | Research Surf Security API/capabilities | **P4** | Ambassador: partnership concept | 1 session | 19 Feb Surf Security |
| **W12** | Design Cost Explorer integration via osbot-aws | **P4** | Accountant: cost tracking | 0.5 session | 18 Feb Accountant |
| **W13** | Research chunked streaming for chat (download before upload completes) | **P4** | Future: large file chat | 1--2 sessions | 18 Feb encrypted chat |

### Execution Order

```
Immediate (carry-forward blockers):
  W1: Cache mapping                    --> Unblocks Dev
  W2: Path-based routing               --> Unblocks Dev + SPA

Next (new design work):
  W3: Channel abstraction              --> Enables chat feature
  W4: Compare-and-swap primitive       --> Fixes P3 race conditions
  W5: Delete API design                --> Enables delete feature

Then (research and position):
  W6: BrowserSecurityProvider          --> Enables secure browser path
  W7: PQC position statement           --> Enables investor conversations
  W8: OpenAI GPT research              --> Enables visitor programme
  W9: "Turn it off" architecture       --> Enables P0 runbook

Later (lower priority):
  W10: GCS storage backend             --> Enables GCP deployment
  W11: Surf Security API research      --> Enables partnership
  W12: Cost Explorer integration       --> Enables financial tracking
  W13: Chunked streaming research      --> Enables large file chat
```

---

## 14. Questions for Human

1. **AD-19 (Chat channel ID):** Confirm that `SHA-256(key)` as the channel ID is acceptable. The server learns which transfers are grouped (same channel) but cannot derive the key. Is this acceptable for the zero-knowledge model?

2. **AD-20 (Signalling):** Confirm that polling (2--5 second interval) is acceptable for the chat MVP. WebSocket via API Gateway is a Phase 2 enhancement.

3. **AD-23 (Delete):** Should deletion be available to both sender AND recipient, or sender only? Recipient deletion means anyone with the download link can destroy the data.

4. **AD-18 (SPA shells):** Confirm separate SPA shells for user UI and admin UI. This is cleaner but means two routing implementations.

5. **Channel and workflows:** Should the encrypted chat feature be prioritised above or below the interactive encrypted workflows (v0.4.4)? They are complementary but independent. The chat is simpler and has direct user validation. The workflow is more architecturally complex but has a concrete use case (translation review). The Architect recommends chat first, then workflows within chat channels.

6. **Environment separation (AD-1):** The Chaos Agent requirement makes this urgent. Can we get a decision on environment separation? At minimum, we need a non-production environment where the Chaos Agent and load testing can run.

---

*Architect assessment complete. This document covers 13 briefs across three days, identifies 8 new architectural decisions (AD-17 through AD-24), and prioritises 13 work items. The critical path remains: W1 (cache mapping) then W2 (routing), followed by W3 (channel abstraction for chat). Awaiting human decisions on AD-19 through AD-24 before proceeding with new design work.*

---

This document is released under the Creative Commons Attribution 4.0 International licence (CC BY 4.0). You are free to share and adapt this material for any purpose, including commercially, as long as you give appropriate credit.
