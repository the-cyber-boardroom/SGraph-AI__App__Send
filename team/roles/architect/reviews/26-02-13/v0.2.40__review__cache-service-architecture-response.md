# v0.2.40 -- Architect Review: Cache Service Architecture Response

**Version:** v0.2.40
**Date:** 13 February 2026
**Role:** Architect
**Context:** Response to `team/humans/dinis_cruz/briefs/02/13/v0.2.40__brief__cache-service-architecture.md`
**Supersedes:** Cache service design sections of `v0.2.33__response-to-daily-brief-2__13-feb.md` (Schemas A-D implementation approach, not data models)
**Status:** APPROVED with notes

---

## 1. What Was Superseded and Why the New Approach Is Better

My v0.2.33 review designed a cache service to be built from scratch: `Cache__Service__Send` with custom `Storage_FS` path generation, custom hash-to-ID resolution with sharded directories (`refs/by-hash/a3f7/b2c1/9d4e8f01.json`), and a manually-built namespace management layer. The estimate was 25-35 hours of implementation across 8 tasks.

The brief reveals that the **MGraph-AI Cache Service already exists** as a production PyPI package (`mgraph-ai-service-cache` v0.6.0+, `mgraph-ai-service-cache-client` v0.10.1) with every capability I specified -- and more. The capabilities I designed custom solutions for are already first-class features:

| What I Designed Custom | What Already Exists |
|---|---|
| Custom `Storage_FS` path generation per namespace | Automatic path resolution via 5 storage strategies |
| Sharded directory hash-to-ID resolution (`refs/by-hash/a3f7/...`) | Built-in content-addressable storage with automatic deduplication |
| Custom namespace directory creation | First-class namespace parameter on every API call |
| Custom temporal file writing with date paths | `TEMPORAL` strategy: automatic `year/month/day/hour` filing |
| Custom "latest" pointer management | `TEMPORAL_LATEST` strategy with overwrite semantics |
| Custom `Cache__Service__Send` class (2-3h scaffolding) | `Cache__Service__Client` with `IN_MEMORY` mode (zero-network, same process) |

**The new approach is better for three reasons:**

1. **Effort reduction.** 25-35 hours drops to ~8-12 hours. The wrapper (`Send__Cache__Client`) follows a proven pattern (`Html_Cache__Client`) and the only substantial custom code is the LETS aggregation pipeline (~200-300 lines).

2. **Proven infrastructure.** The cache service is already deployed at `cache.dev.mgraph.ai` and used in production by the MGraph-AI HTML service. We inherit battle-tested storage strategy logic, hash generation, and content-addressable storage rather than reimplementing them.

3. **Clean extraction path.** `IN_MEMORY` mode today, `REMOTE` mode later -- the switch is a configuration change. My v0.2.33 design had no such extraction path because it was a monolithic layer inside the application.

I should have asked about existing MGraph-AI services before designing from scratch. The brief correctly identifies this as the right corrective.

---

## 2. Decision Status: D024-D030

### Decisions Updated (D024-D027)

| # | Original Decision (v0.2.33) | Updated Decision (v0.2.40) | Architect Assessment |
|---|---|---|---|
| **D024** | Cache service built as custom layer on Memory-FS (`Cache__Service__Send`) | Use existing MGraph-AI Cache Service via `Cache__Service__Client` in `IN_MEMORY` mode | **APPROVE.** Strictly better -- same Memory-FS foundation, production-tested code, thin wrapper instead of custom build. |
| **D025** | Import and manually wire `Cache__Hash__Generator` | Already integrated in cache service -- configure `json_field_path` for field-level hashing | **APPROVE.** Less plumbing code. The `json_field_path` parameter on `store__json__cache_key` eliminates custom hash-setup code. |
| **D026** | Custom directory creation for namespace separation | First-class cache service feature -- namespace parameter on every API call | **APPROVE.** Namespaces are built into the service API. No custom directory scaffolding needed. |
| **D027** | Custom LETS pipeline on Storage_FS with manual path construction | Custom LETS pipeline on top of cache service API -- storage handled by service, compute logic is ours | **APPROVE.** The pipeline logic is identical. What changes is that reads/writes go through `Cache__Service__Client` methods instead of raw `Storage_FS` path manipulation. This is cleaner. |

### Decisions Unchanged (D028-D030)

| # | Decision | Status | Notes |
|---|---|---|---|
| **D028** | Human-friendly token names resolved via cache hash | **UNCHANGED.** | Maps directly to `KEY_BASED` strategy with `json_field_path='cache_key'`. The brief confirms this is an exact match. |
| **D029** | AWS cost data in its own namespace | **UNCHANGED.** | `costs` namespace, same aggregation structure. |
| **D030** | GA removal approved once pulse + daily aggregation work | **UNCHANGED.** | GA is already removed (v0.1.3). Server-side analytics is now the only path forward. This decision becomes the completion criterion for Phase 1. |

---

## 3. Assessment of D034: IN_MEMORY Mode + S3 Prefix Architecture

### Decision D034 Summary

Cache service runs in-process (`IN_MEMORY` mode via FastAPI `TestClient`). Transfer service gets `s3_prefix="user/"`. Cache service gets `s3_prefix="admin/"`. Both use the same S3 bucket. Prefixes mirror the Lambda architecture (`lambda__user` / `lambda__admin`). Extraction to a separate Lambda is a configuration change (flip to `REMOTE`).

### Architect Recommendation: **APPROVE**

The IN_MEMORY architecture is sound for three reasons:

**3.1 Zero-latency analytics recording.** Every user request writes a raw analytics event. In `REMOTE` mode this would add 50-100ms of HTTP latency to every request. In `IN_MEMORY` mode the `TestClient` call is an in-process function invocation -- microseconds, not milliseconds. At friends-and-family scale, this is the right trade-off. At scale, we flip to `REMOTE` and accept the latency (or use async fire-and-forget).

**3.2 S3 prefix architecture mirrors Lambda boundaries.** The `user/` and `admin/` prefixes are a logical separation that maps to the two-Lambda architecture. If we later deploy the cache service as its own Lambda, the `admin/` prefix data naturally belongs to that Lambda's IAM policy scope. No data migration needed -- just IAM policy updates and a mode switch.

**3.3 Testing is identical to production.** Tests use `Storage_FS__Memory` behind the cache service's `IN_MEMORY` mode. Production uses `Storage_FS__S3` with prefix separation. The application code (`Send__Cache__Client`) is the same in both environments. This is the Memory-FS abstraction working as designed.

### One Concern: Shared Process State

When the cache service runs in-process, it shares the Lambda's memory footprint, event loop, and error domain. A bug in the cache service (e.g., an unhandled exception during analytics recording) could affect the user-facing transfer operation in the same request. **Mitigation:** wrap analytics recording in a try/except at the middleware level -- never let cache service failures propagate to user-facing responses. Analytics data loss is acceptable; transfer failures are not.

---

## 4. Architectural Risks with New Dependencies

### 4.1 Lambda Package Size

The current user Lambda has a lean dependency set:

```
httpx==0.28.1
memory-fs==v0.40.0
osbot-fast-api-serverless==v1.33.0
```

Adding `mgraph-ai-service-cache` and `mgraph-ai-service-cache-client` introduces two new PyPI packages plus their transitive dependencies. The cache service includes a full FastAPI application (strategy handlers, storage abstraction, route setup) that will be bundled into the Lambda even though we only use the `TestClient` path.

**Risk level: Medium.**

**Mitigation:**
- Measure the Lambda package size before and after adding the dependencies
- Monitor cold start times (target: under 3 seconds for Lambda with Function URL)
- If package size exceeds the 50MB zipped Lambda limit, consider: (a) moving to container-based Lambda (250MB limit), or (b) extracting the cache service to its own Lambda sooner than planned
- The cache service client itself is lightweight; the concern is the service package which includes the full FastAPI app

**Action item for Dev/DevOps:** Add package size measurement to the CI pipeline. Report the delta when these dependencies are added.

### 4.2 Cold Start Impact

The cache service FastAPI app (`Cache_Service__Fast_API().setup()`) must initialise during Lambda cold start. This includes setting up strategy handlers, storage backends, and route tables. In `IN_MEMORY` mode the storage backend is memory-based so S3 initialisation is not triggered until the first request that writes through the S3 backend.

**Risk level: Low-Medium.** FastAPI app setup is typically fast (10-50ms). But it adds to a cold start budget that already includes the main application's FastAPI setup.

**Mitigation:** Measure cold start impact explicitly. If it exceeds 500ms added latency, consider lazy initialisation of the cache service (create on first analytics write, not at Lambda startup).

### 4.3 Version Coupling

SGraph Send now depends on specific versions of `mgraph-ai-service-cache` and `mgraph-ai-service-cache-client`. These are actively developed packages -- breaking changes in the cache service API could break SGraph Send.

**Risk level: Low.** Both packages are maintained by the same organisation. The `Html_Cache__Client` pattern we follow is the canonical usage pattern, so breaking changes are unlikely to target it without a migration path.

**Mitigation:**
- Pin exact versions in `APP__SEND__USER__LAMBDA_DEPENDENCIES` (as we do with all other dependencies)
- The `Send__Cache__Client` wrapper insulates application code from cache service API changes -- if the underlying client API changes, only the wrapper needs updating
- Add a smoke test that verifies cache service health check passes with the pinned version

---

## 5. Data Models from v0.2.33 Remain Valid

The brief confirms this explicitly, and I agree. The v0.2.33 schemas describe **what gets stored**, not **how it gets stored**. The following Type_Safe data models from my previous review remain the canonical definitions:

| Schema | Class | Purpose | Status |
|---|---|---|---|
| **A: Analytics** | `Schema__Analytics__Raw_Event` | One file per server-side event | **VALID** -- stored via `TEMPORAL` strategy |
| **A: Analytics** | `Schema__Analytics__Aggregation` | Aggregated analytics for a time window | **VALID** -- stored via `KEY_BASED` strategy |
| **A: Analytics** | `Schema__Analytics__Pulse` | Real-time rolling window snapshot | **VALID** -- computed on-demand, optionally stored via `TEMPORAL_LATEST` |
| **B: Tokens** | `Schema__Token__Metadata` | Token configuration and usage count | **VALID** -- stored via `KEY_BASED` with `json_field_path='cache_key'` on token name |
| **B: Tokens** | `Schema__Token__Usage_Event` | Single token usage event | **VALID** -- stored as child data under token's `cache_id` |
| **C: Costs** | `Schema__Cost__Raw_Entry` | Raw AWS cost data | **VALID** -- stored via `TEMPORAL` |
| **C: Costs** | `Schema__Cost__Aggregation` | Aggregated cost + unit economics | **VALID** -- stored via `KEY_BASED` |
| **D: Transfers** | `Schema__Transfer__Cache__Summary` | Per-transfer analytics summary | **VALID** -- stored as child data under transfer's `cache_id` |

The LETS pipeline logic (`LETS__Pipeline` class) also remains valid in design. The implementation changes from direct `Storage_FS` path manipulation to cache service API calls, but the Load-Extract-Transform-Save flow, the cascading aggregation model, and the on-demand computation with save-when-computed semantics are all unchanged.

---

## 6. Updated Architecture View

### Before (v0.2.33 Design)

```
Lambda Process
|
+-- Fast_API__SGraph__App__Send__User
|   +-- Transfer__Service
|   |   +-- Storage_FS (Memory or S3)
|   |       +-- transfers/{id}/meta.json
|   |       +-- transfers/{id}/payload
|   |
|   +-- Cache__Service__Send  (CUSTOM, 25-35h to build)
|       +-- Storage_FS (same instance)
|       +-- Cache__Hash__Generator
|       +-- LETS__Pipeline
|       +-- 4 namespace handlers (analytics, tokens, costs, transfers)
|           +-- cache/analytics/refs/by-hash/...    (custom path gen)
|           +-- cache/analytics/data/raw/...        (custom temporal write)
|           +-- cache/tokens/refs/by-hash/...       (custom hash sharding)
|           +-- cache/tokens/data/direct/...        (custom directory setup)
```

### After (v0.2.40 Architecture)

```
Lambda Process
|
+-- Fast_API__SGraph__App__Send__User
|   +-- Transfer__Service
|   |   +-- Storage_FS__S3(s3_prefix="user/")
|   |       +-- user/transfers/{id}/meta.json
|   |       +-- user/transfers/{id}/payload
|   |
|   +-- Send__Cache__Client  (THIN WRAPPER, ~4h)
|       +-- Cache__Service__Client(mode=IN_MEMORY)
|       |   +-- FastAPI TestClient (same process)
|       |       +-- Cache Service App
|       |           +-- Storage_FS__S3(s3_prefix="admin/")
|       |               +-- admin/analytics/data/temporal/...   (auto path gen)
|       |               +-- admin/analytics/data/key-based/...  (auto path gen)
|       |               +-- admin/tokens/data/key-based/...     (auto path gen)
|       |               +-- admin/costs/data/key-based/...      (auto path gen)
|       |               +-- admin/transfers/cache_entries/...    (auto path gen)
|       |
|       +-- Cache__Hash__Generator
|       +-- LETS__Pipeline  (CUSTOM, ~4-6h -- same logic, different storage API)
```

### Key Differences

| Aspect | v0.2.33 | v0.2.40 |
|---|---|---|
| **Storage path generation** | Custom per-namespace handler | Automatic via cache service strategies |
| **Hash-to-ID resolution** | Custom sharded directory lookup | Built into cache service |
| **Temporal filing** | Custom date-path construction | `TEMPORAL` strategy handles it |
| **S3 layout** | Single prefix, `cache/` subdirectory | Two prefixes: `user/` and `admin/` |
| **Extraction path** | None -- monolithic | Flip `IN_MEMORY` to `REMOTE` |
| **Build effort** | 25-35 hours | ~8-12 hours |

---

## 7. Open Questions

| # | Question | Recommendation | Priority |
|---|---|---|---|
| **Q1** | What are the exact package sizes of `mgraph-ai-service-cache` and `mgraph-ai-service-cache-client` (installed, with dependencies)? | Measure before adding to Lambda dependencies. If combined size exceeds 15MB, flag for discussion. | **P1 -- blocking before implementation** |
| **Q2** | Should the cache service's `Storage_FS__S3` instance share the same S3 client connection as the transfer service's instance, or should they be independent? | Independent instances with different `s3_prefix` values. Sharing a client connection risks cross-contamination if one instance's state leaks. `Storage_FS__S3` already handles connection management internally. | P2 |
| **Q3** | What happens to the existing `transfers/` root-level data in S3? The brief says "delete it and start clean with `user/transfers/`". | Agree with the brief. The current `transfers/` root contains only dev/test data from early deployments. Migrate to `user/transfers/` as part of this implementation. Confirm no production data exists at the root level before deleting. | P1 |
| **Q4** | The earlier open questions from v0.2.33 (Q1: case-insensitive tokens, Q2: retention policy, Q3: pulse endpoint location, Q4: token rename support, Q5: projected costs) -- are these still open? | Q1 (case-insensitive): Still recommend yes, lowercase before hashing. Q3 (pulse location): Admin API, confirmed by the brief. Q4 (token rename): Still recommend no, create new/revoke old. Q2 and Q5 remain open but are Phase 3+ concerns. | P3 |
| **Q5** | Should analytics recording failures be silent (logged but no error to user) or should they surface in the health check? | Silent for user-facing requests (try/except wrapper). Surface in admin health check as a degraded-analytics warning. Never fail a transfer operation because analytics recording failed. | P1 |

---

## Summary

The brief is correct: the MGraph-AI Cache Service supersedes the custom build I designed in v0.2.33. I approve the approach. The data models, namespace structure, LETS pipeline logic, and all business-level decisions (D024-D030) remain valid. What changes is the storage layer: from custom `Storage_FS` path generation to cache service API calls via a thin `Send__Cache__Client` wrapper following the `Html_Cache__Client` pattern.

Decision D034 (IN_MEMORY mode with `admin/` prefix) is architecturally sound. The extraction path to `REMOTE` mode is a configuration change with zero application code impact.

The primary risk is Lambda package size -- measure it before committing to the dependency. The secondary risk is shared process state -- wrap analytics recording in error isolation so cache service failures never affect user-facing transfers.

**Recommended next step:** Dev measures the cache service package footprint, then scaffolds `Send__Cache__Client` with a health check and raw analytics event recording. That unblocks everything else.

---

*Architect review complete. This document supersedes the implementation approach (not the data models or business logic) from v0.2.33__response-to-daily-brief-2__13-feb.md.*
