# v0.2.33 -- Architect Response to Daily Brief #2 (13 Feb 2026)

**Version:** v0.2.33
**Date:** 2026-02-13
**Role:** Architect
**Context:** Response to `team/humans/dinis_cruz/briefs/02/13/v0.2.32__daily-brief__2__sgraph-send-13-feb-2026.md`
**Status:** GATING REVIEW -- Cache service schema must be approved before Dev can implement
**Depends on:** Brief #1 decisions D014 (hash-fragment URLs APPROVED), D017 (i18n before content), D019 (separate observability S3 bucket), D018 (tokens use query string)

---

## Executive Summary

This document defines the cache service architecture for SGraph Send. The cache service is the data backbone described in Daily Brief #2 -- it underpins site analytics, the token system, AWS cost tracking, and file storage event data. Four schemas are specified (A-D), each with Type_Safe data models, Memory-FS file tree layouts, and LETS pipeline definitions.

**Key architectural decisions in this review:**

| # | Decision | Recommendation |
|---|----------|----------------|
| D024 | Cache service built as a layer on top of Memory-FS (not a replacement) | APPROVE |
| D025 | Reuse `Cache__Hash__Generator` from osbot-utils for all cache hash operations | APPROVE |
| D026 | Separate S3 prefix namespaces per data domain (analytics, tokens, costs, transfers) | APPROVE |
| D027 | LETS pipeline runs on-demand with cascading save (not scheduled) | APPROVE |
| D028 | Token names are human-friendly strings, resolved to cache IDs via SHA-256 cache hash | APPROVE |
| D029 | AWS cost data stored in its own namespace with daily/weekly/monthly aggregation | APPROVE |
| D030 | Google Analytics removal approved once server-side analytics provides basic pulse | APPROVE |

---

## 1. Cache Service Architecture

### 1.1 Design Principles

The cache service is a **layer on top of Memory-FS**, not a separate storage system. It adds three capabilities to the existing `Storage_FS` interface:

1. **Cache ID resolution** -- every entry gets a unique Cache ID that maps to a file tree
2. **Cache Hash lookup** -- human-meaningful values resolve to Cache IDs via deterministic hashing
3. **Temporal storage** -- automatic date-path filing with a `latest` pointer

The application code continues to use `Storage_FS` for all I/O. The cache service provides the path-generation logic, hash-to-ID resolution, and aggregation orchestration.

### 1.2 Relationship to Existing Code

The project already has a reference `Cache__Service` implementation (provided by the project lead in `team/humans/dinis_cruz/code-example/example-of-memory-fs-with-s3/`). The existing code uses:

- `Cache__Hash__Generator` from `osbot_utils.helpers.cache` -- SHA-256 hashing with configurable length (default: 16 hex chars)
- `Cache__Config` with pluggable storage backends (Memory, S3, Local Disk, SQLite, Zip)
- Namespace-based handler separation
- Strategy-based storage (direct, temporal, temporal_latest, temporal_versioned)

Our implementation will follow this pattern but adapted to SGraph Send's specific domains.

### 1.3 Core Type_Safe Classes

```python
from osbot_utils.type_safe.Type_Safe                                        import Type_Safe
from osbot_utils.type_safe.primitives.domains.identifiers.Cache_Id          import Cache_Id
from osbot_utils.type_safe.primitives.domains.cryptography.safe_str.Safe_Str__Cache_Hash import Safe_Str__Cache_Hash
from osbot_utils.type_safe.primitives.domains.identifiers.safe_int.Timestamp_Now import Timestamp_Now
from osbot_utils.helpers.cache.Cache__Hash__Generator                       import Cache__Hash__Generator
from osbot_utils.helpers.cache.schemas.Schema__Cache__Hash__Config          import Schema__Cache__Hash__Config


class Schema__Cache__Entry(Type_Safe):                                      # Base schema for all cache entries
    cache_id       : Cache_Id                                               # Unique entry identifier (auto-generated)
    cache_hash     : Safe_Str__Cache_Hash                                   # Deterministic hash from source value
    created_at     : Timestamp_Now                                          # When entry was first created
    updated_at     : Timestamp_Now                                          # When entry was last modified
    entry_type     : str                                                    # Domain: 'analytics', 'token', 'cost', 'transfer'
    namespace      : str                                                    # Namespace for handler routing


class Schema__Cache__Temporal__Path(Type_Safe):                             # Temporal path components
    year           : str                                                    # 4-digit year: '2026'
    month          : str                                                    # 2-digit month: '02'
    day            : str                                                    # 2-digit day: '13'
    hour           : str                                                    # 2-digit hour: '14'
```

### 1.4 Cache Service for SGraph Send

```python
class Cache__Service__Send(Type_Safe):                                      # SGraph Send cache service
    storage_fs     : Storage_FS                                             # Shared storage backend (Memory-FS)
    hash_generator : Cache__Hash__Generator                                 # Reuse from osbot-utils
    namespaces     : dict                                                   # Domain -> handler mapping

    # Namespace constants
    NS_ANALYTICS   : str = 'analytics'
    NS_TOKENS      : str = 'tokens'
    NS_COSTS       : str = 'costs'
    NS_TRANSFERS   : str = 'transfers'
```

### 1.5 Storage Namespace Layout (Top-Level)

```
(Memory-FS root)/
|-- cache/
|   |-- analytics/                  # Site analytics cache (Schema A)
|   |   |-- refs/
|   |   |   |-- by-hash/            # Cache hash -> Cache ID mapping
|   |   |   |-- by-id/              # Cache ID -> file paths mapping
|   |   |-- data/
|   |   |   |-- raw/                # Raw event files (one per request)
|   |   |   |-- aggregations/       # LETS aggregation outputs
|   |   |   |-- latest/             # Current-state pointers
|   |-- tokens/                     # Token cache (Schema B)
|   |   |-- refs/
|   |   |   |-- by-hash/            # token-name hash -> Cache ID
|   |   |   |-- by-id/              # Cache ID -> token data paths
|   |   |-- data/
|   |   |   |-- direct/             # Token metadata (one folder per token)
|   |   |   |-- events/             # Token usage events (temporal)
|   |-- costs/                      # AWS cost cache (Schema C)
|   |   |-- refs/
|   |   |   |-- by-hash/
|   |   |   |-- by-id/
|   |   |-- data/
|   |   |   |-- raw/                # Raw cost data files
|   |   |   |-- aggregations/       # Cost aggregations
|   |   |   |-- latest/
|   |-- transfers/                  # Transfer event cache (Schema D)
|   |   |-- refs/
|   |   |   |-- by-hash/            # transfer_id hash -> Cache ID
|   |   |   |-- by-id/
|   |   |-- data/
|   |   |   |-- events/             # Per-transfer events (temporal)
|   |   |   |-- aggregations/       # Transfer analytics aggregations
|   |   |   |-- latest/
|-- transfers/                      # Existing transfer storage (UNCHANGED)
|   |-- {transfer_id}/
|       |-- meta.json
|       |-- payload
|       |-- events/
```

**Critical note:** The existing `transfers/` tree at the root level is UNCHANGED. The `cache/transfers/` namespace contains derived event data and aggregations, not the primary transfer storage. This ensures backward compatibility with the live system.

---

## 2. Schema A: Site Analytics Cache

### 2.1 What Replaces Google Analytics

Server-side analytics built on three data sources:

| Source | Data | Collection Method |
|--------|------|-------------------|
| CloudFront access logs | Every HTTP request to send.sgraph.ai | S3 log delivery to observability bucket (D019) |
| Lambda execution logs | Request metadata, timing, errors | Application-level event recording |
| Application events | file_created, file_viewed, file_downloaded, token_used | Already captured in transfer events |

No client-side JavaScript analytics. No cookies. No consent required.

### 2.2 Cache Entries for Analytics

Each analytics data source gets its own cache hash. The hash is derived from the source identifier + time window.

```python
class Schema__Analytics__Raw_Event(Type_Safe):                              # One file per server-side event
    event_id       : str                                                    # Unique event identifier
    event_type     : str                                                    # 'page_view', 'api_call', 'file_upload', 'file_download', 'token_use'
    timestamp      : Timestamp_Now                                          # When event occurred
    source         : str                                                    # 'cloudfront', 'lambda', 'application'
    path           : str                                                    # Request path (e.g., '/send/v0/v0.1/v0.1.0/index.html')
    method         : str                                                    # HTTP method
    status_code    : int                                                    # Response status
    duration_ms    : int                                                    # Request duration
    ip_hash        : str                                                    # SHA-256 of IP with daily salt
    user_agent_normalised : str                                             # Browser family + OS only
    country        : str                                                    # GeoIP country (populations large enough to be non-identifying)
    referrer_domain: str                                                    # Referrer domain only (not full URL)
    content_bytes  : int                                                    # Response size in bytes
    transfer_id    : str                                                    # If applicable (empty for non-transfer requests)
    token_id       : str                                                    # If applicable (empty for non-token requests)


class Schema__Analytics__Aggregation(Type_Safe):                            # Aggregated analytics for a time window
    window_start   : int                                                    # Start of window (epoch ms)
    window_end     : int                                                    # End of window (epoch ms)
    window_type    : str                                                    # '30min', 'hourly', 'daily', 'monthly', 'yearly'
    total_requests : int                                                    # Total HTTP requests in window
    unique_visitors: int                                                    # Unique ip_hash values
    page_views     : int                                                    # Requests to HTML pages
    api_calls      : int                                                    # Requests to /transfers/* endpoints
    file_uploads   : int                                                    # Successful upload_completed events
    file_downloads : int                                                    # Successful download events
    token_uses     : int                                                    # Token validation events
    error_count    : int                                                    # 4xx + 5xx responses
    avg_duration_ms: int                                                    # Average request duration
    total_bytes    : int                                                    # Total response bytes served
    top_paths      : dict                                                   # Path -> count (top 20)
    top_countries  : dict                                                   # Country -> count
    top_referrers  : dict                                                   # Referrer domain -> count
    status_codes   : dict                                                   # Status code -> count
```

### 2.3 Analytics File Tree

```
cache/analytics/data/
|-- raw/
|   |-- 2026/02/13/14/                          # Temporal path: year/month/day/hour
|   |   |-- {timestamp}__{event_id}.json        # One file per event
|   |   |-- {timestamp}__{event_id}.json
|   |-- 2026/02/13/15/
|   |   |-- {timestamp}__{event_id}.json
|-- aggregations/
|   |-- 30min/
|   |   |-- 2026/02/13/14-00.json               # 14:00-14:30 aggregation
|   |   |-- 2026/02/13/14-30.json               # 14:30-15:00 aggregation
|   |-- hourly/
|   |   |-- 2026/02/13/14.json                  # 14:00-15:00 (combines 2 x 30min)
|   |   |-- 2026/02/13/15.json
|   |-- daily/
|   |   |-- 2026/02/13.json                     # Full day (combines 24 x hourly)
|   |-- monthly/
|   |   |-- 2026/02.json                        # Full month (combines daily)
|   |-- yearly/
|   |   |-- 2026.json                           # Full year (combines monthly)
|-- latest/
|   |-- current-30min.json                      # Most recent 30min aggregation
|   |-- current-hourly.json                     # Most recent hourly aggregation
|   |-- current-daily.json                      # Most recent daily aggregation
|   |-- pulse.json                              # Real-time pulse (last 5 minutes, rolling)
```

### 2.4 Temporal Granularity

| Level | Window | Built From | When Computed |
|-------|--------|------------|---------------|
| Raw | Single event | Source data | On every request (immediate) |
| Pulse | 5 minutes (rolling) | Raw events | On-demand (read triggers computation) |
| 30-minute | 30 minutes | Raw events | On-demand, saved once, never recomputed |
| Hourly | 1 hour | 2 x 30-minute | On-demand, combines saved 30-min files |
| Daily | 24 hours | 24 x hourly | On-demand, combines saved hourly files |
| Monthly | Calendar month | N x daily | On-demand, combines saved daily files |
| Yearly | Calendar year | 12 x monthly | On-demand, combines saved monthly files |

### 2.5 Cache Hash Strategy for Analytics

Analytics entries are hashed by their **time window identifier**:

```
hash("analytics:30min:2026-02-13T14:00") -> cache_hash -> cache_id -> aggregation file
hash("analytics:hourly:2026-02-13T14")   -> cache_hash -> cache_id -> aggregation file
hash("analytics:daily:2026-02-13")       -> cache_hash -> cache_id -> aggregation file
hash("analytics:pulse:current")          -> cache_hash -> cache_id -> latest pulse data
```

This means querying "what happened today?" hashes the string `"analytics:daily:2026-02-13"`, finds the cache ID, and reads the pre-computed aggregation. If the aggregation does not exist yet, the LETS pipeline computes it from the next-lower granularity (24 x hourly) and saves it.

---

## 3. Schema B: Token System Cache

### 3.1 Token Name to Cache Hash Resolution

This is the core insight from the brief: tokens have **human-friendly names** (`community-x`, `football-123`, `beta-2025`) and the name resolves to a cache entry containing all token data.

**Resolution flow:**

```
Token name "community-x"
    |
    v
Cache__Hash__Generator.from_string("community-x")
    |
    v
SHA-256 hash (first 16 hex chars): "a3f7b2c19d4e8f01"   (the cache hash)
    |
    v
Lookup: cache/tokens/refs/by-hash/a3f7/b2c1/9d4e8f01.json
    |
    v
Resolves to Cache ID: "c7e2a1b4f09d"
    |
    v
cache/tokens/data/direct/c7e2a1b4f09d/token.json         (token metadata)
cache/tokens/data/events/c7e2a1b4f09d/2026/02/13/14/...   (usage events)
```

**Note on hash sharding:** The by-hash reference path uses directory sharding (`a3f7/b2c1/9d4e8f01.json`) to avoid flat directory performance issues at scale. The first 4 chars, next 4 chars, then remaining chars become nested directories. This matches the pattern in the reference `Cache__Service` code.

### 3.2 Token Data Models

```python
class Schema__Token__Metadata(Type_Safe):                                   # Token metadata (lives in cache entry)
    cache_id         : Cache_Id                                             # Cache entry ID
    cache_hash       : Safe_Str__Cache_Hash                                 # Hash of token name
    token_name       : str                                                  # Human-friendly name: 'community-x'
    usage_limit      : int                                                  # Max uses: 5, 10, 100, 1000, 10000
    usage_count      : int                                                  # Current usage count
    status           : str                                                  # 'active', 'exhausted', 'revoked', 'expired'
    created_at       : Timestamp_Now                                        # Creation timestamp
    created_by       : str                                                  # Admin identifier
    expires_at       : int                                                  # Expiry timestamp (0 = no expiry)
    revoked_at       : int                                                  # Revocation timestamp (0 = not revoked)
    last_used_at     : int                                                  # Last usage timestamp
    metadata         : dict                                                 # Flexible: batch_id, notes, community_name, etc.


class Schema__Token__Usage_Event(Type_Safe):                                # Single token usage event
    event_id         : str                                                  # Unique event ID
    timestamp        : Timestamp_Now                                        # When token was used
    ip_hash          : str                                                  # SHA-256 of user IP (daily salt)
    user_agent_normalised : str                                             # Browser family + OS
    country          : str                                                  # GeoIP country
    action           : str                                                  # 'page_opened', 'upload_initiated', 'upload_completed'
    transfer_id      : str                                                  # If a transfer was created, its ID
    success          : bool                                                 # Whether the action succeeded
    rejection_reason : str                                                  # If failed: 'exhausted', 'expired', 'revoked'
```

### 3.3 Token File Tree

```
cache/tokens/data/
|-- direct/
|   |-- {cache_id}/
|   |   |-- token.json                          # Schema__Token__Metadata
|   |   |-- events/
|   |   |   |-- 2026/02/13/14/
|   |   |   |   |-- {timestamp}__{event_id}.json  # Schema__Token__Usage_Event
|   |   |   |-- 2026/02/13/15/
|   |   |   |   |-- {timestamp}__{event_id}.json
|   |   |-- aggregations/
|   |   |   |-- daily/
|   |   |   |   |-- 2026-02-13.json             # Daily usage summary for this token
|   |   |   |-- monthly/
|   |   |       |-- 2026-02.json
|   |   |-- latest/
|   |       |-- summary.json                    # Current state snapshot
```

### 3.4 Token Lifecycle Flow

```
1. ADMIN CREATES TOKEN
   |
   Admin provides: name="community-x", limit=100
   |
   v
   cache_hash = hash_generator.from_string("community-x")
   cache_id   = Cache_Id(Random_Guid())
   |
   v
   Write: cache/tokens/refs/by-hash/{sharded_hash}.json
          { "cache_ids": [{"cache_id": cache_id, "created_at": now}],
            "latest_id": cache_id, "total_versions": 1 }
   Write: cache/tokens/refs/by-id/{sharded_id}.json
          { "cache_hash": cache_hash, "file_paths": {...} }
   Write: cache/tokens/data/direct/{cache_id}/token.json
          Schema__Token__Metadata(token_name="community-x", usage_limit=100, ...)

2. USER PRESENTS TOKEN
   |
   URL: https://send.sgraph.ai/?token=community-x
   |
   v
   cache_hash = hash_generator.from_string("community-x")
   |
   v
   Look up: cache/tokens/refs/by-hash/{sharded_hash}.json
   -> get latest cache_id
   |
   v
   Read: cache/tokens/data/direct/{cache_id}/token.json
   -> check: status == 'active' AND usage_count < usage_limit AND not expired
   |
   v
   IF ALLOWED:
     Increment usage_count
     Write usage event to temporal path
     Update latest/summary.json
     Proceed with transfer
   |
   IF REJECTED:
     Write rejection event
     Return error (exhausted / expired / revoked)

3. ADMIN VIEWS TOKEN DASHBOARD
   |
   Read all token metadata from direct/{cache_id}/token.json for each active token
   Read aggregations for usage graphs
   |
   v
   Admin UI displays: name, usage_count / usage_limit, status, recent events
```

### 3.5 Token URL Format

From Brief #1 decision D018: token goes in the **query string** (not hash fragment) because the server must see it.

```
https://send.sgraph.ai/?token=community-x
```

The server reads the query parameter, hashes the token name, resolves to the cache entry, validates, and either grants or denies access. The token name itself is human-friendly and can appear in shared URLs.

### 3.6 Collision Handling

SHA-256 truncated to 16 hex chars gives 64 bits of entropy. At the token scale we expect (hundreds to low thousands), collision probability is negligible (birthday bound: ~2^32 entries for 50% collision chance with 64-bit hashes).

However, the by-hash reference file supports **multiple cache IDs per hash** (the `cache_ids` array). If a collision ever occurs, the system can disambiguate by checking the stored `token_name` field. This is a safety net, not an expected path.

---

## 4. Schema C: AWS Cost Cache

### 4.1 Cost Data Collection

AWS Cost Explorer API provides cost and usage data. The collector pattern (from Brief #1) applies:

1. Collector runs on-demand or on schedule
2. Fetches cost data from AWS Cost Explorer (via osbot-aws, never boto3 directly)
3. Tracks high-water mark (last date fetched) to avoid re-fetching
4. Saves raw cost data to cache service temporal path

```python
class Schema__Cost__Raw_Entry(Type_Safe):                                   # Raw cost data from AWS
    date             : str                                                  # Date: '2026-02-13'
    service          : str                                                  # AWS service: 'AmazonS3', 'AWSLambda', 'AmazonCloudFront'
    amount_usd       : float                                                # Cost in USD
    currency         : str                                                  # 'USD'
    usage_quantity   : float                                                # Usage units
    usage_unit       : str                                                  # 'GB', 'Requests', 'GB-Hours'
    region           : str                                                  # AWS region
    collected_at     : Timestamp_Now                                        # When this data was fetched


class Schema__Cost__Aggregation(Type_Safe):                                 # Aggregated cost data
    window_start     : int                                                  # Start of window (epoch ms)
    window_end       : int                                                  # End of window (epoch ms)
    window_type      : str                                                  # 'daily', 'weekly', 'monthly'
    total_cost_usd   : float                                                # Total cost for window
    cost_by_service  : dict                                                 # Service name -> cost
    cost_per_request : float                                                # total_cost / total_requests (from analytics)
    cost_per_transfer: float                                                # total_cost / total_transfers
    cost_per_active_user : float                                            # total_cost / unique_visitors
    total_requests   : int                                                  # From analytics aggregation (cross-reference)
    total_transfers  : int                                                  # From analytics aggregation
    unique_visitors  : int                                                  # From analytics aggregation
    top_cost_drivers : dict                                                 # Service -> percentage of total cost
```

### 4.2 Cost File Tree

```
cache/costs/data/
|-- raw/
|   |-- 2026/02/13/
|   |   |-- cost-data.json                      # Raw AWS cost data for this day
|-- aggregations/
|   |-- daily/
|   |   |-- 2026-02-13.json                     # Daily cost + unit economics
|   |-- weekly/
|   |   |-- 2026-W07.json                       # Weekly cost (ISO week)
|   |-- monthly/
|   |   |-- 2026-02.json                        # Monthly cost + trends
|-- latest/
|   |-- current-daily.json                      # Most recent daily cost summary
|   |-- current-monthly.json                    # Running month-to-date
|-- collector/
|   |-- high-water-mark.json                    # Last fetched date per source
```

### 4.3 Unit Economics Correlation

Cost aggregations cross-reference analytics aggregations to compute unit economics. The `cost_per_request`, `cost_per_transfer`, and `cost_per_active_user` fields require reading the corresponding analytics aggregation for the same time window.

```python
# Pseudocode for daily cost aggregation with unit economics
def aggregate_daily_cost(date_str):
    raw_costs     = read_raw_cost_files(f"cache/costs/data/raw/{date_path}/")
    analytics_agg = read_file(f"cache/analytics/data/aggregations/daily/{date_str}.json")

    total_cost    = sum(entry.amount_usd for entry in raw_costs)
    total_reqs    = analytics_agg.total_requests   if analytics_agg else 0
    total_xfers   = analytics_agg.file_uploads     if analytics_agg else 0
    unique_users  = analytics_agg.unique_visitors  if analytics_agg else 0

    aggregation = Schema__Cost__Aggregation(
        total_cost_usd      = total_cost                                    ,
        cost_per_request    = total_cost / total_reqs  if total_reqs  else 0,
        cost_per_transfer   = total_cost / total_xfers if total_xfers else 0,
        cost_per_active_user= total_cost / unique_users if unique_users else 0,
        ...
    )
    save_to_cache(f"cache/costs/data/aggregations/daily/{date_str}.json", aggregation)
    save_to_cache(f"cache/costs/data/latest/current-daily.json", aggregation)
```

### 4.4 Cache Hash Strategy for Costs

```
hash("cost:daily:2026-02-13")     -> cache_hash -> cache_id -> daily cost file
hash("cost:weekly:2026-W07")      -> cache_hash -> cache_id -> weekly cost file
hash("cost:monthly:2026-02")      -> cache_hash -> cache_id -> monthly cost file
```

---

## 5. Schema D: File Storage Event Integration

### 5.1 Design: Cache Layer Over Existing Transfer Storage

The existing transfer storage (`transfers/{transfer_id}/meta.json`, `payload`, `events/`) remains the source of truth. The cache layer adds:

1. **Derived event data** -- each transfer event also writes to the analytics raw event stream
2. **Per-file aggregations** -- how many views, downloads, etc. per transfer (rolled up from events)
3. **Cross-transfer analytics** -- global file activity aggregations

```python
class Schema__Transfer__Cache__Summary(Type_Safe):                          # Per-transfer analytics summary
    transfer_id      : str                                                  # The transfer ID
    cache_id         : Cache_Id                                             # Cache entry for this transfer
    total_views      : int                                                  # Times info endpoint was called
    total_downloads  : int                                                  # Times download was completed
    created_at       : int                                                  # Transfer creation timestamp
    last_activity_at : int                                                  # Most recent event timestamp
    events           : list                                                 # Recent event summaries
    active_hours     : dict                                                 # Hour-of-day -> activity count
    active_countries : dict                                                 # Country -> download count
```

### 5.2 Event Integration Flow

When a transfer event occurs (upload, download, view), three writes happen:

```
1. PRIMARY (existing): transfers/{transfer_id}/events/{timestamp}__{event}.json
   (This is the source of truth, unchanged from current implementation)

2. ANALYTICS RAW: cache/analytics/data/raw/{year}/{month}/{day}/{hour}/{timestamp}__{event_id}.json
   (Feeds the site analytics pipeline -- Schema A)

3. TRANSFER CACHE: cache/transfers/data/events/{cache_id}/{year}/{month}/{day}/{hour}/{timestamp}__{event_id}.json
   (Feeds per-file analytics -- Schema D)
```

The transfer's `cache_hash` is derived from the transfer_id:

```
hash("transfer:abc123def456") -> cache_hash -> cache_id -> per-transfer event cache
```

### 5.3 Transfer Event File Tree

```
cache/transfers/data/
|-- events/
|   |-- {cache_id}/                             # Per-transfer event cache
|   |   |-- 2026/02/13/14/
|   |   |   |-- {timestamp}__{event_id}.json
|   |   |-- 2026/02/13/15/
|   |       |-- {timestamp}__{event_id}.json
|-- aggregations/
|   |-- per-transfer/
|   |   |-- {cache_id}/
|   |   |   |-- summary.json                   # Schema__Transfer__Cache__Summary
|   |-- global/
|   |   |-- daily/
|   |   |   |-- 2026-02-13.json                 # All-transfers daily activity
|   |   |-- monthly/
|   |       |-- 2026-02.json
|-- latest/
|   |-- active-transfers.json                   # List of transfers with recent activity
```

### 5.4 Encrypted File Binary Storage

The brief mentions that cache entries can hold binary data (encrypted file contents). For SGraph Send, the encrypted payload is **already stored** at `transfers/{transfer_id}/payload`. The cache layer does NOT duplicate the payload. It stores only metadata, events, and aggregations.

If in future we need to support alternative storage backends for payloads (e.g., moving large files to a different S3 tier), the cache entry's file refs can point to the payload location without copying bytes.

---

## 6. LETS Pipeline Architecture

### 6.1 The LETS Principle: Load, Extract, Transform, Save

```
L - LOAD    : Read raw event files from temporal path
E - EXTRACT : Parse JSON, filter by time window, validate schema
T - TRANSFORM: Aggregate (count, sum, average, top-N), correlate across domains
S - SAVE    : Write aggregation result to cache, update latest pointer
```

### 6.2 Pipeline Execution Model: On-Demand with Cascading Save

**Decision D027: The LETS pipeline runs on-demand, not on a schedule.**

When a query requests aggregated data:

1. Check if the aggregation file exists (e.g., `cache/analytics/data/aggregations/hourly/2026/02/13/14.json`)
2. If it exists and the time window is complete (past hour), return it -- the aggregation is immutable
3. If it exists but the time window is current (this hour), recompute from raw data since this is a live window
4. If it does not exist, compute from the next-lower granularity:
   - Hourly: combine 2 x 30-minute aggregations (or compute from raw if those are missing)
   - Daily: combine 24 x hourly
   - Monthly: combine N x daily
   - Yearly: combine 12 x monthly
5. Save the result (LETS principle: save the query result)
6. Update the `latest/` pointer

**Why on-demand, not scheduled?**

- At current scale (friends and family), scheduled aggregation would run on empty data most of the time
- On-demand means we only pay for computation when someone actually reads the dashboard
- The "save when computed" pattern means each aggregation is computed at most once
- When traffic grows enough to warrant scheduled computation, the same pipeline code runs on a cron -- no architecture change needed

### 6.3 Aggregation Cascade Diagram

```
EVERY REQUEST writes a raw event file
     |
     v
cache/analytics/data/raw/2026/02/13/14/{timestamp}__{event_id}.json
     |
     | (on-demand query: "show me the last 30 minutes")
     v
30-MINUTE AGGREGATION
  Read: all raw files in 2026/02/13/14-00/ to 2026/02/13/14-29/
  Write: cache/analytics/data/aggregations/30min/2026/02/13/14-00.json
  Write: cache/analytics/data/latest/current-30min.json
     |
     | (on-demand query: "show me the last hour")
     v
HOURLY AGGREGATION
  Read: 2 x 30-minute files (14-00.json + 14-30.json)
  Write: cache/analytics/data/aggregations/hourly/2026/02/13/14.json
  Write: cache/analytics/data/latest/current-hourly.json
     |
     | (on-demand query: "show me today")
     v
DAILY AGGREGATION
  Read: up to 24 x hourly files
  Write: cache/analytics/data/aggregations/daily/2026-02-13.json
  Write: cache/analytics/data/latest/current-daily.json
     |
     v
MONTHLY -> YEARLY (same pattern, combine lower granularity)
```

### 6.4 The Pulse: Near-Real-Time View

The "pulse" is a special aggregation for the current moment:

```python
class Schema__Analytics__Pulse(Type_Safe):                                  # Real-time-ish traffic pulse
    computed_at      : Timestamp_Now                                        # When pulse was computed
    window_minutes   : int           = 5                                    # Rolling window size
    active_requests  : int                                                  # Requests in last N minutes
    active_visitors  : int                                                  # Unique ip_hashes in last N minutes
    active_transfers : int                                                  # Transfers in progress
    recent_events    : list                                                 # Last 10 events (sanitised)
```

The pulse is **always recomputed** (never cached) because it represents a rolling window. It reads the last N minutes of raw event files and produces a snapshot. This answers the brief's requirement: "is anyone using the site right now?"

The pulse endpoint should be lightweight -- at friends-and-family scale, reading 5 minutes of raw files means reading at most a few dozen small JSON files.

### 6.5 LETS Pipeline Implementation Class

```python
class LETS__Pipeline(Type_Safe):                                            # Load, Extract, Transform, Save
    storage_fs       : Storage_FS                                           # Shared storage backend
    hash_generator   : Cache__Hash__Generator                               # For cache hash computation

    def compute_aggregation(self, domain, window_type, window_key):
        """
        domain     : 'analytics', 'costs', 'transfers'
        window_type: '30min', 'hourly', 'daily', 'monthly', 'yearly'
        window_key : '2026-02-13', '2026-02-13T14', etc.
        """
        # 1. Check if aggregation already exists and is immutable (past window)
        # 2. If yes, return cached result
        # 3. If no, compute from lower granularity or raw data
        # 4. Save result to aggregation path AND latest pointer
        # 5. Return result

    def compute_pulse(self, window_minutes=5):
        """Always recomputed -- reads last N minutes of raw events."""

    def aggregate_raw_events(self, raw_file_paths):
        """L-E-T: Load files, Extract event data, Transform into aggregation."""

    def cascade_aggregations(self, lower_granularity_paths):
        """Combine existing aggregations into higher granularity."""
```

---

## 7. Decision Recommendations

### D024: Cache Service as a Layer on Memory-FS

**APPROVE.** The cache service adds path-generation, hash-resolution, and aggregation logic on top of Memory-FS. It does not replace `Storage_FS` -- it uses it. This means:

- All existing storage backends (Memory, S3, Local Disk) work automatically
- Tests continue to use `Storage_FS__Memory` with zero changes
- No new infrastructure dependencies

### D025: Reuse Cache__Hash__Generator from osbot-utils

**APPROVE.** The `Cache__Hash__Generator` already provides:
- SHA-256 hashing with configurable length (default 16 hex chars)
- `from_string()`, `from_bytes()`, `from_json()` methods
- Type-safe inputs and outputs (`Safe_Str__Cache_Hash`)

This is exactly what we need for token-name-to-cache-hash resolution, analytics window hashing, and cost data hashing. No custom hash logic needed.

The existing `hash_ip` in `Transfer__Service` (line 133) should also migrate to `Cache__Hash__Generator` (consistent hashing across the codebase). This aligns with the existing TODO in the code and issue DEV-008.

### D026: Separate Namespace Prefixes

**APPROVE.** Four namespaces (`analytics`, `tokens`, `costs`, `transfers`) each get their own `cache/` subdirectory. Benefits:
- Clean separation of concerns
- Independent lifecycle management (analytics data can have shorter TTL than token data)
- Easier debugging (list files in one namespace without noise from others)
- Maps cleanly to the existing `Cache__Handler` per-namespace pattern

### D027: On-Demand LETS Pipeline

**APPROVE.** At current scale, on-demand is the right choice. The architecture supports transitioning to scheduled execution without code changes -- the same `LETS__Pipeline.compute_aggregation()` method runs whether triggered by an API call or a cron job.

**Trigger points:**
- Admin dashboard loads -> triggers aggregation computation
- Analytics API endpoint called -> triggers aggregation computation
- Future: CloudWatch Events / EventBridge rule -> triggers same computation on schedule

### D028: Human-Friendly Token Names via Cache Hash

**APPROVE.** Token names like `community-x` are hashed to 16-hex-char cache hashes using `Cache__Hash__Generator`. The cache hash resolves to a Cache ID which contains the full token file tree.

**Migration note:** The existing token schema (from Architect Brief #1 response, `Schema__Token` in the v0.1.2 data model) uses `token_id` with a `tok_` prefix as the primary key. The new model uses `token_name` (human-friendly) as the lookup key and `cache_id` as the storage key. The Dev will need to migrate the token storage layout from `tokens/{token_id}.json` to the cache service namespace.

### D029: AWS Cost Data in Its Own Namespace

**APPROVE.** Cost data has a different collection cadence (daily at most), different sources (AWS Cost Explorer API), and different aggregation needs (weekly and monthly, not 30-minute). A separate namespace keeps it cleanly isolated.

### D030: Google Analytics Removal

**APPROVE** once the server-side analytics can answer two questions:
1. "Is anyone using the site right now?" (the pulse endpoint)
2. "How many people used the site today?" (the daily aggregation)

These are the minimum viable replacements identified in the brief. The full analytics feature set (top paths, geographic distribution, referrer analysis) can follow after GA is removed.

---

## 8. Integration with Brief #1 Decisions

| Brief #1 Decision | Impact on Cache Service |
|-------------------|------------------------|
| D014: Hash-fragment URLs APPROVED | No impact on cache service. Hash fragment stays client-side. |
| D017: i18n before content | No impact on cache service. i18n is a frontend concern. |
| D018: Tokens use query string | Token names are visible to the server in the URL query string. The cache service hashes the token name from the query param to resolve the cache entry. |
| D019: Separate observability S3 bucket | CloudFront logs are delivered to the observability bucket. The analytics collector reads from that bucket and writes raw events to the cache service. The cache service itself lives in the main data bucket (not the observability bucket). |
| D022: Separate S3 bucket for observability | The cache service's analytics namespace stores processed/aggregated data. Raw CloudFront logs live in the observability bucket. The collector bridges the two. |

---

## 9. Implementation Sequence for Dev

The Architect recommends the following implementation order. Each step produces testable output.

| # | Task | Depends On | Estimated Size | What It Unblocks |
|---|------|------------|----------------|------------------|
| 1 | `Cache__Service__Send` scaffolding: namespace setup, hash generator wiring, Storage_FS integration | Nothing | Small (2-3h) | Everything below |
| 2 | Token cache: `Schema__Token__Metadata`, create/lookup by name, usage counting | Step 1 | Medium (4-6h) | Token system UI, token validation in upload flow |
| 3 | Analytics raw event recording: write one file per server request | Step 1 | Small (2-3h) | Analytics pipeline |
| 4 | LETS pipeline: pulse computation (read last 5 min of raw events) | Step 3 | Medium (3-4h) | "Is anyone using the site?" -- minimum GA replacement |
| 5 | LETS pipeline: 30-min and hourly aggregations with cascading save | Step 4 | Medium (4-6h) | "How many people today?" -- GA removal trigger |
| 6 | Transfer event integration: dual-write to transfer events + analytics raw stream | Steps 1, 3 | Small (2-3h) | Per-file analytics in UI |
| 7 | Cost collector: fetch from AWS Cost Explorer, store in cache | Step 1 | Medium (3-4h) | Unit economics dashboard |
| 8 | Admin dashboard endpoints: read aggregations, pulse, token summaries | Steps 2, 4, 5 | Medium (4-6h) | Admin UI data source |

**Total estimated implementation time:** 25-35 hours across all tasks.

**Critical path for GA removal:** Steps 1 -> 3 -> 4 -> 5. Once step 5 is complete, the server-side analytics can answer the two minimum questions and GA can be removed.

---

## 10. Testing Strategy

All cache service tests use `Storage_FS__Memory`. No mocks, no patches. The cache service is a pure logic layer on top of Storage_FS, so testing is straightforward:

```python
# Example test pattern
def test_token_create_and_lookup():
    cache_service = Cache__Service__Send(storage_fs=Storage_FS__Memory())

    # Create token
    result = cache_service.create_token(name="community-x", limit=100)
    assert result.token_name  == "community-x"
    assert result.usage_limit == 100
    assert result.usage_count == 0

    # Look up by name
    found = cache_service.lookup_token("community-x")
    assert found.cache_id == result.cache_id
    assert found.status   == "active"

    # Use token
    event = cache_service.use_token("community-x", ip_hash="abc123", action="page_opened")
    assert event.success is True

    found = cache_service.lookup_token("community-x")
    assert found.usage_count == 1


def test_analytics_lets_pipeline():
    cache_service = Cache__Service__Send(storage_fs=Storage_FS__Memory())

    # Write 10 raw events
    for i in range(10):
        cache_service.record_analytics_event(Schema__Analytics__Raw_Event(...))

    # Compute pulse
    pulse = cache_service.compute_pulse(window_minutes=5)
    assert pulse.active_requests == 10

    # Compute 30-minute aggregation
    agg = cache_service.compute_aggregation("analytics", "30min", "2026-02-13T14:00")
    assert agg.total_requests == 10

    # Verify aggregation was saved (LETS principle)
    agg_again = cache_service.compute_aggregation("analytics", "30min", "2026-02-13T14:00")
    # Should read from cache, not recompute
```

---

## 11. Open Questions for Human Review

| # | Question | Recommendation | Impact |
|---|----------|----------------|--------|
| Q1 | Should token names be case-insensitive? (i.e., "Community-X" and "community-x" resolve to the same hash) | Yes -- lowercase before hashing | Low risk, prevents user confusion |
| Q2 | What is the raw event retention policy? (How long do we keep individual request files?) | 90 days raw, aggregations forever | Affects storage costs at scale |
| Q3 | Should the pulse endpoint be a dedicated Lambda URL or part of the admin API? | Admin API (protected) for now | Affects deployment |
| Q4 | Should we support token name changes? (e.g., rename "community-x" to "community-x-2025") | No -- create new token, revoke old one. Names are immutable once hashed. | Simplifies implementation |
| Q5 | For the cost cache, should we also track estimated costs (projected monthly from current daily rate)? | Yes, as a derived field in monthly aggregation | Useful for budgeting |

---

## Summary: Architect Actions from This Review

| # | Action | Priority | Blocks |
|---|--------|----------|--------|
| D024 | Cache service layer on Memory-FS | P1 | All cache implementation |
| D025 | Reuse Cache__Hash__Generator | P1 | Token hash resolution |
| D026 | Namespace separation (analytics, tokens, costs, transfers) | P1 | Storage layout |
| D027 | On-demand LETS pipeline | P1 | Analytics pipeline implementation |
| D028 | Human-friendly token names via cache hash | P1 | Token system UI |
| D029 | AWS cost namespace with daily/weekly/monthly aggregation | P2 | Cost dashboard |
| D030 | GA removal approved once pulse + daily aggregation work | P1 | Privacy posture |

**This review is the gating Architect deliverable for Daily Brief #2. Dev may proceed with implementation once the Conductor confirms these schemas.**

---

*Architect review complete. The cache service is the central new component for this sprint -- it underpins analytics, tokens, costs, and transfer event data. All four schemas are designed to use existing infrastructure (Type_Safe, Memory-FS, Cache__Hash__Generator) with zero new dependencies.*
