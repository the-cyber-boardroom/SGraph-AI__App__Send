# SGraph Send: Agentic Workflow in Practice

**Version:** v0.2.24
**Date:** 2026-02-13
**Author:** Librarian
**Status:** Active

---

## What This Document Is

This is a practical guide to how SGraph Send is actually built and run. It explains the workflow we use every day: how a human project lead directs a team of 15 AI agent roles through LLM sessions, daily briefs, and a shared Git repository.

If you want the theory, read [`v0.1.0__guide__agentic-role-based-workflow.md`](v0.1.0__guide__agentic-role-based-workflow.md). This document is about how it works in practice on a real, live product.

---

## The Big Picture

```
┌──────────────────────────────────────────────────────────────────┐
│                     HUMAN (Project Lead)                         │
│                                                                  │
│  Writes daily briefs, observes users, makes decisions,           │
│  provides reference code from other projects                     │
└──────────────────┬────────────────────┬──────────────────────────┘
                   │                    │
           ┌───────▼───────┐    ┌───────▼───────┐
           │  LLM Session  │    │  LLM Session  │    ... (parallel)
           │  (Claude Code) │    │  (Claude Code) │
           │               │    │               │
           │  Reads brief  │    │  Reads brief  │
           │  Reads code   │    │  Reads repo   │
           │  Writes brief │    │  Writes code  │
           │  for team     │    │  Writes tests │
           └───────┬───────┘    └───────┬───────┘
                   │                    │
                   ▼                    ▼
┌──────────────────────────────────────────────────────────────────┐
│                     GIT REPOSITORY                               │
│                     (the coordination bus)                        │
│                                                                  │
│  team/humans/dinis_cruz/briefs/    ← Human briefs go here        │
│  team/roles/{role}/reviews/        ← Agent responses go here     │
│  team/roles/{role}/ROLE.md         ← Role definitions            │
│  sgraph_ai_app_send/               ← Application code            │
│  tests/                             ← Tests                      │
│  .issues/                           ← Issue tracker               │
│  library/                           ← Docs, specs, guides         │
└──────────────────────────────────────────────────────────────────┘
                   │
                   ▼
┌──────────────────────────────────────────────────────────────────┐
│              NEXT LLM SESSION (any role)                         │
│                                                                  │
│  Reads latest briefs, reads ROLE.md, reads CLAUDE.md,            │
│  reads previous role reviews, picks up where the last            │
│  session left off                                                │
└──────────────────────────────────────────────────────────────────┘
```

---

## How a Typical Day Works

### 1. The Human Writes a Daily Brief

Every morning, the project lead (Dinis Cruz) writes a daily brief. This is a markdown file committed to the repo:

```
team/humans/dinis_cruz/briefs/02/13/v0.2.16__daily-brief__sgraph-send-13-feb-2026.md
```

The brief contains:
- **What happened** — user feedback, observations, milestones
- **What needs to happen** — prioritised tasks, assigned to specific roles
- **Decisions** — direction on open questions
- **Amendments** — corrections to previous plans ("rate limiting can wait", "CORS is already handled")

The brief is addressed **"to all roles via Conductor"** — it's written for the AI team, not for humans. It names specific roles: "Architect + AppSec: review this approach", "Dev: implement this", "DevOps: brief me on AWS settings".

### 2. The Human Starts LLM Sessions

The project lead opens one or more LLM sessions (Claude Code, Claude on the web, etc.) and gives them work. There are several patterns:

#### Pattern A: "Brief the team"

The human points a session at the daily brief and says:

> "Review the daily brief I just published and ask all impacted agents to review their plans and priorities."

The session reads the brief, reads the repo state, and produces review documents for each impacted role — Architect, AppSec, Dev, DevOps, QA, Conductor, Cartographer, Historian, Journalist, Librarian. These are committed to:

```
team/roles/{role}/reviews/26-02-13/v0.2.24__response-to-daily-brief__13-feb.md
```

Plus a Librarian master index that cross-references all responses.

#### Pattern B: "Create a technical brief from reference code"

The human has code from another project (metrics collectors, dashboard generators, etc.) and uploads it alongside the daily brief into a new LLM session:

```
Files uploaded:
  Lambda__Metrics__Collector.py    (129 lines)
  S3__Metrics__Collector.py        (120 lines)
  Dashboard__Generator.py          (251 lines)
  test_Aws_Metrics_Snapshot__run.py (51 lines)
  v0.2.16__daily-brief__...        (236 lines)
```

The human says:

> "Based on the metrics section of my daily brief, and these classes I created in another project, can you help me put this in a detailed technical brief I can add to my SGraph Send Project and brief the Agentic team in there to implement it?"

The session analyses the reference code, maps it to SGraph Send's architecture, and produces a technical brief that the agentic team can pick up and implement. This brief goes into the repo as a new conductor/human brief.

#### Pattern C: "Implement this feature"

The human (or a Conductor brief) assigns a specific implementation task:

> "Implement the hash-fragment URL approach for one-click file access. The Architect and AppSec have already approved it."

A Dev-focused session reads the relevant reviews, implements the code changes, writes tests, and commits.

#### Pattern D: "Review and respond as a specific role"

> "As the Architect, review the hash-fragment approach and confirm whether it's safe."

The session reads the ROLE.md for Architect, reads the relevant code and specs, and produces a review document.

### 3. Everything Goes Through Git

Every output — briefs, reviews, code, tests, decisions — is committed to the Git repository. This is the **coordination bus**. Agents don't talk to each other directly. They communicate through files:

- Human brief → Git → Agent reads brief → Agent writes review → Git → Next agent reads review
- Architect decision → Git → Dev reads decision → Dev writes code → Git → QA reads code

### 4. The Next Session Picks Up Where the Last Left Off

When a new LLM session starts, it reads:
1. **`.claude/CLAUDE.md`** — project rules, stack, conventions, key documents
2. **Latest human brief** — current priorities
3. **Latest Librarian master index** — what all roles have done
4. **Role-specific ROLE.md** — if operating as a specific role
5. **Previous reviews** — what was already decided

This means every session starts with full context. There is no institutional knowledge loss between sessions. The repository IS the institutional memory.

---

## The 15 Roles

### Original 10 (since v0.1.0)

| Role | What It Does | Typical Output |
|------|-------------|----------------|
| **Conductor** | Orchestrates workflow, sequences tasks, routes decisions | Sequencing plans, risk registers, coordination matrices |
| **Architect** | Designs systems, reviews approaches, defines schemas | Technical reviews, schema definitions, i18n designs |
| **Dev** | Implements features, writes code and tests | Source code, unit tests, implementation plans |
| **QA** | Tests adversarially, defines test matrices | Test plans, defect reports, coverage estimates |
| **DevOps** | Manages infrastructure, CI/CD, observability | AWS briefs, pipeline configs, deployment plans |
| **AppSec** | Reviews security, threat models, crypto decisions | Security reviews, threat models, incident analyses |
| **Cartographer** | Maps system topology, data flows, dependencies | System maps, data flow diagrams, dependency graphs |
| **Historian** | Tracks decisions, milestones, patterns over time | Decision logs (D001–D023+), milestone records, timelines |
| **Journalist** | Writes user-facing content, marketing copy | Landing page copy, email templates, "How It Works" |
| **Librarian** | Maintains knowledge, indexes, cross-references | Master indices, this guide, workflow documentation |

### New 5 (since v0.2.16, added 12 Feb 2026)

| Role | What It Does | Typical Output |
|------|-------------|----------------|
| **Advocate** | Champions the user, maintains personas, reviews UX | Persona definitions, design reviews, NPS tracking |
| **Sherpa** | Guides user onboarding, logs friction, observes trails | Journey maps, friction logs, trail analyses |
| **Ambassador** | Drives growth, competitive intel, viral loops | Campaign plans, competitive analyses, viral metrics |
| **DPO** | Ensures UK GDPR compliance, privacy reviews | DPIAs, privacy wording, lawful basis audits |
| **GRC** | Governs risk, compliance, incidents | Risk registers, compliance frameworks |

### The Role Lifecycle

```
Human writes brief
    ↓
Conductor sequences work, assigns to roles
    ↓
Each role reads brief + previous context, produces review/output
    ↓
Librarian indexes all outputs into master index
    ↓
Human reads master index, provides feedback/amendments
    ↓
Next cycle begins
```

---

## How Briefs Flow Through the System

### Example: The 13 Feb 2026 Daily Brief

**08:00 — Human writes brief:**
- First real user tested the MVP
- Observes friction: too many steps, page doesn't explain itself
- Proposes hash-fragment URLs for one-click access
- Requests AWS observability audit
- Assigns tasks to ~10 roles

**08:30 — LLM session activated:**
- Reads the daily brief
- Reads current repo state (56 tests, 5 UI components, 2 Lambdas)
- Reads all previous role reviews and decisions (D001–D013)

**09:00 — 10 role reviews produced:**

| Role | Key Output |
|------|-----------|
| Architect | Hash-fragment APPROVED, token schema designed, i18n architecture defined |
| AppSec | Hash-fragment APPROVED with mitigations (history.replaceState) |
| Dev | 9 implementation tasks sized (2h to 12h each), sequenced |
| DevOps | AWS Tier 1 logging brief, collector pipeline architecture |
| Conductor | 5-phase execution plan, risk register updated, 6 new decisions |
| QA | 35 new tests needed, regression risk assessment |
| Cartographer | Updated data flow diagrams, security boundary maps |
| Historian | 10 new decisions logged (D014–D023), milestone M003 recorded |
| Journalist | Landing page copy plan, email template drafted |
| Librarian | Master index cross-referencing all 10 responses |

**09:30 — Human reads master index:**
- Sees that the gating review (hash-fragment) is complete
- Sees the sequencing plan
- Provides amendments if needed
- Starts next LLM session to begin implementation

---

## How Reference Code Gets Integrated

The human often has code from other projects that can be adapted for SGraph Send. The workflow:

1. **Human uploads reference code** to a new LLM session (Python files, configs, etc.)
2. **Human provides context**: "Based on the metrics section of my daily brief, and these classes..."
3. **LLM session analyses** the reference code:
   - Understands the patterns and architecture
   - Maps them to SGraph Send's stack (Type_Safe not Pydantic, osbot-aws not boto3, Memory-FS not direct S3)
   - Identifies what needs to change for SGraph Send's conventions
4. **LLM session produces a technical brief** — a detailed document the agentic team can implement from
5. **Brief is committed to the repo** — other sessions pick it up

This pattern means the human doesn't have to manually translate code between projects. They provide the raw material and context; the LLM session does the adaptation.

---

## The Repository as the Single Source of Truth

Everything is in Git. Nothing lives only in chat logs or LLM session transcripts. This is critical because:

1. **LLM sessions are ephemeral** — they end, context is lost. But their outputs (committed files) persist.
2. **Any session can continue the work** — a new session reads the repo and has full context.
3. **The human can review everything** — briefs, reviews, code, decisions are all files they can read.
4. **History is preserved** — Git log shows when every decision was made, every review was written.
5. **Parallel work is safe** — multiple sessions can work on different branches simultaneously.

### Key Repository Locations

| What | Where |
|------|-------|
| Human briefs | `team/humans/dinis_cruz/briefs/{MM}/{DD}/` |
| Role reviews | `team/roles/{role}/reviews/{YY-MM-DD}/` |
| Role definitions | `team/roles/{role}/ROLE.md` |
| Project rules | `.claude/CLAUDE.md` |
| Master indices | `team/roles/librarian/reviews/{YY-MM-DD}/` |
| Application code | `sgraph_ai_app_send/` |
| Frontend code | `sgraph_ai_app_send__ui__user/` |
| Tests | `tests/unit/` |
| Issue tracker | `.issues/` |
| Guides & docs | `library/guides/` |
| Specs | `library/docs/specs/` |

---

## File Naming Conventions

All review and brief files follow a strict naming convention:

```
{version}__{type}__{description}.md
```

Examples:
- `v0.2.24__response-to-daily-brief__13-feb.md`
- `v0.2.24__master-index__daily-brief-responses-13-feb.md`
- `v0.2.16__daily-brief__sgraph-send-13-feb-2026.md`
- `v0.2.24__status-report__post-launch.md`

The version comes from `sgraph_ai_app_send/version`. Double underscore `__` separates version, type, and description. This makes files sortable, searchable, and unambiguous.

---

## How to Start a New LLM Session for SGraph Send

### For briefing the team (Conductor pattern)

```
Prompt: "Review the latest daily brief at team/humans/dinis_cruz/briefs/02/13/...
and ask all impacted agents to review their plans and priorities."
```

The session will:
1. Read `.claude/CLAUDE.md` (automatically loaded)
2. Read the daily brief
3. Read current master index
4. Read each role's latest reviews
5. Produce review documents for each impacted role
6. Write a master index
7. Commit and push

### For implementing a feature (Dev pattern)

```
Prompt: "Implement the hash-fragment URL approach. The Architect and AppSec
reviews are at team/roles/architect/reviews/26-02-13/... and
team/roles/appsec/reviews/26-02-13/..."
```

### For creating a technical brief from reference code

```
Upload: reference Python files + daily brief
Prompt: "Based on the metrics section of my daily brief, and these classes
I created in another project, can you help me put this in a detailed
technical brief I can add to my SGraph Send Project and brief the
Agentic team to implement it?"
```

### For a specific role review

```
Prompt: "As the AppSec role, review the hash-fragment approach for URL-based
key sharing. Read the proposal in the daily brief and provide a security
analysis."
```

---

## What Makes This Work

### 1. CLAUDE.md as the Universal Context

Every LLM session reads `.claude/CLAUDE.md` first. This 200-line file contains:
- Project identity and stack
- All key rules (Type_Safe not Pydantic, osbot-aws not boto3, no mocks)
- Repo structure
- Current sprint focus
- Links to all key documents

This means every session starts grounded. No session starts "cold."

### 2. Role Reviews as Persistent Memory

Each role's reviews folder is a growing knowledge base:
```
team/roles/architect/reviews/
  26-02-10/v0.2.1__response-to-infrastructure-brief.md
  26-02-13/v0.2.24__response-to-daily-brief__13-feb.md
```

New sessions read previous reviews to understand context and avoid contradicting earlier decisions.

### 3. Master Indices as Cross-References

The Librarian's master index is the entry point for understanding "what has been done." Instead of reading 10 individual role reviews, you read one master index that summarises all of them and identifies cross-cutting themes.

### 4. Human Amendments in Real Time

The human can amend priorities mid-stream: "rate limiting can wait", "CORS is already handled." These amendments are captured in the role reviews and flow into future sessions.

### 5. Decisions Are Logged and Numbered

The Historian tracks every decision (D001, D002, ..., D023+). When a future session needs to understand "why do we use hash fragments?" it can look up D014 and get the full rationale, who made it, and when.

---

## Current State (13 Feb 2026)

- **Product:** Live at `send.sgraph.ai` — first real user tested it
- **Team:** 15 AI agent roles + 1 human project lead
- **Decisions logged:** 23+ (D001–D023)
- **Milestones:** M001 (repo creation), M002 (MVP launch), M003 (first real user)
- **Tests:** 56 passing (target: ~91 after current sprint)
- **Reviews produced:** 50+ documents across all roles
- **Daily brief cadence:** Every working day

---

## Summary

The SGraph Send agentic workflow is:

1. **Human-led** — the project lead writes daily briefs that set priorities and make decisions
2. **AI-executed** — 15 specialised agent roles produce reviews, code, tests, and plans
3. **Git-coordinated** — everything goes through the repository; no information lives only in chat
4. **LLM-session-based** — each Claude session reads the repo, does its work, commits, and exits
5. **Self-documenting** — the Historian logs decisions, the Librarian indexes everything, the repo IS the memory

The human doesn't need to hold everything in their head. They write briefs, review master indices, and make decisions. The AI team handles the analysis, implementation, and documentation. The repository is the coordination bus that connects them all.

---

*Librarian — SGraph Send Agentic Workflow Guide*
*Version v0.2.24 — 13 February 2026*
