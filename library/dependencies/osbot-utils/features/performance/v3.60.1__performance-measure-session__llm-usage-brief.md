# Performance_Measure__Session - LLM Usage Brief

**Version**: v3.60.1  
**Purpose**: Guide for LLMs and developers on using the nanosecond-precision performance measurement framework  
**Location**: `osbot_utils.testing.performance`  
**Repo**: https://github.com/owasp-sbot/OSBot-Utils  
**Install**: `pip install osbot-utils`

---

## What is Performance_Measure__Session?

**Performance_Measure__Session is a high-precision benchmarking framework that produces statistically stable, reproducible performance metrics at nanosecond resolution.** It eliminates the noise and variability that plagues typical Python timing approaches by using Fibonacci-based sampling, outlier removal, and dynamic score normalization.

### The Problem It Solves

Python's built-in timing approaches are noisy and inconsistent:

```python
# Standard library - unreliable and verbose
import time

# Single measurement - wildly variable due to GC, context switches
start = time.perf_counter_ns()
my_function()
elapsed = time.perf_counter_ns() - start  # Could vary 10x between runs!

# Manual averaging - still affected by outliers
times = []
for _ in range(100):
    start = time.perf_counter_ns()
    my_function()
    times.append(time.perf_counter_ns() - start)
avg = sum(times) / len(times)  # One GC pause ruins the average
```

**With Performance_Measure__Session:**

```python
from osbot_utils.testing.performance.Performance_Measure__Session import Perf

with Perf() as _:
    _.measure(my_function).print()
# Output: my_function | score: 2,000 ns | raw: 1,847 ns
# Stable, reproducible, statistically robust
```

### Design Philosophy

1. **Statistical robustness** — Fibonacci sampling captures warm-up effects; outlier trimming removes GC noise
2. **Reproducible scores** — Dynamic normalization produces consistent values across runs
3. **CI-aware assertions** — Automatic adjustment for slower GitHub Actions runners
4. **Zero ceremony** — Context manager pattern, fluent API, sensible defaults
5. **Type_Safe integration** — All models use runtime type checking

---

## Quick Start

### 1. Import and Measure a Function

```python
from osbot_utils.testing.performance.Performance_Measure__Session import Perf

def my_function():
    return [i * 2 for i in range(100)]

with Perf() as _:
    _.measure(my_function).print()
```

Output:
```
my_function                    | score:   2,000 ns  | raw:   1,847 ns
```

### 2. Assert Performance in Tests

```python
from unittest                                                                     import TestCase
from osbot_utils.testing.performance.Performance_Measure__Session import Perf

class test_my_performance(TestCase):
    
    @classmethod
    def setUpClass(cls):
        cls.session = Perf(assert_enabled=True)
    
    def test_critical_operation(self):
        def fast_operation():
            return sum(range(10))
        
        with self.session as _:
            _.measure(fast_operation).print().assert_time(1_000, 2_000)
```

### 3. That's It

The session handles all the complexity: 1,595 measurements using Fibonacci scaling, outlier removal, score normalization, and CI-aware assertions.

> **Tip**: For slow functions (>100ms), use `measure__quick()` or `measure__fast()` to reduce test time.

---

## Import Reference

```python
# Core session - choose your preferred alias
from osbot_utils.testing.performance.Performance_Measure__Session import Performance_Measure__Session  # Full name
from osbot_utils.testing.performance.Performance_Measure__Session import Perf                          # Short (recommended)
from osbot_utils.testing.performance.Performance_Measure__Session import perf_session                  # snake_case
from osbot_utils.testing.performance.Performance_Measure__Session import performance_session           # Full snake_case

# Loop presets (optional - use measure__quick() and measure__fast() instead)
from osbot_utils.testing.performance.Performance_Measure__Session import MEASURE__INVOCATION__LOOPS        # Full: 1,595 invocations
from osbot_utils.testing.performance.Performance_Measure__Session import MEASURE__INVOCATION__LOOPS__QUICK  # Quick: 19 invocations
from osbot_utils.testing.performance.Performance_Measure__Session import MEASURE__INVOCATION__LOOPS__FAST   # Fast: 87 invocations

# Data models (typically not needed directly)
from osbot_utils.testing.performance.models.Model__Performance_Measure__Result      import Model__Performance_Measure__Result
from osbot_utils.testing.performance.models.Model__Performance_Measure__Measurement import Model__Performance_Measure__Measurement
```

**Recommended usage:**

```python
from osbot_utils.testing.performance.Performance_Measure__Session import Perf

with Perf() as _:
    _.measure__fast(my_function).print()
```

---

## Core Concepts

### The Measurement Loop Constant

```python
MEASURE__INVOCATION__LOOPS = [1, 2, 3, 5, 8, 13, 21, 34, 55, 89, 144, 233, 377, 610]
```

This Fibonacci sequence provides:
- **Rapid early sampling** (1, 2, 3, 5, 8) — catches cold-start behavior
- **Logarithmic scaling** — captures warm-up and cache effects
- **Large final samples** (233, 377, 610) — statistical stability
- **Total: 1,595 invocations** per `measure()` call

### Score vs Raw Score

| Metric | Description | Use Case |
|--------|-------------|----------|
| `raw_score` | Weighted median-mean after outlier removal | Debugging, detailed analysis |
| `final_score` | Normalized to magnitude-appropriate precision | Assertions, comparisons |

**Why two scores?** Raw scores vary slightly between runs (1,847ns vs 1,912ns). Final scores are normalized (both → 2,000ns) for stable assertions.

### CI Environment Handling

When running in GitHub Actions, assertions automatically adjust:

- `assert_time()` — Uses last expected time × 5 as upper bound
- `assert_time__less_than()` — Multiplies threshold by 6

This accounts for GitHub's slower, shared runners without changing your test code.

---

## Architecture

The system consists of three `Type_Safe` classes:

```
┌─────────────────────────────────────────────────────────────────────┐
│                   Performance_Measure__Session                       │
│  ─────────────────────────────────────────────────────────────────  │
│  result         : Model__Performance_Measure__Result                │
│  assert_enabled : bool                                              │
│  padding        : int                                               │
│  ─────────────────────────────────────────────────────────────────  │
│  measure()      → runs Fibonacci-based sampling                     │
│  print()        → formatted output                                  │
│  assert_time()  → validate against expected values                  │
└────────────────────────────────────────────────────────────────────┘
                              │
                              ▼
┌─────────────────────────────────────────────────────────────────────┐
│                 Model__Performance_Measure__Result                   │
│  ─────────────────────────────────────────────────────────────────  │
│  measurements : Dict[int, Model__Performance_Measure__Measurement]  │
│  name         : str                                                 │
│  raw_score    : float                                               │
│  final_score  : float                                               │
└────────────────────────────────────────────────────────────────────┘
                              │
                              ▼
┌─────────────────────────────────────────────────────────────────────┐
│              Model__Performance_Measure__Measurement                 │
│  ─────────────────────────────────────────────────────────────────  │
│  avg_time     : int       # Average time in nanoseconds             │
│  min_time     : int       # Minimum time observed                   │
│  max_time     : int       # Maximum time observed                   │
│  median_time  : int       # Median time                             │
│  stddev_time  : float     # Standard deviation                      │
│  raw_times    : List[int] # Raw measurements for analysis           │
│  sample_size  : int       # Number of measurements taken            │
│  score        : float     # Normalized score for this sample        │
│  raw_score    : float     # Unrounded score                         │
└────────────────────────────────────────────────────────────────────┘
```

## Measurement Algorithm

### Raw Score Calculation

```python
def calculate_raw_score(self, times: List[int]) -> int:
    sorted_times = sorted(times)                    # Sort for analysis
    trim_size    = max(1, len(times) // 10)         # Remove ~10% outliers from each end
    trimmed      = sorted_times[trim_size:-trim_size]
    med          = median(trimmed)
    trimmed_mean = mean(trimmed)
    raw_score    = int(med * 0.6 + trimmed_mean * 0.4)  # Weighted: 60% median, 40% mean
    return raw_score
```

**Why this approach?**
- Removes outliers (GC pauses, context switches)
- Median dominance (0.6 weight) resists extreme values
- Mean contribution (0.4 weight) captures overall distribution

### Stable Score Normalization

```python
def calculate_stable_score(self, raw_score: float) -> int:
    if raw_score < 1_000:                                    # Under 1µs
        return int(round(raw_score / 100) * 100)             # Round to nearest 100ns
    elif raw_score < 10_000:                                 # Under 10µs
        return int(round(raw_score / 1000) * 1000)           # Round to nearest 1000ns
    elif raw_score < 100_000:                                # Under 100µs
        return int(round(raw_score / 10000) * 10000)         # Round to nearest 10000ns
    else:                                                    # Above 100µs
        return int(round(raw_score / 100000) * 100000)       # Round to nearest 100000ns
```

**Why dynamic normalization?** Makes scores stable across runs while maintaining meaningful precision at each magnitude.

## Usage Patterns

### Pattern 1: Basic Measurement (Context Manager)

```python
from osbot_utils.testing.performance.Performance_Measure__Session import Perf

def my_function():
    return [i * 2 for i in range(100)]

with Perf() as _:
    _.measure(my_function).print()
```

Output:
```
my_function                    | score:   2,000 ns  | raw:   1,847 ns
```

### Pattern 2: Reusable Session (Recommended for Test Suites)

```python
class test_performance(TestCase):

    @classmethod
    def setUpClass(cls):
        cls.session = Perf(assert_enabled=True)

    def test_basic_operations(self):
        def operation_a(): ...
        def operation_b(): ...

        with self.session as _:
            _.measure(operation_a).print().assert_time(1_000, 2_000)
            _.measure(operation_b).print().assert_time(5_000)
```

### Pattern 3: Measuring Class Instantiation

```python
from osbot_utils.type_safe.Type_Safe import Type_Safe

class MyClass(Type_Safe):
    name  : str
    count : int

with Perf() as _:
    _.measure(MyClass).print()  # Measures ctor performance
```

### Pattern 4: Chained Methods (Fluent API)

```python
with Perf() as _:
    _.measure(target).print().assert_time(1_000, 2_000)      # Print then assert
    _.measure(target).assert_time(1_000)                      # Assert without print
    _.measure(target).print(40)                               # Custom padding (40 chars)
```

### Pattern 5: Cross-Environment Assertions

```python
# For cross-environment stability (local vs CI)
session.measure(target).assert_time(
    1_000,      # Fast local machine
    2_000,      # Slower machine
    10_000      # GitHub Actions (used as base for 5x multiplier)
)
```

### Pattern 6: Quick Mode for Slow Functions

```python
# For functions that take >100ms, use quick mode to avoid long test times
def test_slow_initialization():
    def init_heavy_object():
        HeavyObject()  # Takes ~1.8 seconds

    with Perf() as _:
        # Quick mode: 19 invocations instead of 1,595
        _.measure__quick(init_heavy_object).print().assert_time__less_than(2_000_000_000)
```

**Measurement Methods:**

| Method | Invocations | Use Case |
|--------|-------------|----------|
| `measure(target)` | 1,595 | Default, full precision |
| `measure__fast(target)` | 87 | Balanced speed/precision |
| `measure__quick(target)` | 19 | Slow functions (>100ms) |

### Pattern 7: Custom Loop Configuration

```python
# For complete control, pass any list of loop sizes
with Perf() as _:
    _.measure(target, loops=[1, 3, 10])           # Only 14 invocations
    _.measure(target, loops=[100])                 # Single batch of 100
    _.measure(target, loops=[1, 1, 1, 1, 1])       # 5 single measurements
```

### Pattern 8: Detailed Report with Histogram

```python
with Perf() as _:
    _.measure(my_function).print_report()
```

Output:
```
────────────────────────────────────────────────────────────
  Performance Report: my_function
────────────────────────────────────────────────────────────

  Score        :     2.000 µs   (normalized)
  Raw Score    :     1.996 µs   (actual)

  Samples      :        1,595
  Min          :     1.883 µs
  Max          :    12.160 µs
  Average      :     2.136 µs
  Median       :     1.994 µs
  Std Dev      :       830 ns

  Variance     : 6.5x   (max/min ratio)

  Distribution:                               count       %

    1.883 µs - 2.910 µs   │████████████████████████████████████████  1550 ( 97.0%) ◀ score
    2.910 µs - 3.938 µs   │                                            20 (  1.3%)
    3.938 µs - 4.966 µs   │                                            10 (  0.6%)
    ...
────────────────────────────────────────────────────────────
```

The `◀ score` marker shows which bin contains the calculated score.

---

## Common Recipes

### Recipe: Comparing Two Implementations

```python
def compare_implementations():
    def impl_a():
        return sorted(data)
    
    def impl_b():
        return list(sorted(data))
    
    with Perf() as _:
        print()
        _.measure(impl_a).print(20)
        _.measure(impl_b).print(20)
```

### Recipe: Exploratory Performance Analysis

```python
# Disable assertions to explore without test failures
session = Perf(assert_enabled=False)

with session as _:
    print("\nBaseline measurements:")
    _.measure(operation_v1).print()
    _.measure(operation_v2).print()
    _.measure(operation_v3).print()
```

### Recipe: Measuring Slow Initialization

```python
# For heavy objects that take >100ms to initialize
with Perf() as _:
    _.measure__quick(HeavyClass).print()           # 19 invocations
    _.measure__quick(AnotherHeavy).print()

# For medium-slow functions, use fast mode for better precision
with Perf() as _:
    _.measure__fast(MediumClass).print()           # 87 invocations
```

### Recipe: Measuring Type_Safe Inheritance Impact

```python
class Base(Type_Safe):
    value: str

class Level1(Base):
    extra: int

class Level2(Level1):
    more: float

with Perf() as _:
    print("\nInheritance depth impact:")
    _.measure(Base  ).print()   # Fastest
    _.measure(Level1).print()   # Slower
    _.measure(Level2).print()   # Slowest
```

### Recipe: Debugging Performance Issues

```python
# Use print_report() to understand distribution and outliers
with Perf() as _:
    _.measure(suspicious_function).print_report()

# If variance is high (e.g., 10x+), you may have:
# - GC pauses affecting some runs
# - Cold cache on first iterations
# - External I/O or network calls
# - Contention from other processes
```

---

## API Reference

### Performance_Measure__Session

| Method | Description | Returns |
|--------|-------------|---------|
| `measure(target, loops=None)` | Run measurements on target (default: 1,595 invocations) | `self` (for chaining) |
| `measure__quick(target)` | Quick mode - 19 invocations (for slow functions >100ms) | `self` (for chaining) |
| `measure__fast(target)` | Fast mode - 87 invocations (balanced speed/precision) | `self` (for chaining) |
| `print(padding=None)` | Print one-line result | `self` (for chaining) |
| `print_report(bins=10, width=40)` | Print detailed report with histogram | `self` (for chaining) |
| `assert_time(*expected)` | Assert final_score matches one of expected values | `self` (for chaining) |
| `assert_time__less_than(max)` | Assert final_score is below threshold | `self` (for chaining) |
| `assert_time__more_than(min)` | Assert final_score is above threshold | `self` (for chaining) |

### Constructor Parameters

| Parameter | Type | Default | Description |
|-----------|------|---------|-------------|
| `assert_enabled` | `bool` | `True` | Enable/disable assertion methods |
| `padding` | `int` | `30` | Name column width for `print()` |

### measure() Parameters

| Parameter | Type | Default | Description |
|-----------|------|---------|-------------|
| `target` | `Callable` | required | Function or class to measure |
| `loops` | `List[int]` | `None` | Custom loop sizes (default uses `MEASURE__INVOCATION__LOOPS`) |

---

## Best Practices

### DO: Use Class-Level Session in Test Suites

```python
# ✅ Good - session reused across tests
class test_performance(TestCase):
    @classmethod
    def setUpClass(cls):
        cls.session = Perf(assert_enabled=True)
    
    def test_one(self):
        with self.session as _:
            _.measure(func_a).assert_time(1_000)
    
    def test_two(self):
        with self.session as _:
            _.measure(func_b).assert_time(2_000)
```

### DO: Define Time Thresholds as Class Attributes

```python
# ✅ Good - readable, reusable thresholds
cls.time_1_kns  =   1_000    # 1µs
cls.time_5_kns  =   5_000    # 5µs
cls.time_10_kns =  10_000    # 10µs

session.measure(target).assert_time(self.time_1_kns, self.time_5_kns)
```

### DO: Provide Multiple Expected Values for CI Stability

```python
# ✅ Good - handles different environments
session.measure(target).assert_time(
    1_000,      # Local fast machine
    2_000,      # Local slower machine
    10_000      # GitHub Actions (last value × 5 used as upper bound)
)
```

### DO: Use `assert_time__less_than` for Upper Bounds

```python
# ✅ Good - when you only care about maximum acceptable time
session.measure(target).assert_time__less_than(30_000)
```

### DO: Use Quick Mode for Slow Functions

```python
# ✅ Good - use measure__quick for slow functions (>100ms)
session.measure__quick(slow_init).print()

# ✅ Also good - use measure__fast for medium-slow functions
session.measure__fast(medium_init).print()

# ❌ Bad - full 1,595 iterations of a 1s function = 26 minutes!
session.measure(slow_init)  # Don't do this for slow functions
```

### DON'T: Create New Sessions Inside Each Test

```python
# ❌ Bad - unnecessary overhead
def test_something(self):
    session = Perf()  # Creates new session each time
    with session as _:
        _.measure(func).assert_time(1_000)
```

### DON'T: Measure Functions with Side Effects

```python
# ❌ Bad - file I/O timing varies wildly
def writes_to_disk():
    with open('/tmp/file.txt', 'w') as f:
        f.write('data')

session.measure(writes_to_disk)  # Results will be inconsistent
```

### DON'T: Use Raw Literal Values Without Convention

```python
# ❌ Bad - magic numbers
session.measure(func).assert_time(2000, 5000, 10000)

# ✅ Good - clear naming convention
session.measure(func).assert_time(self.time_2_kns, self.time_5_kns, self.time_10_kns)
```

### DON'T: Skip Output When Debugging

```python
# ❌ Bad - hard to debug assertion failures
session.measure(target).assert_time(1_000)

# ✅ Good - see actual values when debugging
session.measure(target).print().assert_time(1_000)

# ✅ Even better - see full distribution for analysis
session.measure(target).print_report().assert_time(1_000)
```

---

## Troubleshooting

### Problem: Assertion Fails Locally But Passes in CI

**Cause**: Local machine is faster than expected CI thresholds

**Solution**: Add your local timing as first expected value:

```python
# Your local machine runs at 800ns, CI expects 2000ns
session.measure(target).assert_time(
    1_000,      # Local (rounded to nearest 1000)
    2_000       # CI baseline
)
```

### Problem: Assertion Passes Locally But Fails in CI

**Cause**: CI is slower than the 5x multiplier allows

**Solution 1**: Add a higher expected value:
```python
session.measure(target).assert_time(
    1_000,      # Local
    2_000,
    5_000       # Higher baseline for CI (5x = 25,000ns max)
)
```

**Solution 2**: Use `assert_time__less_than` instead:
```python
session.measure(target).assert_time__less_than(10_000)  # CI gets 6x = 60,000ns
```

### Problem: Inconsistent Scores Between Runs

**Cause 1**: Function has variable execution time (I/O, network, randomness)

**Solution**: Measure pure computational functions only

**Cause 2**: System under heavy load

**Solution**: Run tests in isolation, close other applications

### Problem: Score Is 0ns

**Cause**: Function is too fast to measure (sub-100ns)

**Solution**: Wrap in a loop or measure a larger unit of work:
```python
def measure_100_iterations():
    for _ in range(100):
        tiny_function()

session.measure(measure_100_iterations).print()
# Then divide result by 100 mentally
```

### Problem: `assert_enabled=False` Not Working

**Cause**: Session created before setting `assert_enabled`

**Solution**: Pass to constructor:
```python
# ✅ Correct
session = Perf(assert_enabled=False)

# ❌ Won't work - attribute is set at construction
session = Perf()
session.assert_enabled = False  # Too late
```

### Problem: Can't Compare Results Across Python Versions

**Cause**: Different Python versions have different performance characteristics

**Solution**: Accept this as expected; use version-specific thresholds if needed, or use `assert_time__less_than` for broader compatibility.

---

## Summary Checklist

When using Performance_Measure__Session (or `Perf` alias):

- [ ] Import `Perf` from `osbot_utils.testing.performance.Performance_Measure__Session`
- [ ] Create session once in `setUpClass()`, not per-test
- [ ] Use context manager pattern: `with self.session as _:`
- [ ] Define time thresholds with naming convention: `time_X_kns`
- [ ] Provide multiple expected values for cross-environment stability
- [ ] Put fastest expected value first, CI baseline last
- [ ] Use `.print()` for one-line output during development
- [ ] Use `.print_report()` for detailed analysis with histogram
- [ ] Use `assert_time()` for exact normalized values
- [ ] Use `assert_time__less_than()` for upper-bound checks
- [ ] Set `assert_enabled=False` for exploratory analysis
- [ ] Measure pure functions (no I/O, no network, no randomness)
- [ ] Use `measure__quick()` for slow functions (>100ms)
- [ ] Use `measure__fast()` for balanced speed/precision
- [ ] Remember: CI gets 5-6x multiplier automatically
- [ ] Remember: 1,595 invocations per `measure()` call (or fewer with `loops` parameter)
- [ ] Remember: Scores are normalized by magnitude (100ns, 1000ns, 10000ns, 100000ns)
