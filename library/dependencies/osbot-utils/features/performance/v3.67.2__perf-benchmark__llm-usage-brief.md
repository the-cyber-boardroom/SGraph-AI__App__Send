# Perf_Benchmark - LLM Usage Brief

**Version**: v3.61.0  
**Purpose**: Guide for LLMs and developers on using the type-safe performance benchmarking framework  
**Location**: `osbot_utils.helpers.performance.benchmark`  
**Repo**: https://github.com/owasp-sbot/OSBot-Utils  
**Install**: `pip install osbot-utils`

---

## What is Perf_Benchmark?

**Perf_Benchmark is a type-safe benchmarking framework that builds on Performance_Measure__Session to provide structured comparison, trend tracking, and multi-format reporting of benchmark results.** While `Performance_Measure__Session` (Perf) gives you stable timing measurements, Perf_Benchmark adds the ability to compare sessions over time, detect regressions, and export results in multiple formats.

### The Problem It Solves

Raw timing measurements are useful, but real-world benchmarking needs more:

```python
# With Performance_Measure__Session - you get stable timings
with Perf() as _:
    _.measure(my_function).print()  # Output: my_function | score: 2,000 ns

# But what about:
# - Did performance regress since last release?
# - How do different implementations compare?
# - Is the improvement statistically significant?
# - Can I export results for CI dashboards?
```

**With Perf_Benchmark:**

```python
from osbot_utils.helpers.performance.benchmark.Perf_Benchmark__Diff import Perf_Benchmark__Diff

diff = Perf_Benchmark__Diff()
diff.load_folder('/benchmarks/')

# Compare two sessions
comparison = diff.compare_two()
print(comparison.status)  # Enum__Comparison__Status.SUCCESS

# See evolution across all sessions
evolution = diff.compare_all()
for bench in evolution.evolutions:
    print(f"{bench.name}: {bench.trend}")  # "dict_create: IMPROVEMENT"

# Get statistics
stats = diff.statistics()
print(f"Regressions: {stats.regression_count}, Improvements: {stats.improvement_count}")
```

### Design Philosophy

1. **Schema-driven** — All results are structured `Type_Safe` schemas, not formatted strings
2. **Calculation separated from presentation** — Get data, then choose how to display it
3. **Status enums for error handling** — Clear, type-safe error states
4. **Multiple export formats** — Text, HTML, JSON from the same data
5. **Built on Performance_Measure__Session** — Leverages proven measurement infrastructure

---

## Quick Start

### 1. Run Benchmarks and Save Results

```python
from osbot_utils.helpers.performance.benchmark.Perf_Benchmark__Timing import Perf_Benchmark__Timing

config = Schema__Perf_Benchmark__Timing__Config(
    title       = 'My Benchmark Suite',
    output_path = '/benchmarks/'
)

with Perf_Benchmark__Timing(config=config) as timing:
    timing.benchmark('A_01__dict_create'  , dict)
    timing.benchmark('A_02__list_create'  , list)
    timing.benchmark('B_01__set_create'   , set)
    
    timing.reporter().save_all()  # Saves .txt, .json, .md, .html
```

### 2. Compare Sessions Over Time

```python
from osbot_utils.helpers.performance.benchmark.Perf_Benchmark__Diff import Perf_Benchmark__Diff

diff = Perf_Benchmark__Diff()
diff.load_folder('/benchmarks/')  # Loads all .json files

# Get structured comparison
comparison = diff.compare_two()
if comparison.status == Enum__Comparison__Status.SUCCESS:
    for comp in comparison.comparisons:
        print(f"{comp.name}: {comp.change_percent}% ({comp.trend})")
```

### 3. Export in Multiple Formats

```python
from osbot_utils.helpers.performance.benchmark.export.Perf_Benchmark__Export__Text import Perf_Benchmark__Export__Text
from osbot_utils.helpers.performance.benchmark.export.Perf_Benchmark__Export__HTML import Perf_Benchmark__Export__HTML

text_export = Perf_Benchmark__Export__Text()
html_export = Perf_Benchmark__Export__HTML()

# Same data, different formats
print(text_export.export_comparison(comparison))
file_create('report.html', html_export.export_evolution(evolution))
```

### 4. That's It

The framework handles all the complexity: type-safe schemas, trend detection, percentage calculations, and format-specific rendering.

> **Tip**: Use `Perf_Benchmark__Hypothesis` for CI/CD integration with pass/fail assertions.

---

## Import Reference

```python
# Core timing engine
from osbot_utils.helpers.performance.benchmark.Perf_Benchmark__Timing          import Perf_Benchmark__Timing

# Multi-session comparison
from osbot_utils.helpers.performance.benchmark.Perf_Benchmark__Diff            import Perf_Benchmark__Diff

# Hypothesis testing for CI/CD
from osbot_utils.helpers.performance.benchmark.Perf_Benchmark__Hypothesis      import Perf_Benchmark__Hypothesis

# Report generation
from osbot_utils.helpers.performance.benchmark.Perf_Benchmark__Timing__Reporter import Perf_Benchmark__Timing__Reporter

# Export formats
from osbot_utils.helpers.performance.benchmark.export.Perf_Benchmark__Export__Text import Perf_Benchmark__Export__Text
from osbot_utils.helpers.performance.benchmark.export.Perf_Benchmark__Export__HTML import Perf_Benchmark__Export__HTML
from osbot_utils.helpers.performance.benchmark.export.Perf_Benchmark__Export__JSON import Perf_Benchmark__Export__JSON

# Key schemas
from osbot_utils.helpers.performance.benchmark.schemas.Schema__Perf__Comparison__Two import Schema__Perf__Comparison__Two
from osbot_utils.helpers.performance.benchmark.schemas.Schema__Perf__Evolution       import Schema__Perf__Evolution
from osbot_utils.helpers.performance.benchmark.schemas.Schema__Perf__Statistics      import Schema__Perf__Statistics

# Enums
from osbot_utils.helpers.performance.benchmark.schemas.enums.Enum__Comparison__Status import Enum__Comparison__Status
from osbot_utils.helpers.performance.benchmark.schemas.enums.Enum__Benchmark__Trend   import Enum__Benchmark__Trend

# Config
from osbot_utils.helpers.performance.benchmark.schemas.timing.Schema__Perf_Benchmark__Timing__Config import Schema__Perf_Benchmark__Timing__Config
```

**Recommended minimal import:**

```python
from osbot_utils.helpers.performance.benchmark.Perf_Benchmark__Timing import Perf_Benchmark__Timing
from osbot_utils.helpers.performance.benchmark.Perf_Benchmark__Diff   import Perf_Benchmark__Diff
```

---

## Core Concepts

### Schemas vs Formatted Strings

Previous approaches returned formatted strings:

```python
# ❌ Old approach - returns formatted string
result = diff.compare_two()  # "Session A vs Session B\n..."
# Can't inspect data programmatically
```

Perf_Benchmark returns schemas:

```python
# ✅ New approach - returns structured schema
result = diff.compare_two()  # Schema__Perf__Comparison__Two

result.status           # Enum__Comparison__Status.SUCCESS
result.title_a          # "Session 1"
result.title_b          # "Session 2"
result.comparisons[0]   # Schema__Perf__Benchmark__Comparison
result.comparisons[0].change_percent  # -10.5
result.comparisons[0].trend           # Enum__Benchmark__Trend.IMPROVEMENT
```

### Status Enums for Error Handling

Every comparison method returns a status:

```python
class Enum__Comparison__Status(Enum):
    SUCCESS                     = 'success'
    ERROR_NO_SESSIONS           = 'error_no_sessions'
    ERROR_INSUFFICIENT_SESSIONS = 'error_insufficient_sessions'
    ERROR_NO_COMMON_BENCHMARKS  = 'error_no_common_benchmarks'
```

Usage:

```python
result = diff.compare_two()

if result.status == Enum__Comparison__Status.SUCCESS:
    # Process comparisons
    for comp in result.comparisons:
        print(f"{comp.name}: {comp.trend}")
        
elif result.status == Enum__Comparison__Status.ERROR_INSUFFICIENT_SESSIONS:
    print(f"Error: {result.error}")  # Human-readable message
```

### Trend Detection

Performance changes are classified into 5 levels:

| Trend | Symbol | Change |
|-------|--------|--------|
| `STRONG_IMPROVEMENT` | ▼▼▼ | > 10% faster |
| `IMPROVEMENT` | ▼ | 0-10% faster |
| `UNCHANGED` | ─ | 0% change |
| `REGRESSION` | ▲ | 0-10% slower |
| `STRONG_REGRESSION` | ▲▲▲ | > 10% slower |

```python
for comp in comparison.comparisons:
    if comp.trend == Enum__Benchmark__Trend.STRONG_REGRESSION:
        print(f"⚠️ ALERT: {comp.name} regressed {comp.change_percent}%")
```

### Dual Storage in Timing

`Perf_Benchmark__Timing` stores both summary results and full measurement data:

```python
with Perf_Benchmark__Timing(config=config) as timing:
    timing.benchmark('A_01__test', my_function)
    
    # Quick access to scores
    timing.results['A_01__test'].final_score  # 2000
    
    # Deep analysis - full measurement data
    session = timing.sessions['A_01__test']
    session.result.measurements[8].stddev_time   # Standard deviation for 8-iteration sample
    session.result.measurements[610].raw_times   # All 610 raw measurements
```

---

## Architecture

```
┌─────────────────────────────────────────────────────────────────────────────┐
│                         Perf_Benchmark__Timing                               │
│  ─────────────────────────────────────────────────────────────────────────  │
│  config   : Schema__Perf_Benchmark__Timing__Config                          │
│  results  : Dict__Benchmark_Results        (lightweight summaries)          │
│  sessions : Dict__Benchmark_Sessions       (full measurement data)          │
│  ─────────────────────────────────────────────────────────────────────────  │
│  benchmark(id, target, threshold) → Schema__Perf__Benchmark__Result         │
│  reporter() → Perf_Benchmark__Timing__Reporter                              │
└─────────────────────────────────────────────────────────────────────────────┘
                                    │
                    ┌───────────────┴───────────────┐
                    ▼                               ▼
┌───────────────────────────────┐   ┌───────────────────────────────────────┐
│  Dict__Benchmark_Results       │   │  Dict__Benchmark_Sessions              │
│  ───────────────────────────  │   │  ───────────────────────────────────  │
│  'A_01__test' → Result         │   │  'A_01__test' → Performance_Measure    │
│  'A_02__test' → Result         │   │  'A_02__test' → Performance_Measure    │
│  'B_01__test' → Result         │   │  'B_01__test' → Performance_Measure    │
└───────────────────────────────┘   └───────────────────────────────────────┘
        │                                       │
        │ final_score, raw_score                │ measurements, raw_times,
        │ section, index, name                  │ stddev, min, max, median
        ▼                                       ▼
   Quick assertions               Deep statistical analysis


┌─────────────────────────────────────────────────────────────────────────────┐
│                          Perf_Benchmark__Diff                                │
│  ─────────────────────────────────────────────────────────────────────────  │
│  sessions : List__Benchmark_Sessions                                        │
│  ─────────────────────────────────────────────────────────────────────────  │
│  load_session(filepath) → self                                              │
│  load_folder(path) → self                                                   │
│  compare_two(a, b) → Schema__Perf__Comparison__Two                          │
│  compare_all() → Schema__Perf__Evolution                                    │
│  statistics() → Schema__Perf__Statistics                                    │
└─────────────────────────────────────────────────────────────────────────────┘
                                    │
                    ┌───────────────┼───────────────┐
                    ▼               ▼               ▼
          Schema__Perf__    Schema__Perf__   Schema__Perf__
          Comparison__Two    Evolution        Statistics


┌─────────────────────────────────────────────────────────────────────────────┐
│                          Export System                                       │
│  ─────────────────────────────────────────────────────────────────────────  │
│  Perf_Benchmark__Export (base)                                              │
│      ├── export_comparison(Schema) → str                                    │
│      ├── export_evolution(Schema) → str                                     │
│      └── export_statistics(Schema) → str                                    │
│                                                                             │
│  Implementations:                                                           │
│      ├── Perf_Benchmark__Export__Text  (Print_Table, ASCII)                 │
│      ├── Perf_Benchmark__Export__HTML  (Chart.js visualizations)            │
│      └── Perf_Benchmark__Export__JSON  (Serialized schemas)                 │
└─────────────────────────────────────────────────────────────────────────────┘
```

---

## Usage Patterns

### Pattern 1: Basic Benchmark Suite

```python
from osbot_utils.helpers.performance.benchmark.Perf_Benchmark__Timing import Perf_Benchmark__Timing
from osbot_utils.helpers.performance.benchmark.schemas.timing.Schema__Perf_Benchmark__Timing__Config import Schema__Perf_Benchmark__Timing__Config

config = Schema__Perf_Benchmark__Timing__Config(
    title            = 'Type_Safe Performance Suite',
    print_to_console = True
)

with Perf_Benchmark__Timing(config=config) as timing:
    # Section A: Python baselines
    timing.benchmark('A_01__dict_create' , dict)
    timing.benchmark('A_02__list_create' , list)
    timing.benchmark('A_03__set_create'  , set)
    
    # Section B: Custom functions
    timing.benchmark('B_01__my_function' , my_function)
    timing.benchmark('B_02__my_class'    , MyClass)
    
    # Print summary
    timing.reporter().print_summary()
```

### Pattern 2: Threshold Assertions

```python
# Define time constants
time_500_ns  =     500
time_1_kns   =   1_000
time_10_kns  =  10_000
time_100_kns = 100_000

with Perf_Benchmark__Timing(config=config) as timing:
    # Assert benchmarks stay under thresholds
    timing.benchmark('A_01__fast_op', fast_function, assert_less_than=time_1_kns)
    timing.benchmark('A_02__slow_op', slow_function, assert_less_than=time_100_kns)
```

### Pattern 3: Save and Compare Sessions

```python
# Session 1: Save baseline
config = Schema__Perf_Benchmark__Timing__Config(
    title       = 'Baseline v1.0',
    output_path = '/benchmarks/'
)

with Perf_Benchmark__Timing(config=config) as timing:
    timing.benchmark('A_01__test', my_function)
    timing.reporter().save_json('/benchmarks/baseline_v1.json')

# Later: Session 2 after changes
config.title = 'After Optimization v1.1'

with Perf_Benchmark__Timing(config=config) as timing:
    timing.benchmark('A_01__test', my_function)
    timing.reporter().save_json('/benchmarks/optimized_v1.1.json')

# Compare
diff = Perf_Benchmark__Diff()
diff.load_session('/benchmarks/baseline_v1.json')
diff.load_session('/benchmarks/optimized_v1.1.json')

comparison = diff.compare_two()
for comp in comparison.comparisons:
    print(f"{comp.name}: {comp.change_percent:+.1f}% {comp.trend.value}")
```

### Pattern 4: Track Evolution Over Time

```python
diff = Perf_Benchmark__Diff()
diff.load_folder('/benchmarks/')  # Loads all .json files chronologically

evolution = diff.compare_all()

print(f"Tracking {evolution.session_count} sessions")
for ev in evolution.evolutions:
    scores = [int(s) for s in ev.scores]
    print(f"{ev.name}: {scores} → trend: {ev.trend.value}")
```

### Pattern 5: Get Summary Statistics

```python
diff = Perf_Benchmark__Diff()
diff.load_folder('/benchmarks/')

stats = diff.statistics()

print(f"Sessions analyzed: {stats.session_count}")
print(f"Benchmarks tracked: {stats.benchmark_count}")
print(f"Improvements: {stats.improvement_count} (avg {stats.avg_improvement:.1f}%)")
print(f"Regressions: {stats.regression_count} (avg {stats.avg_regression:.1f}%)")

if stats.best_improvement:
    print(f"Best: {stats.best_improvement.name} ({stats.best_improvement.change_percent:+.1f}%)")
if stats.worst_regression:
    print(f"Worst: {stats.worst_regression.name} ({stats.worst_regression.change_percent:+.1f}%)")
```

### Pattern 6: Hypothesis Testing for CI/CD

```python
from osbot_utils.helpers.performance.benchmark.Perf_Benchmark__Hypothesis import Perf_Benchmark__Hypothesis

# Load baseline and run current benchmarks
hypothesis = Perf_Benchmark__Hypothesis(tolerance=10.0)  # 10% tolerance
hypothesis.load_baseline('/benchmarks/baseline.json')

with Perf_Benchmark__Timing(config=config) as timing:
    timing.benchmark('A_01__test', my_function)
    hypothesis.timing = timing

# Test for regressions
result = hypothesis.test_no_regression()

if result.status == Enum__Hypothesis__Status.FAILED:
    print(f"❌ Regression detected: {result.comments}")
    exit(1)
else:
    print(f"✅ No regressions (within {hypothesis.tolerance}% tolerance)")
```

### Pattern 7: Export to Multiple Formats

```python
from osbot_utils.helpers.performance.benchmark.export.Perf_Benchmark__Export__Text import Perf_Benchmark__Export__Text
from osbot_utils.helpers.performance.benchmark.export.Perf_Benchmark__Export__HTML import Perf_Benchmark__Export__HTML
from osbot_utils.helpers.performance.benchmark.export.Perf_Benchmark__Export__JSON import Perf_Benchmark__Export__JSON

diff = Perf_Benchmark__Diff()
diff.load_folder('/benchmarks/')

comparison = diff.compare_two()
evolution  = diff.compare_all()
stats      = diff.statistics()

# Text export (for console/logs)
text = Perf_Benchmark__Export__Text()
print(text.export_comparison(comparison))
print(text.export_statistics(stats))

# HTML export (for dashboards)
html = Perf_Benchmark__Export__HTML()
file_create('evolution.html', html.export_evolution(evolution))

# JSON export (for APIs/storage)
json_export = Perf_Benchmark__Export__JSON()
file_create('stats.json', json_export.export_statistics(stats))
```

### Pattern 8: Using with TestCase

```python
from osbot_utils.helpers.performance.benchmark.TestCase__Benchmark__Timing import TestCase__Benchmark__Timing

class test_My_Performance(TestCase__Benchmark__Timing):
    
    def test_dict_creation(self):
        self.timing.benchmark('A_01__dict_create', dict)
        self.timing.benchmark('A_02__list_create', list)
        
        # Access results
        assert self.timing.results['A_01__dict_create'].final_score < 5000
```

---

## API Reference

### Perf_Benchmark__Timing

| Method | Parameters | Returns | Description |
|--------|------------|---------|-------------|
| `benchmark(id, target, assert_less_than)` | `Safe_Str__Benchmark_Id`, `Callable`, `Optional[int]` | `Schema__Perf__Benchmark__Result` | Run benchmark and store result |
| `reporter()` | — | `Perf_Benchmark__Timing__Reporter` | Create reporter for output generation |

| Attribute | Type | Description |
|-----------|------|-------------|
| `config` | `Schema__Perf_Benchmark__Timing__Config` | Configuration options |
| `results` | `Dict__Benchmark_Results` | Lightweight result summaries |
| `sessions` | `Dict__Benchmark_Sessions` | Full measurement data |

### Perf_Benchmark__Diff

| Method | Parameters | Returns | Description |
|--------|------------|---------|-------------|
| `load_session(filepath)` | `Safe_Str__File__Path` | `self` | Load single session JSON |
| `load_folder(path)` | `Safe_Str__File__Path` | `self` | Load all .json files in folder |
| `compare_two(a, b)` | `Optional[int]`, `Optional[int]` | `Schema__Perf__Comparison__Two` | Compare two sessions |
| `compare_all()` | — | `Schema__Perf__Evolution` | Track evolution across all sessions |
| `statistics()` | — | `Schema__Perf__Statistics` | Calculate summary statistics |

### Perf_Benchmark__Export__*

| Method | Parameters | Returns | Description |
|--------|------------|---------|-------------|
| `export_comparison(schema)` | `Schema__Perf__Comparison__Two` | `str` | Format comparison result |
| `export_evolution(schema)` | `Schema__Perf__Evolution` | `str` | Format evolution data |
| `export_statistics(schema)` | `Schema__Perf__Statistics` | `str` | Format statistics |

### Perf_Benchmark__Timing__Reporter

| Method | Parameters | Returns | Description |
|--------|------------|---------|-------------|
| `build_text()` | — | `Safe_Str__Text` | Generate text report |
| `build_json()` | — | `dict` | Generate JSON-serializable dict |
| `build_markdown()` | — | `Safe_Str__Markdown` | Generate markdown report |
| `build_html()` | — | `Safe_Str__Html` | Generate HTML with Chart.js |
| `save_all()` | — | `None` | Save all formats to output_path |
| `save_text(filepath)` | `Safe_Str__File__Path` | `None` | Save text file |
| `save_json(filepath)` | `Safe_Str__File__Path` | `None` | Save JSON file |
| `save_markdown(filepath)` | `Safe_Str__File__Path` | `None` | Save markdown file |
| `save_html(filepath)` | `Safe_Str__File__Path` | `None` | Save HTML file |
| `print_summary()` | — | `None` | Print to console |
| `compare(other)` | `Perf_Benchmark__Timing__Reporter` | `Safe_Str__Text` | Compare with another reporter |

---

## Best Practices

### DO: Use Consistent Benchmark IDs

```python
# ✅ Good - clear naming convention
timing.benchmark('A_01__dict_create', dict)
timing.benchmark('A_02__list_create', list)
timing.benchmark('B_01__type_safe_init', Type_Safe)

# Section letter (A, B, C...)
# Index within section (01, 02, 03...)
# Descriptive name (lowercase_with_underscores)
```

### DO: Save Sessions for Historical Comparison

```python
# ✅ Good - save after each benchmark run
with Perf_Benchmark__Timing(config=config) as timing:
    # ... run benchmarks ...
    timing.reporter().save_json(f'/benchmarks/session_{date.today()}.json')
```

### DO: Check Status Before Processing Results

```python
# ✅ Good - handle errors gracefully
comparison = diff.compare_two()

if comparison.status == Enum__Comparison__Status.SUCCESS:
    for comp in comparison.comparisons:
        process(comp)
else:
    print(f"Cannot compare: {comparison.error}")
```

### DO: Use Exporters for Presentation

```python
# ✅ Good - separate data from presentation
comparison = diff.compare_two()  # Get data
text = Perf_Benchmark__Export__Text()
print(text.export_comparison(comparison))  # Format for display
```

### DO: Define Time Thresholds as Constants

```python
# ✅ Good - readable thresholds
time_100_ns  =     100
time_500_ns  =     500
time_1_kns   =   1_000
time_5_kns   =   5_000
time_10_kns  =  10_000
time_100_kns = 100_000
time_1_mns   = 1_000_000

timing.benchmark('A_01__fast', fast_op, assert_less_than=time_1_kns)
```

### DON'T: Mix String and Schema Approaches

```python
# ❌ Bad - treating schema like a string
comparison = diff.compare_two()
if 'error' in str(comparison):  # Don't do this
    handle_error()

# ✅ Good - use schema properties
if comparison.status != Enum__Comparison__Status.SUCCESS:
    print(comparison.error)
```

### DON'T: Forget to Load Sessions Before Comparing

```python
# ❌ Bad - no sessions loaded
diff = Perf_Benchmark__Diff()
comparison = diff.compare_two()  # ERROR_NO_SESSIONS

# ✅ Good - load first
diff = Perf_Benchmark__Diff()
diff.load_folder('/benchmarks/')
comparison = diff.compare_two()  # SUCCESS (if >= 2 sessions)
```

### DON'T: Ignore Trend Information

```python
# ❌ Bad - only checking percentage
if comp.change_percent > 10:
    alert()

# ✅ Good - use trend enum for clarity
if comp.trend == Enum__Benchmark__Trend.STRONG_REGRESSION:
    alert(f"{comp.name} regressed {comp.change_percent}%")
```

### DON'T: Create New Timing Instance Per Benchmark

```python
# ❌ Bad - overhead of creating new instances
for func in functions:
    with Perf_Benchmark__Timing(config=config) as timing:
        timing.benchmark(f'test_{func.__name__}', func)

# ✅ Good - single instance for all benchmarks
with Perf_Benchmark__Timing(config=config) as timing:
    for func in functions:
        timing.benchmark(f'test_{func.__name__}', func)
```

---

## Troubleshooting

### Problem: compare_two() Returns ERROR_NO_SESSIONS

**Cause**: No sessions loaded

**Solution**: Load sessions first

```python
diff = Perf_Benchmark__Diff()
diff.load_folder('/benchmarks/')  # Or load_session() for individual files

# Verify sessions loaded
print(f"Sessions loaded: {len(diff.sessions)}")
```

### Problem: compare_two() Returns ERROR_INSUFFICIENT_SESSIONS

**Cause**: Only one session loaded (need at least 2)

**Solution**: Ensure multiple sessions exist

```python
# Check how many sessions
print(f"Sessions: {len(diff.sessions)}")

# Load more if needed
diff.load_session('/benchmarks/session2.json')
```

### Problem: No Common Benchmarks Found

**Cause**: Sessions have different benchmark IDs

**Solution**: Use consistent benchmark IDs across sessions

```python
# Session 1
timing.benchmark('A_01__test', func)

# Session 2 - use SAME ID
timing.benchmark('A_01__test', func)  # ✅ Same ID
timing.benchmark('A_01__test_v2', func)  # ❌ Different ID - won't compare
```

### Problem: Percentage Shows Repeating Decimals

**Cause**: Using raw float instead of Safe_Float__Percentage_Change

**Solution**: This is handled automatically by the schema

```python
# Schema uses Safe_Float__Percentage_Change which rounds to 2 decimal places
comp.change_percent  # -11.11 (not -11.111111111111)
```

### Problem: HTML Export Missing Chart

**Cause**: Chart.js CDN not loading

**Solution**: Ensure network access, or use text/JSON export

```python
# HTML requires Chart.js from CDN
html = Perf_Benchmark__Export__HTML()
content = html.export_evolution(evolution)

# Alternative: use text export
text = Perf_Benchmark__Export__Text()
print(text.export_evolution(evolution))
```

### Problem: Can't Access Individual Measurements

**Cause**: Using `results` (summaries) instead of `sessions` (full data)

**Solution**: Access the sessions dict for detailed measurements

```python
# Summary only
timing.results['A_01__test'].final_score  # 2000

# Full measurement data
session = timing.sessions['A_01__test']
session.result.measurements[610].stddev_time
session.result.measurements[610].raw_times
```

---

## Summary Checklist

When using Perf_Benchmark:

**Setup:**
- [ ] Import `Perf_Benchmark__Timing` for running benchmarks
- [ ] Import `Perf_Benchmark__Diff` for comparing sessions
- [ ] Import exporters for output formatting
- [ ] Define time threshold constants (`time_1_kns`, `time_10_kns`, etc.)

**Running Benchmarks:**
- [ ] Use consistent ID format: `{Section}_{Index}__{name}` (e.g., `A_01__dict_create`)
- [ ] Use context manager: `with Perf_Benchmark__Timing(config) as timing:`
- [ ] Save sessions with `reporter().save_json()` for future comparison
- [ ] Use `assert_less_than` for threshold enforcement

**Comparing Sessions:**
- [ ] Load sessions with `load_folder()` or `load_session()`
- [ ] Check status enum before processing results
- [ ] Use `compare_two()` for two-session diff
- [ ] Use `compare_all()` for evolution tracking
- [ ] Use `statistics()` for summary metrics

**Exporting Results:**
- [ ] Use `Perf_Benchmark__Export__Text` for console/logs
- [ ] Use `Perf_Benchmark__Export__HTML` for dashboards (Chart.js)
- [ ] Use `Perf_Benchmark__Export__JSON` for APIs/storage
- [ ] Same schema works with all exporters

**Error Handling:**
- [ ] Always check `result.status` before accessing data
- [ ] Handle `ERROR_NO_SESSIONS`, `ERROR_INSUFFICIENT_SESSIONS`
- [ ] Access `result.error` for human-readable messages

**Key Types:**
- [ ] `Schema__Perf__Comparison__Two` — two-session comparison
- [ ] `Schema__Perf__Evolution` — multi-session tracking
- [ ] `Schema__Perf__Statistics` — summary statistics
- [ ] `Enum__Comparison__Status` — error handling
- [ ] `Enum__Benchmark__Trend` — performance trend (5 levels)
