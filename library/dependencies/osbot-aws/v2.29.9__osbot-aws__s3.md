# OSBot-AWS S3: LLM Briefing Document

## Why OSBot-AWS S3?

The native boto3 S3 API is verbose, inconsistent, and requires significant boilerplate for common operations. OSBot-AWS wraps this complexity to provide:

**1. Simplified Method Signatures**
```python
# boto3 native - verbose and error-prone
s3_client = boto3.client('s3')
response = s3_client.get_object(Bucket='my-bucket', Key='folder/file.txt')
content = response['Body'].read().decode('utf-8')

# OSBot-AWS - clean and intuitive
from osbot_aws.aws.s3.S3 import S3
content = S3().file_contents('my-bucket', 'folder/file.txt')
```

**2. Consistent Return Values**
```python
# boto3 native - inconsistent responses, must check for errors manually
try:
    s3_client.head_object(Bucket='bucket', Key='key')
    exists = True
except ClientError:
    exists = False

# OSBot-AWS - boolean returns where expected
exists = S3().file_exists('bucket', 'key')  # True/False
```

**3. Automatic Content-Type Handling**
```python
# boto3 native - must manually set content types
s3_client.put_object(Bucket='b', Key='file.html', Body=content, ContentType='text/html; charset=utf-8')

# OSBot-AWS - auto-detects from file extension
S3().file_upload_to_key('/path/file.html', 'bucket', 'file.html')  # ContentType set automatically
```

**4. Built-in Caching & Session Management**
- Cached boto3 clients (no repeated authentication overhead)
- Automatic credential resolution (env vars, profiles, IAM roles)
- LocalStack compatibility for local development

**5. Higher-Level Abstractions**
- `S3__DB_Base`: Use S3 as an application data store with simple save/load operations
- `S3__Key_Generator`: Time-partitioned key generation for data pipelines
- Pre-signed URL helpers for secure temporary access

## Overview

The S3 module in OSBot-AWS provides comprehensive S3 operations through multiple classes:

| Class | Purpose |
|-------|---------|
| `S3` | Core S3 operations (files, buckets, policies) |
| `S3__DB_Base` | S3-backed storage abstraction for applications |
| `S3__Key_Generator` | S3 key path generation with time-based partitioning |
| `S3__On_Temp_Bucket` | Temporary bucket operations |
| `S3__Zip_Bytes` | Zip file operations with S3 |

## Core S3 Class

### Import and Initialization

```python
from osbot_aws.aws.s3.S3 import S3

s3 = S3()                               # Uses default credentials
```

### Custom Endpoint (LocalStack)

```python
from osbot_aws.aws.s3.S3 import S3
from osbot_aws.aws.session.Session__Kwargs__S3 import Session__Kwargs__S3

session_kwargs = Session__Kwargs__S3(
    endpoint_url = 'http://localhost:4566',      # LocalStack default
    region_name  = 'us-east-1'
)
s3 = S3(session_kwargs__s3=session_kwargs)
```

---

## Bucket Operations

### Create Bucket
```python
result = s3.bucket_create(
    bucket     = 'my-bucket',
    region     = 'us-east-1',
    versioning = True                   # Optional: enable versioning
)
# Returns: {'status': 'ok', 'data': location} or {'status': 'error', 'data': error_msg}
```

### Check Bucket Exists
```python
exists     = s3.bucket_exists('my-bucket')      # True/False
not_exists = s3.bucket_not_exists('my-bucket')  # True/False
```

### Delete Bucket
```python
deleted = s3.bucket_delete('my-bucket')         # True if deleted
```

### List Buckets
```python
bucket_names = s3.buckets()                     # List of bucket names (sorted)
```

### Bucket Versioning
```python
s3.bucket_versioning__enable('my-bucket')
enabled = s3.bucket_versioning__enabled('my-bucket')    # True/False
status  = s3.bucket_versioning__status('my-bucket')     # 'Enabled' or 'Not Enabled'
```

### Delete All Files in Bucket
```python
s3.bucket_delete_all_files('my-bucket')         # Handles versioned buckets too
```

---

## File Operations

### Upload Files

**From local file:**
```python
s3.file_upload_to_key(
    file   = '/path/to/file.txt',
    bucket = 'my-bucket',
    key    = 'folder/file.txt'
)
```

**From bytes:**
```python
s3.file_create_from_bytes(
    file_bytes   = b'binary content',
    bucket       = 'my-bucket',
    key          = 'folder/file.bin',
    metadata     = {'custom': 'value'},         # Optional
    content_type = 'application/octet-stream'   # Optional
)
```

**From string:**
```python
s3.file_create_from_string(
    file_contents = 'text content',
    bucket        = 'my-bucket',
    key           = 'folder/file.txt'
)
```

**From string as gzip:**
```python
s3.file_create_from_string_as_gzip(
    file_contents = 'text content',
    bucket        = 'my-bucket',
    key           = 'folder/file.txt.gz'
)
```

**Upload folder as zip:**
```python
s3.folder_upload(
    folder    = '/path/to/folder',
    s3_bucket = 'my-bucket',
    s3_key    = 'archives/folder.zip'
)
```

### Download Files

**To temp file (returns path):**
```python
local_path = s3.file_download(
    bucket    = 'my-bucket',
    key       = 'folder/file.txt',
    use_cache = True                    # Optional: skip if already downloaded
)
```

**To specific location:**
```python
success = s3.file_download_to(
    bucket      = 'my-bucket',
    key         = 'folder/file.txt',
    target_file = '/path/to/local/file.txt',
    use_cache   = False
)
```

**Download and delete from S3:**
```python
local_path = s3.file_download_and_delete(bucket='my-bucket', key='folder/file.txt')
```

### Read File Contents

**As bytes:**
```python
data = s3.file_bytes(bucket='my-bucket', key='folder/file.bin')
data = s3.file_bytes(bucket='my-bucket', key='file.bin', version_id='abc123')  # Specific version
```

**As string:**
```python
text = s3.file_contents(
    bucket   = 'my-bucket',
    key      = 'folder/file.txt',
    encoding = 'utf-8'                  # Default
)
```

**From gzip:**
```python
text = s3.file_contents_from_gzip(bucket='my-bucket', key='folder/file.txt.gz')
```

### File Existence & Info

```python
exists     = s3.file_exists('my-bucket', 'folder/file.txt')
not_exists = s3.file_not_exists('my-bucket', 'folder/file.txt')

details      = s3.file_details('my-bucket', 'folder/file.txt')   # Full head_object response
content_type = s3.file_content_type('my-bucket', 'folder/file.txt')
metadata     = s3.file_metadata('my-bucket', 'folder/file.txt')
size_mb      = s3.file_size_in_Mb('my-bucket', 'folder/file.txt')
```

### File Operations

**Delete:**
```python
deleted = s3.file_delete(bucket='my-bucket', key='folder/file.txt')
```

**Delete multiple:**
```python
s3.files_delete(bucket='my-bucket', keys=['file1.txt', 'file2.txt'])
```

**Copy:**
```python
s3.file_copy(
    bucket_source      = 'source-bucket',
    key_source         = 'source/file.txt',
    bucket_destination = 'dest-bucket',
    key_destination    = 'dest/file.txt',
    metadata           = {'new': 'metadata'},   # Optional
    content_type       = 'text/plain'           # Optional
)
```

**Move:**
```python
moved = s3.file_move(
    src_bucket  = 'source-bucket',
    src_key     = 'source/file.txt',
    dest_bucket = 'dest-bucket',
    dest_key    = 'dest/file.txt'
)
```

### Update Metadata & Content Type

```python
s3.file_metadata_update(
    bucket   = 'my-bucket',
    key      = 'folder/file.txt',
    metadata = {'key': 'value'}
)

s3.file_content_type_update(
    bucket       = 'my-bucket',
    key          = 'folder/file.txt',
    metadata     = {'key': 'value'},            # Required with content type update
    content_type = 'text/plain'
)
```

### File Versioning

```python
versions       = s3.file_versions('my-bucket', 'folder/file.txt')
delete_markers = s3.file_versions__delete_markers('my-bucket', 'folder/file.txt')
```

---

## Folder/Listing Operations

### List Files

**All files with prefix:**
```python
files = s3.find_files(
    bucket = 'my-bucket',
    prefix = 'folder/',                 # Optional
    filter = '.txt'                     # Optional: filter by substring
)
```

**Files in folder (non-recursive):**
```python
files = s3.folder_files(
    s3_bucket        = 'my-bucket',
    parent_folder    = 'folder/',
    return_full_path = False            # True for full S3 keys
)
```

### List Folders

```python
folders = s3.folder_list(
    s3_bucket        = 'my-bucket',
    parent_folder    = 'parent/',
    return_full_path = False
)
```

### Folder Contents (files + folders)

```python
contents = s3.folder_contents(
    s3_bucket        = 'my-bucket',
    parent_folder    = 'folder/',
    return_full_path = False
)
# Returns: {'folders': ['subfolder1', 'subfolder2'], 'files': ['file1.txt', 'file2.txt']}
```

---

## Pre-Signed URLs

```python
url = s3.create_pre_signed_url(
    bucket_name = 'my-bucket',
    object_name = 'folder/file.txt',
    operation   = 'get_object',         # or 'put_object'
    expiration  = 3600                  # Seconds (default: 1 hour)
)
```

---

## Bucket Notifications

```python
# Get current notification config
config = s3.bucket_notification('my-bucket')

# Create notification (e.g., trigger Lambda)
notification_config = {
    'LambdaFunctionConfigurations': [{
        'LambdaFunctionArn': 'arn:aws:lambda:...',
        'Events': ['s3:ObjectCreated:*'],
        'Filter': {
            'Key': {
                'FilterRules': [{'Name': 'prefix', 'Value': 'uploads/'}]
            }
        }
    }]
}
s3.bucket_notification_create('my-bucket', notification_config)
```

---

## Bucket Policies

```python
policy            = s3.policy('my-bucket')
statements        = s3.policy_statements('my-bucket')
statements_by_sid = s3.policy_statements('my-bucket', index_by='Sid')

# Create/update policy
s3.policy_create('my-bucket', statements=[...])
```

---

## Content Type Mappings

Built-in content type detection by file extension:

```python
from osbot_aws.aws.s3.S3 import S3_FILES_CONTENT_TYPES

# Supported mappings:
# .js   -> application/javascript; charset=utf-8
# .jpg  -> image/jpeg
# .png  -> image/png
# .html -> text/html; charset=utf-8
# .css  -> text/css; charset=utf-8
# .json -> application/json; charset=utf-8
# .pdf  -> application/pdf
# .zip  -> application/zip
# ... and more
```

---

## S3__DB_Base: Application Storage

High-level abstraction for using S3 as application storage:

```python
from osbot_aws.aws.s3.S3__DB_Base import S3__DB_Base

class MyAppStorage(S3__DB_Base):
    bucket_name__prefix: str = 'myapp'
    bucket_name__suffix: str = 'data'
    bucket_versioning  : bool = True
    save_as_gz         : bool = False
```

### Key Features

**Auto-generated bucket names:**
```python
storage = MyAppStorage()
bucket  = storage.s3_bucket()           # e.g., 'myapp-123456789012-data'
```

**Setup (creates bucket if needed):**
```python
storage.setup()
```

**File operations:**
```python
# Save data (auto JSON serialization)
storage.s3_save_data(data={'key': 'value'}, s3_key='records/item.json')

# Read data
data = storage.s3_file_data('records/item.json')
data = storage.s3_file_contents_json('records/item.json')
data = storage.s3_file_contents_obj('records/item.json')  # Python object

# Check existence
exists = storage.s3_file_exists('records/item.json')

# Delete
storage.s3_file_delete('records/item.json')

# List
files   = storage.s3_folder_files('records/')
folders = storage.s3_folder_list('records/')
```

**Pre-signed URLs for temp uploads:**
```python
urls = storage.s3_temp_folder__pre_signed_urls_for_object(
    source     = 'api',
    reason     = 'user upload',
    who        = 'user@example.com',
    expiration = 3600
)
# Returns: {'pre_signed_url__get': '...', 'pre_signed_url__put': '...', ...}
```

---

## S3__Key_Generator: Time-Based Partitioning

Generate S3 keys with time-based folder structure:

```python
from osbot_aws.aws.s3.S3__Key_Generator import S3__Key_Generator
from osbot_utils.helpers.Safe_Id import Safe_Id

generator = S3__Key_Generator(
    root_folder = 'data',
    server_name = 'api-server-1',
    use_date    = True,
    use_hours   = True,
    use_minutes = True                  # Groups into 5-minute blocks
)

# Generate key
s3_key = generator.s3_key(
    area    = Safe_Id('events'),
    file_id = Safe_Id('abc123')
)
# Result: 'data/api-server-1/events/2024-01-15/14/30/abc123.json'

# Get folder for today
folder = generator.s3_folder__for_day()
# Result: 'data/api-server-1/2024-01-15'
```

---

## S3__On_Temp_Bucket: Temporary Storage

Standardized temp bucket operations:

```python
from osbot_aws.aws.s3.S3__On_Temp_Bucket import S3__On_Temp_Bucket

temp_s3 = S3__On_Temp_Bucket()

# Bucket name format: '{account_id}--osbot--temp--{region_name}'
bucket_name = temp_s3.bucket_name()

# Operations
temp_s3.add_file('/local/file.txt', 'remote/file.txt')
temp_s3.create_file__from_string('test.txt', 'content')
temp_s3.create_file__from_bytes('test.bin', b'bytes')

contents = temp_s3.file_contents('test.txt')
exists   = temp_s3.file_exists('test.txt')
deleted  = temp_s3.file_delete('test.txt')
```

---

## Virtual Storage Abstraction

S3-backed implementation of virtual storage interface:

```python
from osbot_aws.aws.s3.S3__Virtual_Storage import Virtual_Storage__S3

storage = Virtual_Storage__S3(
    root_folder = 's3-cache/',
    s3_db       = my_s3_db_instance
)

# Standard operations
storage.json__save('config.json', {'key': 'value'})
data = storage.json__load('config.json')
exists = storage.file__exists('config.json')
files = storage.files__all()
```

---

## Helper Functions

```python
from osbot_aws.aws.s3.S3 import s3_file_download, s3_file_download_to

# Quick download
local_path = s3_file_download('bucket', 'key', use_cache=True)
success    = s3_file_download_to('bucket', 'key', '/local/path', use_cache=False)
```

---

## Threading Configuration

```python
s3 = S3()
s3.dont_use_threads()                   # Disable threading for uploads

# Or configure transfer
from boto3.s3.transfer import TransferConfig
config = s3.transfer_config()           # Returns TransferConfig with use_threads setting
```

---

## Complete Example: Application Data Store

```python
from osbot_aws.aws.s3.S3__DB_Base import S3__DB_Base
from osbot_utils.helpers.Safe_Id import Safe_Id

class UserDataStore(S3__DB_Base):
    bucket_name__prefix: str  = 'myapp'
    bucket_name__suffix: str  = 'userdata'
    bucket_versioning  : bool = True

    def save_user(self, user_id: str, data: dict):
        s3_key = f'users/{user_id}.json'
        return self.s3_save_data(data, s3_key)

    def get_user(self, user_id: str):
        s3_key = f'users/{user_id}.json'
        return self.s3_file_data(s3_key)

    def delete_user(self, user_id: str):
        s3_key = f'users/{user_id}.json'
        return self.s3_file_delete(s3_key)

    def list_users(self):
        files = self.s3_folder_files('users/')
        return [f.replace('.json', '') for f in files]

# Usage
store = UserDataStore()
store.setup()                           # Creates bucket if needed

store.save_user('user123', {'name': 'Alice', 'email': 'alice@example.com'})
user = store.get_user('user123')
users = store.list_users()
```
